{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a06af59-c00d-47cd-98f7-0b6bb5a2cabe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c314b2b8-8cdb-4289-a277-553c6083c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter, namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    TimeSeriesSplit,\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import (\n",
    "    make_scorer,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    balanced_accuracy_score,\n",
    "    precision_recall_fscore_support as score,\n",
    "    f1_score,\n",
    "    r2_score,\n",
    "    mean_absolute_error as mae,\n",
    "    mean_squared_error as mse\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from pyriemann.estimation import XdawnCovariances, BlockCovariances, Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from pyriemann.classification import MDM, TSclassifier\n",
    "from pyriemann.utils.mean import mean_riemann\n",
    "\n",
    "from causallearn.search.FCMBased import lingam\n",
    "from tigramite import data_processing as dp, independence_tests, pcmci\n",
    "\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "manualSeed = 111\n",
    "SEED = manualSeed\n",
    "FREQ = 250\n",
    "random.seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b026a65-f04c-4a5c-b87c-9ab0e975644e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d382f3-3455-4628-afa1-652f5209d415",
   "metadata": {},
   "source": [
    "Functions to upload subjects data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6343a6d1-dcce-41b5-b4bc-c74081a17671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(arr, mean=None, std=None):\n",
    "    # arr.shape = (K_EVENTS, K_CHANNELS, TIMESTEPS)\n",
    "    arr_mean = arr.mean(axis=(0, 2), keepdims=True) if mean is None else mean.reshape(1, -1, 1)\n",
    "    arr_std = arr.std(axis=(0, 2), keepdims=True) if std is None else std.reshape(1, -1, 1)\n",
    "    return (arr - arr_mean) / arr_std\n",
    "\n",
    "\n",
    "def load_subject(npz_path: Path, **kwargs):\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    # signals.shape = (K_EVENTS, EEG_CHANNELS + IMU_CHANNELS, TIMESTEPS)\n",
    "    signals = d['signals'].astype(np.float32)\n",
    "    channels = d['channels']\n",
    "    target = d['target']\n",
    "\n",
    "    # ----- split scalp EEG vs noise vs \"other\" -----\n",
    "    # Assumption: first block = scalp, second block = noise (same length)\n",
    "    idx_noise_start = np.where(np.char.startswith(channels, 'N-'))[0][0]\n",
    "    scalp = signals[:, :idx_noise_start, :]\n",
    "    noise = signals[:, idx_noise_start:2*idx_noise_start, :]\n",
    "    other = signals[:, 2*idx_noise_start:, :]\n",
    "    eeg = scalp - noise\n",
    "\n",
    "    imu_names = (\n",
    "        'LISCM', 'LSSCM', 'LSTrap', 'LITrap',\n",
    "        'RITrap', 'RISCM', 'RSSCM', 'RSTrap',\n",
    "        'Participant_Paddle_Acc_X(g)',\n",
    "        'Participant_Paddle_Acc_Y(g)',\n",
    "        'Participant_Paddle_Acc_Z(g)'\n",
    "    )\n",
    "    imu_idx = np.array(\n",
    "        [int(np.where(channels == n)[0][0]) for n in imu_names]\n",
    "    )\n",
    "    imu = signals[:, imu_idx, :]\n",
    "    imu_idx -= 2*idx_noise_start\n",
    "\n",
    "    eeg_mean, eeg_std = kwargs.get('eeg_mean'), kwargs.get('eeg_std')\n",
    "    imu_mean, imu_std = kwargs.get('imu_mean'), kwargs.get('imu_std')\n",
    "    imu_mean = imu_mean[imu_idx] if imu_mean is not None else None\n",
    "    imu_std = imu_std[imu_idx] if imu_std is not None else None\n",
    "    \n",
    "    eeg = zscore(eeg, eeg_mean, eeg_std)\n",
    "    imu = zscore(imu, imu_mean, imu_std)\n",
    "\n",
    "    target[target == 2] = 1\n",
    "\n",
    "    return eeg, imu, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47b55c-1410-4cab-afc5-263a35b3b2bc",
   "metadata": {},
   "source": [
    "Extract pre-calculated mean and std from `collect_data_*.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192ed8e6-0434-43a1-9857-8437dd71fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_HUMAN = \"../data/human-player/signals-stats/\"\n",
    "\n",
    "eeg_mean = np.load(os.path.join(PATH_HUMAN, \"eeg_mean.npy\"))\n",
    "eeg_std = np.load(os.path.join(PATH_HUMAN, \"eeg_std.npy\"))\n",
    "imu_mean = np.load(os.path.join(PATH_HUMAN, \"imu_mean.npy\"))\n",
    "imu_std = np.load(os.path.join(PATH_HUMAN, \"imu_std.npy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ed8f8-144e-43b4-90f5-5803b34e114b",
   "metadata": {},
   "source": [
    "Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3066f46-e320-4ef7-b120-2601487a2693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00da33ed2504481ea7f8718cecb21c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (18826, 119, 250) (18826, 11, 250) (18826,)\n"
     ]
    }
   ],
   "source": [
    "root = Path(\"../data/human-player/data/\")\n",
    "eeg_list, imu_list, y_list, subjects_list = [], [], [], []\n",
    "\n",
    "for npz_file in tqdm(sorted(root.glob(\"signals_*.npz\"))):\n",
    "    subject_id = int(npz_file.name[-6:-4])\n",
    "    eeg, imu, target = load_subject(\n",
    "        npz_file,\n",
    "        eeg_mean=eeg_mean,\n",
    "        eeg_std=eeg_std,\n",
    "        imu_mean=imu_mean,\n",
    "        imu_std=imu_std\n",
    "    )\n",
    "    assert len(eeg) == len(imu) == len(target)\n",
    "    assert eeg.shape[2] == imu.shape[2]\n",
    "    assert eeg.shape[1] == 119\n",
    "    assert 3 <= imu.shape[1] <= 23\n",
    "    \n",
    "    eeg_list.append(eeg)\n",
    "    imu_list.append(imu)\n",
    "    y_list.append(target)\n",
    "    subjects_list.append([subject_id] * len(target))\n",
    "\n",
    "eeg_total = np.concatenate(eeg_list, axis=0)         # (N, K_EEG, K_TS)\n",
    "imu_total = np.concatenate(imu_list, axis=0)         # (N, K_IMU, K_TS)\n",
    "y_total = np.concatenate(y_list, axis=0).astype(int) # (N,)\n",
    "subjects_total = np.concatenate(subjects_list, axis=0)\n",
    "\n",
    "del eeg_list, imu_list, y_list\n",
    "\n",
    "print(\"Loaded:\", eeg_total.shape, imu_total.shape, y_total.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147dfd42-420c-4d8e-a246-3f3728ff22b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Riemannian space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57eafc-9fd8-4c53-8e7f-9faa172ec63f",
   "metadata": {},
   "source": [
    "| Method | 200 EEG Matrices | 1000 EEG Matrices |\n",
    "| ------ | ---------------- | ----------------- |\n",
    "| SCM    | 1.22             | 22.4              |\n",
    "| lwf    | 1.05             | 7.4               |\n",
    "| oas    | 1.21             | 10.6              |\n",
    "| mcd    | 22.8             | > 1 minute        |\n",
    "| hub    | 8.21             | > 1 minute        |\n",
    "\n",
    "We'll stop on `oas` method\n",
    "\n",
    "Scikit-lean materials: [link](https://scikit-learn.org/stable/modules/covariance.html#shrunk-covariance)\n",
    "\n",
    "pyRiemann materials: [link](https://pyriemann.readthedocs.io/en/latest/auto_examples/covariance-estimation/plot_covariance_estimation_robust.html#sphx-glr-auto-examples-covariance-estimation-plot-covariance-estimation-robust-py)\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "%%time\n",
    "eeg_slice = eeg_total[:1000]\n",
    "y_slice = y_total[:1000]\n",
    "\n",
    "xd  = XdawnCovariances(nfilter=8, estimator='scm').fit(eeg_slice, y_slice)\n",
    "cov = xd.transform(eeg_slice)                           # (N, P, P)  P=16\n",
    "ts  = TangentSpace(metric='riemann').fit(cov)\n",
    "eeg_feat = ts.transform(cov)    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "991aeed2-f2fb-4435-8736-91ab3fa9e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 49s, sys: 1min 52s, total: 3min 42s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def imu_stats(arr):\n",
    "    # arr: (N, C, T) → output shape (N, (C+1)*4)\n",
    "    acceleration = np.sqrt(np.sum(np.square(arr[:, -3:, :]), axis=1, keepdims=True))\n",
    "    arr_stacked = np.concatenate((arr, acceleration), axis=1)\n",
    "    mean = arr_stacked.mean(axis=2)\n",
    "    std = arr_stacked.std(axis=2)\n",
    "    mx = arr_stacked.max(axis=2)\n",
    "    mn = arr_stacked.min(axis=2)\n",
    "    return np.concatenate([mean, std, mx, mn], axis=1)\n",
    "\n",
    "\n",
    "xd  = XdawnCovariances(nfilter=8, estimator='oas').fit(eeg_total, y_total)\n",
    "cov = xd.transform(eeg_total)                           # (N, P, P)  P=16\n",
    "ts  = TangentSpace(metric='riemann').fit(cov)\n",
    "eeg_feat = ts.transform(cov)                         # (N, P*(P+1)/2)\n",
    "\n",
    "imu_feat = imu_stats(imu_total) # (N, (C+1)*4)\n",
    "\n",
    "X_feat = np.hstack([eeg_feat, imu_feat])             # (N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "61ebfdde-8491-442e-a215-e0fee2a7dcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18826, 1176), (18826, 48), (18826, 1224))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_feat.shape, imu_feat.shape, X_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa0cee-81fd-48ad-82d6-9efa89f6de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del eeg_feat, imu_feat, X_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1db87b-4846-4ba3-a121-49291591cbf1",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d05694-5999-49b7-8eca-9002ebe4d2f4",
   "metadata": {},
   "source": [
    "### No models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487cf4a-f90b-447a-9edb-1de4ec3d34d6",
   "metadata": {},
   "source": [
    "Target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9692a558-44df-4905-8ab9-21c98f5ebc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1: 0.09768405396791671, 0: 0.8405927971953681, 1: 0.06172314883671518}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, counts = np.unique(y_total, return_counts=True)\n",
    "dict(zip(labels, counts / counts.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d54e1d-1f15-42a2-a1db-35abfb2f0d5c",
   "metadata": {},
   "source": [
    "Predict only zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f338e46f-7f8b-42d7-8cdc-fa6439af6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_true, y_pred):\n",
    "    return {\n",
    "        'macro': round(f1_score(y_true, y_pred, average='macro'), 3),\n",
    "        'weighted': round(f1_score(y_true, y_pred, average='weighted'), 3),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "897e54fb-cfc0-414f-be16-124728eef203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro': 0.304, 'weighted': 0.768}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(y_total, np.zeros_like(y_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f2fb31-0756-4c70-a57e-f0899f90b178",
   "metadata": {},
   "source": [
    "Random prediction (with equal chances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2bbb6a2d-61f3-4200-bd00-290757813e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro': 0.24454999999999988, 'weighted': 0.42286999999999986}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "rand_pred_score = np.zeros(2)\n",
    "N = 100\n",
    "\n",
    "for _ in range(N):\n",
    "    rand_prediction = np.random.randint(low=-1, high=2, size=len(y_total))\n",
    "    rand_pred_score += np.array(\n",
    "        list(get_scores(y_total, rand_prediction).values())\n",
    "    )\n",
    "rand_pred_score /= N\n",
    "dict(zip(('macro', 'weighted'), rand_pred_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5aecab-f818-44c6-a18b-74bfbf54d9d2",
   "metadata": {},
   "source": [
    "Random prediction (proportional chances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cfbadcff-4b44-4a27-81b5-5ad7aa31aaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro': 0.33304, 'weighted': 0.7197199999999997}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chances_sampler(values, probs, size):\n",
    "    assert np.allclose(np.sum(probs), 1)\n",
    "    \n",
    "    unif_sample = np.random.rand(size)\n",
    "    ranges = [0] + np.cumsum(probs).tolist()\n",
    "    samples = np.full(size, values[-1])\n",
    "\n",
    "    for ind, value in enumerate(values):\n",
    "        mask = np.bitwise_and(ranges[ind] <= unif_sample, unif_sample < ranges[ind + 1])\n",
    "        samples[mask] = value\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "rand_pred_score = np.zeros(2)\n",
    "N = 100\n",
    "\n",
    "for _ in range(N):\n",
    "    rand_prediction = chances_sampler([-1, 0, 1], [0.1, 0.84, 0.06], y_total.size)\n",
    "    rand_pred_score += np.array(\n",
    "        list(get_scores(y_total, rand_prediction).values())\n",
    "    )\n",
    "\n",
    "rand_pred_score /= N\n",
    "dict(zip(('macro', 'weighted'), rand_pred_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0decf77-ff7f-4f99-b7f6-ec308199a396",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "|             | only \"neutral\" | random (equal) | random (prop) |\n",
    "|-------------|----------------|----------------|---------------|\n",
    "| f1-macro    |      0.304     |      0.244     |   **0.333**   |\n",
    "| f1-weighted |    **0.768**   |      0.423     |     0.720     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1939f56f-49ca-44e3-b974-c56e23dadfac",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb001249-6e01-410f-b117-076d76571644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19868b30d1484e3aab6e39ec21274d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46091c1aa8c04910bc62d9bd491c4541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e2e3e9df43490e8a03e7cfb278dde4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c87626b8374b0cbe87923ecc8d4231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.97 s, sys: 2.11 s, total: 5.07 s\n",
      "Wall time: 5min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL_NAMES = (\"LogReg\", \"kNN\", \"CatBoost\")\n",
    "\n",
    "\n",
    "def instantiate_model(model_nm):\n",
    "    assert model_nm in MODEL_NAMES\n",
    "\n",
    "    if model_nm == \"LogReg\":\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(\n",
    "                max_iter=2000,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "        )\n",
    "    elif model_nm == \"kNN\":\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        )\n",
    "    elif model_nm == \"CatBoost\":\n",
    "        return CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'EEG_ONLY': eeg_feat,\n",
    "    'IMU_ONLY': imu_feat,\n",
    "    'TOTAL': X_feat\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "scoring = {\n",
    "    'f1-macro': make_scorer(f1_score, average='macro'),\n",
    "    'f1-weighted': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "baseline_results_cv = []\n",
    "\n",
    "for dataset_nm, dataset in tqdm(datasets.items()):\n",
    "    for model_nm in tqdm(MODEL_NAMES):\n",
    "        model = instantiate_model(model_nm)\n",
    "        scores = cross_validate(model, dataset, y_total, cv=cv, scoring=scoring, n_jobs=4)\n",
    "        row = {\n",
    "            'model': model_nm,\n",
    "            'data': dataset_nm,\n",
    "            'f1_macro_mean': scores['test_f1-macro'].mean(),\n",
    "            'f1_macro_std': scores['test_f1-macro'].std(),\n",
    "            'f1_weighted_mean': scores['test_f1-weighted'].mean(),\n",
    "            'f1_weighted_std': scores['test_f1-weighted'].std(),\n",
    "        }\n",
    "        baseline_results_cv.append(row)\n",
    "    gc.collect()\n",
    "        \n",
    "baseline_results_cv = pd.DataFrame(baseline_results_cv)\n",
    "baseline_results_cv.to_csv('../experiment-results/human-player/baseline_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7922e0bd-888e-44f1-bc71-e0c034640d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data</th>\n",
       "      <th>f1_macro_mean</th>\n",
       "      <th>f1_macro_std</th>\n",
       "      <th>f1_weighted_mean</th>\n",
       "      <th>f1_weighted_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>EEG_ONLY</td>\n",
       "      <td>0.384565</td>\n",
       "      <td>0.008038</td>\n",
       "      <td>0.627462</td>\n",
       "      <td>0.003938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kNN</td>\n",
       "      <td>EEG_ONLY</td>\n",
       "      <td>0.340214</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.768822</td>\n",
       "      <td>0.003023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>EEG_ONLY</td>\n",
       "      <td>0.304465</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.767792</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>IMU_ONLY</td>\n",
       "      <td>0.357405</td>\n",
       "      <td>0.008640</td>\n",
       "      <td>0.639138</td>\n",
       "      <td>0.008640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kNN</td>\n",
       "      <td>IMU_ONLY</td>\n",
       "      <td>0.335120</td>\n",
       "      <td>0.010677</td>\n",
       "      <td>0.772294</td>\n",
       "      <td>0.003770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>IMU_ONLY</td>\n",
       "      <td>0.319251</td>\n",
       "      <td>0.005429</td>\n",
       "      <td>0.772677</td>\n",
       "      <td>0.002239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>TOTAL</td>\n",
       "      <td>0.386057</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.633662</td>\n",
       "      <td>0.001441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kNN</td>\n",
       "      <td>TOTAL</td>\n",
       "      <td>0.346712</td>\n",
       "      <td>0.008934</td>\n",
       "      <td>0.772443</td>\n",
       "      <td>0.002801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>TOTAL</td>\n",
       "      <td>0.312686</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.770581</td>\n",
       "      <td>0.000857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model      data  f1_macro_mean  f1_macro_std  f1_weighted_mean  \\\n",
       "0    LogReg  EEG_ONLY       0.384565      0.008038          0.627462   \n",
       "1       kNN  EEG_ONLY       0.340214      0.007964          0.768822   \n",
       "2  CatBoost  EEG_ONLY       0.304465      0.000018          0.767792   \n",
       "3    LogReg  IMU_ONLY       0.357405      0.008640          0.639138   \n",
       "4       kNN  IMU_ONLY       0.335120      0.010677          0.772294   \n",
       "5  CatBoost  IMU_ONLY       0.319251      0.005429          0.772677   \n",
       "6    LogReg     TOTAL       0.386057      0.003570          0.633662   \n",
       "7       kNN     TOTAL       0.346712      0.008934          0.772443   \n",
       "8  CatBoost     TOTAL       0.312686      0.001845          0.770581   \n",
       "\n",
       "   f1_weighted_std  \n",
       "0         0.003938  \n",
       "1         0.003023  \n",
       "2         0.000126  \n",
       "3         0.008640  \n",
       "4         0.003770  \n",
       "5         0.002239  \n",
       "6         0.001441  \n",
       "7         0.002801  \n",
       "8         0.000857  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3dc149-2d4e-4206-9254-97fb0f41416d",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52901e9f-c35b-410f-8666-f6b1569cc497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=14935  test=3891 windows\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 0)  train ↔ test split by participant\n",
    "#     ---------------------------------\n",
    "# subjects_total : (N,)  int   1 … 25 for every window/row\n",
    "# ================================================================\n",
    "train_mask = subjects_total <= 20\n",
    "test_mask  = subjects_total > 20\n",
    "\n",
    "X_eeg_tr, X_eeg_te = eeg_feat[train_mask], eeg_feat[test_mask]\n",
    "X_imu_tr, X_imu_te = imu_feat[train_mask], imu_feat[test_mask]\n",
    "X_feat_tr, X_feat_te = X_feat[train_mask], X_feat[test_mask]\n",
    "y_tr, y_te = y_total[train_mask], y_total[test_mask]\n",
    "\n",
    "print(f\"train={len(y_tr)}  test={len(y_te)} windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22e2f51e-972f-44e1-a428-dcbd02bcd058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42b065f7e064b64b671de4f3491f737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45862f565aae4996b44190ef6483d8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca20d61646c453ca9d4c460215b8fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b64b114d08d4113981d7fb8012bed1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38min 32s, sys: 2min 26s, total: 40min 59s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL_NAMES = (\"LogReg\", \"kNN\", \"CatBoost\")\n",
    "\n",
    "\n",
    "def instantiate_model(model_nm):\n",
    "    assert model_nm in MODEL_NAMES\n",
    "\n",
    "    if model_nm == \"LogReg\":\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(\n",
    "                max_iter=2000,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "        )\n",
    "    elif model_nm == \"kNN\":\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        )\n",
    "    elif model_nm == \"CatBoost\":\n",
    "        return CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'EEG_ONLY': (X_eeg_tr, X_eeg_te),\n",
    "    'IMU_ONLY': (X_imu_tr, X_imu_te),\n",
    "    'TOTAL': (X_feat_tr, X_feat_te)\n",
    "}\n",
    "baseline_results_tr_te = []\n",
    "\n",
    "for dataset_nm, (temp_tr, temp_te) in tqdm(datasets.items()):\n",
    "    for model_nm in tqdm(MODEL_NAMES):\n",
    "        model = instantiate_model(model_nm).fit(temp_tr, y_tr)\n",
    "        model.fit(temp_tr, y_tr)\n",
    "        y_pred = model.predict(temp_te)\n",
    "        \n",
    "        row = {\n",
    "            'model': model_nm,\n",
    "            'data': dataset_nm,\n",
    "            'f1_macro': f1_score(y_te, y_pred, average='macro'),\n",
    "            'f1_weighted': f1_score(y_te, y_pred, average='weighted'),\n",
    "        }\n",
    "        baseline_results_tr_te.append(row)\n",
    "    gc.collect()\n",
    "    \n",
    "baseline_results_tr_te = pd.DataFrame(baseline_results_tr_te)\n",
    "baseline_results_tr_te.to_csv('../experiment-results/human-player/baseline_tr_te.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76a2dc4c-27d8-4a27-accb-f494c5e63ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>data</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>EEG_ONLY</td>\n",
       "      <td>0.351678</td>\n",
       "      <td>0.666268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kNN</td>\n",
       "      <td>EEG_ONLY</td>\n",
       "      <td>0.324475</td>\n",
       "      <td>0.810013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>EEG_ONLY</td>\n",
       "      <td>0.311762</td>\n",
       "      <td>0.821590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>IMU_ONLY</td>\n",
       "      <td>0.326279</td>\n",
       "      <td>0.646842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kNN</td>\n",
       "      <td>IMU_ONLY</td>\n",
       "      <td>0.325246</td>\n",
       "      <td>0.814854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>IMU_ONLY</td>\n",
       "      <td>0.314076</td>\n",
       "      <td>0.821655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>TOTAL</td>\n",
       "      <td>0.346474</td>\n",
       "      <td>0.655367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kNN</td>\n",
       "      <td>TOTAL</td>\n",
       "      <td>0.330003</td>\n",
       "      <td>0.814501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>TOTAL</td>\n",
       "      <td>0.314275</td>\n",
       "      <td>0.822065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model      data  f1_macro  f1_weighted\n",
       "0    LogReg  EEG_ONLY  0.351678     0.666268\n",
       "1       kNN  EEG_ONLY  0.324475     0.810013\n",
       "2  CatBoost  EEG_ONLY  0.311762     0.821590\n",
       "3    LogReg  IMU_ONLY  0.326279     0.646842\n",
       "4       kNN  IMU_ONLY  0.325246     0.814854\n",
       "5  CatBoost  IMU_ONLY  0.314076     0.821655\n",
       "6    LogReg     TOTAL  0.346474     0.655367\n",
       "7       kNN     TOTAL  0.330003     0.814501\n",
       "8  CatBoost     TOTAL  0.314275     0.822065"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results_tr_te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fffd40-660d-4fc2-9c6b-dd44cc9c8ca6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Causal dim.reduction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe511dd-3fd0-4c76-9772-471f20eb411a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PurePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3be0573d-dcdb-4e3d-bd52-18be6513dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurePCA:\n",
    "    \"\"\"\n",
    "    Two *independent* PCA projections (no causal block, no lag scan).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_x : int   – # components kept for X\n",
    "    d_y : int   – # components kept for Y\n",
    "    \"\"\"\n",
    "    def __init__(self, d_x=10, d_y=10):\n",
    "        self.d_x = d_x\n",
    "        self.d_y = d_y\n",
    "        self.pca_x_ = None\n",
    "        self.pca_y_ = None\n",
    "\n",
    "    # ---------- scikit-style API ----------\n",
    "    def fit(self, X, Y=None):\n",
    "        \"\"\"Fit two separate PCAs.\"\"\"\n",
    "        self.pca_x_ = PCA(n_components=self.d_x, random_state=SEED).fit(X)\n",
    "        self.pca_y_ = PCA(n_components=self.d_y, random_state=SEED).fit(Y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, Y):\n",
    "        Zx = self.pca_x_.transform(X)\n",
    "        Zy = self.pca_y_.transform(Y)\n",
    "        return (Zx, Zy)\n",
    "\n",
    "    def fit_transform(self, X, Y):\n",
    "        self.fit(X, Y)\n",
    "        return self.transform(X, Y)\n",
    "\n",
    "    # optional reconstructions\n",
    "    def reconstruct_x(self, X):\n",
    "        return self.pca_x_.inverse_transform(self.pca_x_.transform(X))\n",
    "\n",
    "    def reconstruct_y(self, Y):\n",
    "        return self.pca_y_.inverse_transform(self.pca_y_.transform(Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf3528-e2f9-4642-8ca3-fc214b178f5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PureCCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f039b18-9aeb-4d3f-a275-ab9c28f7ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PureCCA:\n",
    "    \"\"\"\n",
    "    Keep **only** the canonical sub-space (no reconstructive block).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    d_c : int  – # canonical dimensions to keep\n",
    "    scale : bool – passed straight to sklearn.CCA\n",
    "    \"\"\"\n",
    "    def __init__(self, d_x=5, d_y=5):\n",
    "        self.d_x = d_x\n",
    "        self.d_y = d_y\n",
    "        self.cca_ = None\n",
    "\n",
    "    # ---------- scikit-style API ----------\n",
    "    def fit(self, X, Y):\n",
    "        self.cca_ = CCA(\n",
    "            n_components=min(self.d_x, self.d_y),\n",
    "            scale=True,\n",
    "            max_iter=1000,\n",
    "            tol=3e-5)\\\n",
    "        .fit(X, Y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, Y):\n",
    "        Zx, Zy = self.cca_.transform(X, Y)\n",
    "        return (Zx, Zy)\n",
    "\n",
    "    def fit_transform(self, X, Y):\n",
    "        self.fit(X, Y)\n",
    "        return self.transform(X, Y, split)\n",
    "\n",
    "    def reconstruct_x(self, X):\n",
    "        Zx = self.cca_.transform(X)\n",
    "        X_recon = Zx @ self.cca_.x_weights_.T\n",
    "        return X_recon\n",
    "\n",
    "    def reconstruct_y(self, Y):\n",
    "        _, Wy = self.cca_.transform(\n",
    "            np.zeros(len(Y), len(self.cca_.x_loadings_)),\n",
    "            Y\n",
    "        )\n",
    "        Y_recon = Wy @ self.cca_.y_weights_.T\n",
    "        return Y_recon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed40dc6-637f-4169-aba4-0e4d35a6efa0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CaSCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "510f8b4b-0120-4d3a-bf15-71d5f076c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaSCA:\n",
    "    \"\"\"\n",
    "    CaSCA pipeline as a scikit-learn style transformer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lag_set : sequence of int\n",
    "        Candidate lags to scan for causal delay.\n",
    "    d_c : int\n",
    "        Number of causal dimensions.\n",
    "    d_hid : int\n",
    "        Total hidden dimensionality (causal + reconstructive).\n",
    "    \"\"\"\n",
    "    def __init__(self, lag_set, d_c=1, d_hid=3):\n",
    "        if isinstance(d_hid, tuple):\n",
    "            assert d_c <= min(d_hid), \"Causal dim should be <= hidden dim.\"\n",
    "        else:\n",
    "            assert d_c <= d_hid, \"Causal dim should be <= hidden dim.\"\n",
    "        self.lag_set = lag_set\n",
    "        self.d_c = d_c\n",
    "        self.d_hid = d_hid\n",
    "        self.cca_ = None\n",
    "        self.pca_x_ = None\n",
    "        self.pca_y_ = None\n",
    "        self.tau_star_ = None\n",
    "\n",
    "    def _get_lags(self, X, Y):\n",
    "        T = len(X)\n",
    "        X_tau = X[:T - self.tau_star_] if self.tau_star_ else X\n",
    "        Y_tau = Y[self.tau_star_:] if self.tau_star_ else Y\n",
    "        return X_tau, Y_tau\n",
    "        \n",
    "    def _scan_lagged_cca(\n",
    "        self, X: np.ndarray, Y: np.ndarray\n",
    "    ):\n",
    "        \"\"\"Return τ⋆ and its first canonical correlation.\"\"\"\n",
    "        if len(self.lag_set) == 1:\n",
    "            self.tau_star_ = self.lag_set[0]\n",
    "            return\n",
    "            \n",
    "        best_tau, best_rho = None, -np.inf\n",
    "        T = len(X)\n",
    "        for tau in self.lag_set:\n",
    "            Xτ = X[:T - tau] if tau else X\n",
    "            Yτ = Y[tau:] if tau else Y\n",
    "            cca = CCA(n_components=1, scale=True)\n",
    "            u, v = cca.fit_transform(Xτ, Yτ)\n",
    "            rho = np.corrcoef(u[:, 0], v[:, 0])[0, 1]\n",
    "            if rho > best_rho:\n",
    "                best_tau, best_rho = tau, rho\n",
    "\n",
    "        self.tau_star_ = best_tau\n",
    "\n",
    "    def _deflate(self, Xc, Yc, cca_proj_x, cca_proj_y, return_padded=False):\n",
    "        Zc = np.pad(cca_proj_x, ((0, self.tau_star_), (0, 0)), mode=\"edge\")\n",
    "        Wc = np.pad(cca_proj_y, ((self.tau_star_, 0), (0, 0)), mode=\"edge\")\n",
    "        \n",
    "        X_res = Xc - Zc @ self.cca_.x_weights_.T\n",
    "        Y_res = Yc - Wc @ self.cca_.y_weights_.T\n",
    "\n",
    "        if return_padded:\n",
    "            return X_res, Y_res, Zc, Wc\n",
    "        else:\n",
    "            return X_res, Y_res\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # center\n",
    "        T = len(X)\n",
    "        Xc = X - X.mean(axis=0)\n",
    "        Yc = Y - Y.mean(axis=0)\n",
    "        \n",
    "        # scan lag\n",
    "        self._scan_lagged_cca(X, Y)\n",
    "                \n",
    "        # fit CCA causal block\n",
    "        X_tau, Y_tau = self._get_lags(Xc, Yc)\n",
    "        self.cca_ = CCA(n_components=self.d_c, scale=True, max_iter=1000, tol=3e-05)\n",
    "        U, V = self.cca_.fit_transform(X_tau, Y_tau)\n",
    "\n",
    "        # deflate\n",
    "        X_res, Y_res = self._deflate(Xc, Yc, U, V)\n",
    "        \n",
    "        # reconstruction PCA\n",
    "        if isinstance(self.d_hid, (int, np.int32, np.int64)):\n",
    "            d_res = self.d_hid - self.d_c\n",
    "            self.pca_x_ = PCA(n_components=d_res, random_state=SEED)\n",
    "            self.pca_y_ = PCA(n_components=d_res, random_state=SEED)\n",
    "        elif isinstance(self.d_hid, tuple):\n",
    "            d_res_x = self.d_hid[0] - self.d_c\n",
    "            d_res_y = self.d_hid[1] - self.d_c\n",
    "            self.pca_x_ = PCA(n_components=d_res_x, random_state=SEED)\n",
    "            self.pca_y_ = PCA(n_components=d_res_y, random_state=SEED)\n",
    "            \n",
    "        self.pca_x_.fit(X_res)\n",
    "        self.pca_y_.fit(Y_res)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, Y, split=False):\n",
    "        \"\"\"\n",
    "        Transform new data into [causal, reconstructive] latents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X, Y : array-like, shape (T, n_x), (T, n_y)\n",
    "        split : bool\n",
    "            If True, return Z_c, Z_r, W_c, W_r; else concatenate Z and reconstructive.\n",
    "        \"\"\"\n",
    "        Xc = X - X.mean(axis=0)\n",
    "        Yc = Y - Y.mean(axis=0)\n",
    "        T, _ = Xc.shape\n",
    "        \n",
    "        # lagged\n",
    "        X_tau, Y_tau = self._get_lags(Xc, Yc)\n",
    "        U, V = self.cca_.transform(X_tau, Y_tau)\n",
    "        \n",
    "        # deflate\n",
    "        X_res, Y_res, Zc, Wc = self._deflate(Xc, Yc, U, V, return_padded=True)\n",
    "        \n",
    "        Zr = self.pca_x_.transform(X_res)\n",
    "        Wr = self.pca_y_.transform(Y_res)\n",
    "        \n",
    "        if split:\n",
    "            return Zc, Zr, Wc, Wr\n",
    "        else:\n",
    "            return np.hstack([Zc, Zr]), np.hstack([Wc, Wr])\n",
    "\n",
    "    def fit_transform(self, X, Y, split=False):\n",
    "        self.fit(X, Y)\n",
    "        return self.transform(X, Y, split)\n",
    "\n",
    "    def reconstruct_x(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Given a fitted CaSCA model and original X, reconstruct X from latents:\n",
    "           X_hat = mean(X) + Z_c @ A_c^T + Z_r @ PCA.components_\n",
    "        \"\"\"\n",
    "        Xc = X - X.mean(axis=0)\n",
    "        T, _ = Xc.shape\n",
    "        tau = self.tau_star_\n",
    "        X_tau = Xc[:T - tau] if tau else Xc\n",
    "        \n",
    "        U_c = self.cca_.transform(X_tau)\n",
    "        Zc = np.pad(U_c, ((0, tau), (0, 0)), mode=\"edge\")\n",
    "        X_res = Xc - Zc @ self.cca_.x_weights_.T\n",
    "        Zr = self.pca_x_.transform(X_res)\n",
    "        Xc_hat = Zc @ self.cca_.x_weights_.T + Zr @ self.pca_x_.components_\n",
    "        X_hat = Xc_hat + X.mean(axis=0)\n",
    "        return X_hat\n",
    "\n",
    "    def reconstruct_y(self, Y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reconstruct Y analogous to X reconstruction.\n",
    "        \"\"\"\n",
    "        Yc = Y - Y.mean(axis=0)\n",
    "        T, _ = Yc.shape\n",
    "        tau = self.tau_star_\n",
    "        Y_tau = Yc[:T - tau] if tau else Xc\n",
    "        \n",
    "        _, V_c = self.cca_.transform(\n",
    "            np.zeros((len(Y_tau), len(self.cca_.x_loadings_))),\n",
    "            Y_tau\n",
    "        )\n",
    "        Wc = np.pad(V_c, ((tau, 0), (0, 0)), mode=\"edge\")\n",
    "        Y_res = Yc - Wc @ self.cca_.y_weights_.T\n",
    "        Wr = self.pca_y_.transform(Y_res)\n",
    "        Yc_hat = Wc @ self.cca_.y_weights_.T + Wr @ self.pca_y_.components_\n",
    "        Y_hat = Yc_hat + Y.mean(axis=0)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47397a36-96b5-42ad-8926-4b205f506aba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ICA-LinGAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bec59-8bba-464a-b732-a34d34a09063",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossICA_LiNGAM:\n",
    "    \"\"\"\n",
    "    1. ICA on [X|Y]          (stat. independence)\n",
    "    2. DirectLiNGAM on ICs   (keep sources that drive across blocks)\n",
    "    Parameters\n",
    "    ----------\n",
    "    random_state : int | None\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=0):\n",
    "        self.random_state = random_state\n",
    "        self.ica_ = None\n",
    "        self.lingam_ = None\n",
    "        self.keep_idx_ = None          # indices of causal sources\n",
    "        self.W_ = None                 # mixing sub-matrix ( (p+q) × d )\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def fit(self, X, Y):\n",
    "        XY = np.hstack([X, Y])\n",
    "        self.ica_ = FastICA(n_components=\"auto\",\n",
    "                            whiten=\"unit-variance\",\n",
    "                            random_state=self.random_state)\n",
    "        S = self.ica_.fit_transform(XY)               # (N, k)\n",
    "\n",
    "        # run LiNGAM on sources\n",
    "        self.lingam_ = lingam.DirectLiNGAM(random_state=self.random_state)\n",
    "        self.lingam_.fit(S)\n",
    "        B = self.lingam_.adjacency_matrix_            # (k,k)\n",
    "\n",
    "        # map each source to \"lives mostly in X or Y\"\n",
    "        load = self.ica_.mixing_                      # (p+q, k)\n",
    "        p = X.shape[1]\n",
    "        x_mask = np.arange(p)\n",
    "        y_mask = np.arange(p, p + Y.shape[1])\n",
    "\n",
    "        src_on_X = np.where(np.abs(load[x_mask]).sum(0) >\n",
    "                            np.abs(load[y_mask]).sum(0))[0]\n",
    "        src_on_Y = np.setdiff1d(np.arange(load.shape[1]), src_on_X)\n",
    "\n",
    "        # keep sources that drive across blocks\n",
    "        keep_in_X = _select_by_out_degree(\n",
    "            B, src_on_X, src_on_Y)                    # X→Y\n",
    "        keep_in_Y = _select_by_out_degree(\n",
    "            B, src_on_Y, src_on_X)                    # Y→X\n",
    "        self.keep_idx_ = np.sort(np.concatenate([keep_in_X, keep_in_Y]))\n",
    "        self.W_ = load[:, self.keep_idx_]             # projection\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def transform(self, X, Y, split=True):\n",
    "        XY = np.hstack([X, Y])\n",
    "        S  = self.ica_.transform(XY)[:, self.keep_idx_]  # (N, d)\n",
    "        # split coords back to X/Y by loadings magnitude\n",
    "        p = X.shape[1]\n",
    "        x_load = np.abs(self.W_[:p]).sum(0)\n",
    "        y_load = np.abs(self.W_[p:]).sum(0)\n",
    "        Zx = S[:, x_load >= y_load]\n",
    "        Zy = S[:, y_load >  x_load]\n",
    "        return (Zx, Zy) if split else (S,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e3b7782-009b-4fb0-9bc9-195b58a081ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ica_lg = lingam.ICALiNGAM(random_state, max_iter)\n",
    "# ica_lg.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777656d-c725-45fe-9a71-e36f5e941e7b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Granger-PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f11fed57-d6d8-4104-975c-4261d0a38976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGrangerPCA:\n",
    "    \"\"\"\n",
    "    PCA in X and Y  ➜  rank PCs by Granger F-stat X→Y (or Y→X)\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_x, n_y : #PCs to keep initially\n",
    "    lag_order : VAR lag\n",
    "    top_k : final #coords to keep from each side\n",
    "    direction : {\"X2Y\", \"Y2X\"}  – causal direction of interest\n",
    "    \"\"\"\n",
    "    def __init__(self, n_x=10, n_y=10, lag_order=5,\n",
    "                 top_k=5, direction=\"X2Y\", random_state=0):\n",
    "        self.n_x, self.n_y = n_x, n_y\n",
    "        self.lag_order = lag_order\n",
    "        self.top_k = top_k\n",
    "        self.direction = direction.upper()\n",
    "        self.random_state = random_state\n",
    "        self.pca_x_ = None\n",
    "        self.pca_y_ = None\n",
    "        self.order_ = None                 # indices of kept PCs\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def fit(self, X, Y):\n",
    "        self.pca_x_ = PCA(self.n_x, random_state=self.random_state).fit(X)\n",
    "        self.pca_y_ = PCA(self.n_y, random_state=self.random_state).fit(Y)\n",
    "        Zx = self.pca_x_.transform(X)\n",
    "        Zy = self.pca_y_.transform(Y)\n",
    "\n",
    "        Z  = np.hstack([Zx, Zy])\n",
    "        var = sm.tsa.VAR(Z).fit(self.lag_order)\n",
    "\n",
    "        f_stats = []\n",
    "        if self.direction == \"X2Y\":\n",
    "            for k in range(self.n_x):\n",
    "                f = var.test_causality([k],\n",
    "                        list(range(self.n_x, self.n_x + self.n_y)),\n",
    "                        kind='f').test_statistic\n",
    "                f_stats.append(f)\n",
    "            self.order_ = np.argsort(f_stats)[::-1][:self.top_k]\n",
    "        else:  # Y2X\n",
    "            for k in range(self.n_y):\n",
    "                f = var.test_causality(\n",
    "                        [self.n_x + k], list(range(self.n_x)),\n",
    "                        kind='f').test_statistic\n",
    "                f_stats.append(f)\n",
    "            self.order_ = np.argsort(f_stats)[::-1][:self.top_k]\n",
    "\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def transform(self, X, Y, split=True):\n",
    "        Zx = self.pca_x_.transform(X)\n",
    "        Zy = self.pca_y_.transform(Y)\n",
    "\n",
    "        if self.direction == \"X2Y\":\n",
    "            Zx_c = Zx[:, self.order_]\n",
    "            return (Zx_c, Zy) if split else (np.hstack([Zx_c, Zy]),)\n",
    "        else:\n",
    "            Zy_c = Zy[:, self.order_]\n",
    "            return (Zx, Zy_c) if split else (np.hstack([Zx, Zy_c]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9452d03-7c0e-42e1-9096-2ea8db124a83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PCMCI + Eigen Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e715802b-2259-4792-9261-0fae73b01af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCMCIeigen:\n",
    "    \"\"\"\n",
    "    1. PCMCI on [X|Y]\n",
    "    2. build bipartite weight matrix of lagged cross-causal edges\n",
    "    3. SVD / Laplacian eigenvectors → matched coordinates\n",
    "    Parameters\n",
    "    ----------\n",
    "    tau_max : int   (maximum lag for PCMCI)\n",
    "    k_keep  : int   (#eigenvectors per side to keep)\n",
    "    \"\"\"\n",
    "    def __init__(self, tau_max=8, k_keep=4, alpha=0.01):\n",
    "        self.tau_max = tau_max\n",
    "        self.k_keep = k_keep\n",
    "        self.alpha = alpha\n",
    "        self.Ux_, self.Vy_ = None, None   # projection bases\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def fit(self, X, Y):\n",
    "        XY = np.hstack([X, Y])\n",
    "        p, q = X.shape[1], Y.shape[1]\n",
    "\n",
    "        df  = dp.DataFrame(XY)\n",
    "        par = independence_tests.ParCorr()\n",
    "        pc  = pcmci.PCMCI(df, par_corr=par, verbosity=0)\n",
    "        res = pc.run_pcmci(tau_max=self.tau_max, alpha_level=self.alpha)\n",
    "\n",
    "        A = pc._return_graph(res, alpha_level=self.alpha)  # (p+q,p+q, τ+1)\n",
    "        Wxy = np.abs(A[:p, p:, 1:]).sum(axis=2)            # cross X→Y\n",
    "        # SVD of bipartite weight\n",
    "        U, S, Vt = np.linalg.svd(Wxy, full_matrices=False)\n",
    "        self.Ux_ = U[:, :self.k_keep]                      # (p,k)\n",
    "        self.Vy_ = Vt[:self.k_keep].T                      # (q,k)\n",
    "        return self\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    def transform(self, X, Y, split=True):\n",
    "        Zx = X @ self.Ux_\n",
    "        Zy = Y @ self.Vy_\n",
    "        return (Zx, Zy) if split else (np.hstack([Zx, Zy]),)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3499717-b419-465a-a72e-ec2e83c7cef7",
   "metadata": {},
   "source": [
    "### Quality criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e93a410-eb1c-469b-89ab-6b5c47f68f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vif(X: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute Variance Inflation Factor (VIF) for each feature in X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Input data matrix (e.g., latent space representation).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    vif_df : pandas DataFrame\n",
    "        DataFrame with columns ['feature', 'VIF'].\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    vif_values = []\n",
    "    for j in range(n_features):\n",
    "        # Define feature j as target, others as predictors\n",
    "        X_j = X[:, j]\n",
    "        X_others = np.delete(X, j, axis=1)\n",
    "        \n",
    "        # Fit linear regression of X_j on X_others\n",
    "        model = LinearRegression().fit(X_others, X_j)\n",
    "        r2_j = model.score(X_others, X_j)\n",
    "        vif_j = 1.0 / (1.0 - r2_j) if r2_j < 1.0 else np.inf\n",
    "        vif_values.append(vif_j)\n",
    "\n",
    "    return max(vif_values)\n",
    "\n",
    "def compute_cond_number(X: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the condition number of X^T X.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Input data matrix.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cond_number : float\n",
    "        Condition number (ratio of largest to smallest singular value).\n",
    "    \"\"\"\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    cond_number = np.log(s.max() / s.min())\n",
    "    return cond_number\n",
    "\n",
    "\n",
    "def compute_rmse(X_true: np.ndarray, X_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Root Mean Squared Error between X_true and X_pred.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((X_true - X_pred) ** 2))\n",
    "\n",
    "\n",
    "def compute_explained_variance(X_true: np.ndarray, X_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the fraction of variance in X_true explained by X_pred.\n",
    "    \"\"\"\n",
    "    var_true = np.var(X_true, axis=0).sum()\n",
    "    var_resid = np.var(X_true - X_pred, axis=0).sum()\n",
    "    return 1 - (var_resid / var_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6be21-26f6-47c4-bfeb-6f99a7afbb7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prediction pipeline (for PurePCA and PureCCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bcc19d73-3f0f-4f5b-a980-2d74b80da479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3612403894bd4aa493185fae57900a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== summary =====\n",
      " d_hid      VIF  log_cond_XTX  RMSE_recon   ExplVar  LogReg_F1w  LogReg_F1m  kNN_F1w  kNN_F1m  CatBoost_F1w  CatBoost_F1m din_red_model\n",
      "     2 1.217931      2.445204    0.145508  0.347674    0.716432    0.351534 0.814274 0.338152      0.822435      0.314425       PurePCA\n",
      "     2 3.414813      8.282478    0.180449 -0.000043    0.717255    0.344524 0.811348 0.324482      0.821415      0.313985       PureCCA\n",
      "     4 1.219511      2.558359    0.140923  0.388138    0.721258    0.356644 0.811931 0.331186      0.821805      0.314159       PurePCA\n",
      "     4 3.415327      8.826862    0.180450 -0.000049    0.696617    0.340955 0.812972 0.321667      0.822597      0.320912       PureCCA\n",
      "     6 1.225162      2.635144    0.137213  0.419928    0.710594    0.352038 0.813290 0.338758      0.823645      0.323700       PurePCA\n",
      "     6 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "     8 1.226387      2.715840    0.133879  0.447782    0.703983    0.351131 0.813752 0.343895      0.822525      0.316713       PurePCA\n",
      "     8 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "    10 1.249519      2.796621    0.130777  0.473074    0.694325    0.346981 0.813406 0.342590      0.822129      0.316512       PurePCA\n",
      "    10 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "    12 1.255901      2.873216    0.128176  0.493824    0.694659    0.351037 0.811643 0.337402      0.821785      0.314134       PurePCA\n",
      "    12 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "    14 1.263313      3.003920    0.125836  0.512139    0.693612    0.355765 0.809658 0.336039      0.822152      0.316555       PurePCA\n",
      "    14 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "    16 1.276667      3.037103    0.123742  0.528237    0.687546    0.350501 0.809042 0.336774      0.821062      0.311561       PurePCA\n",
      "    16 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "    30 1.462939      3.341702    0.113762  0.601264    0.685737    0.342536 0.813601 0.339948      0.821702      0.311804       PurePCA\n",
      "    30 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "    50 1.537108      3.595021    0.105464  0.657313    0.681540    0.347088 0.813332 0.335078      0.821675      0.314101       PurePCA\n",
      "    50 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "    70 1.563484      3.741565    0.099455  0.695253    0.691642    0.349033 0.814162 0.331617      0.821206      0.311616       PurePCA\n",
      "    70 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n",
      "    90 1.576872      3.839019    0.094440  0.725208    0.631510    0.314054 0.814160 0.332927      0.821462      0.311713       PurePCA\n",
      "    90 3.415327      8.826862    0.180449 -0.000041    0.690671    0.345252 0.814312 0.331160      0.822268      0.323466       PureCCA\n"
     ]
    }
   ],
   "source": [
    "lag_set = [0]               # ignore lag scan in this experiment\n",
    "# grid_d_c  = [1, 3, 5, 7, 9, 11, 13, 15, 20, 30, 40]\n",
    "grid_d_hid = [2, 4, 6, 8, 10, 12, 14, 16, 30, 50, 70, 90]\n",
    "\n",
    "\n",
    "def make_clfs():\n",
    "    return {\n",
    "        \"LogReg\": LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \"kNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        \"CatBoost\": CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            devices='0',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    }\n",
    "\n",
    "# helper: run one grid point\n",
    "def run_grid_point(d_hid, dim_red_model_cls):\n",
    "    # ❶  fit CaSCA on *train* only\n",
    "    dr_model = dim_red_model_cls(d_hid)\n",
    "    dr_model.fit(X_eeg_tr, X_imu_tr)\n",
    "\n",
    "    Ztr, Wtr = dr_model.transform(X_eeg_tr, X_imu_tr)\n",
    "    Zte, Wte = dr_model.transform(X_eeg_te, X_imu_te)\n",
    "\n",
    "    FEAT_tr = np.hstack([Ztr, Wtr])\n",
    "    FEAT_te = np.hstack([Zte, Wte])\n",
    "\n",
    "    # ❷  embedding diagnostics (train set)\n",
    "    vif   = compute_vif(FEAT_tr)\n",
    "    cnum  = compute_cond_number(FEAT_tr)\n",
    "    Xrec  = dr_model.reconstruct_x(X_eeg_tr)\n",
    "    rmse  = compute_rmse(X_eeg_tr, Xrec)\n",
    "    ev    = compute_explained_variance(X_eeg_tr, Xrec)\n",
    "\n",
    "    row = dict(d_hid=d_hid,\n",
    "               VIF=vif, log_cond_XTX=cnum,\n",
    "               RMSE_recon=rmse, ExplVar=ev)\n",
    "\n",
    "    # ❸  classifiers\n",
    "    for name, clf in make_clfs().items():\n",
    "        clf.fit(FEAT_tr, y_tr)\n",
    "        y_pred = clf.predict(FEAT_te)\n",
    "\n",
    "        row[f\"{name}_F1w\"] = f1_score(y_te, y_pred, average=\"weighted\")\n",
    "        row[f\"{name}_F1m\"] = f1_score(y_te, y_pred, average=\"macro\")\n",
    "\n",
    "    return row\n",
    "\n",
    "# ================================================================\n",
    "# 3)  Run over grid & collect\n",
    "# ================================================================\n",
    "results = []\n",
    "for d_hid in tqdm(grid_d_hid):\n",
    "    for dim_red_model_cls in (PurePCA, PureCCA):\n",
    "        row = run_grid_point(d_hid, dim_red_model_cls)\n",
    "        row['din_red_model'] = type(dim_red_model_cls(1)).__name__\n",
    "        results.append(row)\n",
    "    gc.collect()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n===== summary =====\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3e9167d-d10e-46ac-b84b-4cf4233cd6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../experiment-results/human-player/baseline_other.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bedb1e-7136-4953-827f-84d8ca22573a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d645bd45-c2ff-48e0-9c8c-6379a004a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del eeg_feat, imu_feat, X_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e3cb303a-672c-4c86-9c9f-f2b80d8dc28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.47 s, sys: 1.65 s, total: 4.12 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def add_acceleration(arr):\n",
    "    # arr: (N, C, T) → output shape (N, C+1, T)\n",
    "    acceleration = np.sqrt(np.sum(np.square(arr[:, -3:, :]), axis=1, keepdims=True))\n",
    "    arr_stacked = np.concatenate((arr, acceleration), axis=1)\n",
    "    return arr_stacked\n",
    "\n",
    "sfreq = 250\n",
    "win_len = 20\n",
    "step = 20\n",
    "ends = sfreq - np.arange(0, eeg_total.shape[2] - win_len + 1, step)\n",
    "n_win = len(ends)\n",
    "\n",
    "# (N, C, n_win, win_len)  ->  (N, C, n_win) -> (N, C*n_win)\n",
    "eeg_feat = np.stack(\n",
    "    [eeg_total[:, :, e-win_len:e] for e in ends],\n",
    "    axis=2\n",
    ").mean(axis=-1).reshape(len(eeg_total), -1)\n",
    "\n",
    "imu_total_with_acc = add_acceleration(imu_total)\n",
    "imu_feat = np.stack(\n",
    "    [imu_total_with_acc[:, :, e-win_len:e] for e in ends],\n",
    "    axis=2\n",
    ").mean(axis=-1).reshape(len(imu_total), -1)\n",
    "\n",
    "X_feat = np.hstack([eeg_feat, imu_feat])\n",
    "del imu_total_with_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "98717331-70f5-4414-b081-3f37024c6f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18826, 1428), (18826, 144), (18826, 1572))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_feat.shape, imu_feat.shape, X_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1d928-3a8b-4925-a1c5-20834c68f928",
   "metadata": {},
   "source": [
    "**Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6834e3d9-63ab-43cb-8281-96ad9a9446f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=14935  test=3891 windows\n"
     ]
    }
   ],
   "source": [
    "train_mask = subjects_total <= 20\n",
    "test_mask  = subjects_total > 20\n",
    "\n",
    "X_eeg_tr, X_eeg_te = eeg_feat[train_mask], eeg_feat[test_mask]\n",
    "X_imu_tr, X_imu_te = imu_feat[train_mask], imu_feat[test_mask]\n",
    "X_feat_tr, X_feat_te = X_feat[train_mask], X_feat[test_mask]\n",
    "y_tr, y_te = y_total[train_mask], y_total[test_mask]\n",
    "\n",
    "print(f\"train={len(y_tr)}  test={len(y_te)} windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93972880-6b67-4286-93b4-038a1819c735",
   "metadata": {},
   "source": [
    "### EEG vs IMU vs both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "16f12133-2412-4ebb-9fde-3f5a380fc755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae0b25988174ed7a35dd60794144da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9629bf90986148cfb31168edd299040e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05f91f19241442fb118f3828c3db4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84723f19830d4ff1a86ca62090854d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 28s, sys: 3min 31s, total: 47min 59s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL_NAMES = (\"LogReg\", \"kNN\", \"CatBoost\")\n",
    "\n",
    "\n",
    "def instantiate_model(model_nm):\n",
    "    assert model_nm in MODEL_NAMES\n",
    "\n",
    "    if model_nm == \"LogReg\":\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(\n",
    "                max_iter=2000,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "        )\n",
    "    elif model_nm == \"kNN\":\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        )\n",
    "    elif model_nm == \"CatBoost\":\n",
    "        return CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'EEG_ONLY': (X_eeg_tr, X_eeg_te),\n",
    "    'IMU_ONLY': (X_imu_tr, X_imu_te),\n",
    "    'TOTAL': (X_feat_tr, X_feat_te)\n",
    "}\n",
    "original_results_tr_te = []\n",
    "\n",
    "for dataset_nm, (temp_tr, temp_te) in tqdm(datasets.items()):\n",
    "    for model_nm in tqdm(MODEL_NAMES):\n",
    "        model = instantiate_model(model_nm).fit(temp_tr, y_tr)\n",
    "        model.fit(temp_tr, y_tr)\n",
    "        y_pred = model.predict(temp_te)\n",
    "        \n",
    "        row = {\n",
    "            'model': model_nm,\n",
    "            'data': dataset_nm,\n",
    "            'f1_macro': f1_score(y_te, y_pred, average='macro'),\n",
    "            'f1_weighted': f1_score(y_te, y_pred, average='weighted'),\n",
    "        }\n",
    "        original_results_tr_te.append(row)\n",
    "    gc.collect()\n",
    "    \n",
    "original_results_tr_te = pd.DataFrame(original_results_tr_te)\n",
    "original_results_tr_te.to_csv('../experiment-results/human-player/original_tr_te.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd0496-4ba2-4b59-98a3-6d60f959700d",
   "metadata": {},
   "source": [
    "### CaSCA grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6c0025df-400f-4e2c-8801-05321816db44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2cd3484cf244996b190b762bccbf783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad509d00cd2d45b18ca530f4fc2f18a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 1  d_hid= 2\n",
      "  ➜ d_c= 1  d_hid= 4\n",
      "  ➜ d_c= 1  d_hid= 6\n",
      "  ➜ d_c= 1  d_hid= 8\n",
      "  ➜ d_c= 1  d_hid=10\n",
      "  ➜ d_c= 1  d_hid=12\n",
      "  ➜ d_c= 1  d_hid=14\n",
      "  ➜ d_c= 1  d_hid=16\n",
      "  ➜ d_c= 1  d_hid=30\n",
      "  ➜ d_c= 1  d_hid=50\n",
      "  ➜ d_c= 1  d_hid=70\n",
      "  ➜ d_c= 1  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6664cea9ab8344c18960783305ef3f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 3  d_hid= 4\n",
      "  ➜ d_c= 3  d_hid= 6\n",
      "  ➜ d_c= 3  d_hid= 8\n",
      "  ➜ d_c= 3  d_hid=10\n",
      "  ➜ d_c= 3  d_hid=12\n",
      "  ➜ d_c= 3  d_hid=14\n",
      "  ➜ d_c= 3  d_hid=16\n",
      "  ➜ d_c= 3  d_hid=30\n",
      "  ➜ d_c= 3  d_hid=50\n",
      "  ➜ d_c= 3  d_hid=70\n",
      "  ➜ d_c= 3  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0ba556855c45bd8d692b66baee3238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 5  d_hid= 6\n",
      "  ➜ d_c= 5  d_hid= 8\n",
      "  ➜ d_c= 5  d_hid=10\n",
      "  ➜ d_c= 5  d_hid=12\n",
      "  ➜ d_c= 5  d_hid=14\n",
      "  ➜ d_c= 5  d_hid=16\n",
      "  ➜ d_c= 5  d_hid=30\n",
      "  ➜ d_c= 5  d_hid=50\n",
      "  ➜ d_c= 5  d_hid=70\n",
      "  ➜ d_c= 5  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c39b12596cd4e1c8501df9bc2e60027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 7  d_hid= 8\n",
      "  ➜ d_c= 7  d_hid=10\n",
      "  ➜ d_c= 7  d_hid=12\n",
      "  ➜ d_c= 7  d_hid=14\n",
      "  ➜ d_c= 7  d_hid=16\n",
      "  ➜ d_c= 7  d_hid=30\n",
      "  ➜ d_c= 7  d_hid=50\n",
      "  ➜ d_c= 7  d_hid=70\n",
      "  ➜ d_c= 7  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d12579702dd45b3892c16187ccbee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 9  d_hid=10\n",
      "  ➜ d_c= 9  d_hid=12\n",
      "  ➜ d_c= 9  d_hid=14\n",
      "  ➜ d_c= 9  d_hid=16\n",
      "  ➜ d_c= 9  d_hid=30\n",
      "  ➜ d_c= 9  d_hid=50\n",
      "  ➜ d_c= 9  d_hid=70\n",
      "  ➜ d_c= 9  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0b90a185574967abf8a87c7100c50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=11  d_hid=12\n",
      "  ➜ d_c=11  d_hid=14\n",
      "  ➜ d_c=11  d_hid=16\n",
      "  ➜ d_c=11  d_hid=30\n",
      "  ➜ d_c=11  d_hid=50\n",
      "  ➜ d_c=11  d_hid=70\n",
      "  ➜ d_c=11  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372d5c9708af4157a64666bf303a24cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=13  d_hid=14\n",
      "  ➜ d_c=13  d_hid=16\n",
      "  ➜ d_c=13  d_hid=30\n",
      "  ➜ d_c=13  d_hid=50\n",
      "  ➜ d_c=13  d_hid=70\n",
      "  ➜ d_c=13  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4437ed2be1423394c9193476c472d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=15  d_hid=16\n",
      "  ➜ d_c=15  d_hid=30\n",
      "  ➜ d_c=15  d_hid=50\n",
      "  ➜ d_c=15  d_hid=70\n",
      "  ➜ d_c=15  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2056919697b47ed907c67e05497e7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=20  d_hid=30\n",
      "  ➜ d_c=20  d_hid=50\n",
      "  ➜ d_c=20  d_hid=70\n",
      "  ➜ d_c=20  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdcd9fbc814437eaeca7e47692cece5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=30  d_hid=30\n",
      "  ➜ d_c=30  d_hid=50\n",
      "  ➜ d_c=30  d_hid=70\n",
      "  ➜ d_c=30  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1febeaf4ce204ae8a61079c1c154c762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=40  d_hid=50\n",
      "  ➜ d_c=40  d_hid=70\n",
      "  ➜ d_c=40  d_hid=90\n"
     ]
    }
   ],
   "source": [
    "lag_set   = [0]               # ignore lag scan in this experiment\n",
    "grid_d_c  = [1, 3, 5, 7, 9, 11, 13, 15, 20, 30, 40]\n",
    "grid_dhid = [2, 4, 6, 8, 10, 12, 14, 16, 30, 50, 70, 90]\n",
    "\n",
    "# ================================================================\n",
    "# 1)  classifier factory (unchanged except seeds)\n",
    "# ================================================================\n",
    "def make_clfs():\n",
    "    return {\n",
    "        \"LogReg\": LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \"kNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        \"CatBoost\": CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------- helper: run one grid point\n",
    "def run_grid_point(d_c, d_hid):\n",
    "    print(f\"  ➜ d_c={d_c:2d}  d_hid={d_hid:2d}\")\n",
    "\n",
    "    # ❶  fit CaSCA on *train* only\n",
    "    casca = CaSCA(lag_set=[0], d_c=d_c, d_hid=(d_hid, min(d_hid, X_imu_tr.shape[1])))\n",
    "    casca.fit(X_eeg_tr, X_imu_tr)\n",
    "\n",
    "    Ztr, Wtr = casca.transform(X_eeg_tr, X_imu_tr)\n",
    "    Zte, Wte = casca.transform(X_eeg_te, X_imu_te)\n",
    "\n",
    "    FEAT_tr = np.hstack([Ztr, Wtr])\n",
    "    FEAT_te = np.hstack([Zte, Wte])\n",
    "\n",
    "    # ❷  embedding diagnostics (train set)\n",
    "    vif   = compute_vif(FEAT_tr)\n",
    "    cnum  = compute_cond_number(FEAT_tr)\n",
    "    Xrec  = casca.reconstruct_x(X_eeg_tr)\n",
    "    rmse  = compute_rmse(X_eeg_tr, Xrec)\n",
    "    ev    = compute_explained_variance(X_eeg_tr, Xrec)\n",
    "\n",
    "    row = dict(d_c=d_c, d_hid=d_hid,\n",
    "               VIF=vif, log_cond_XTX=cnum,\n",
    "               RMSE_recon=rmse, ExplVar=ev)\n",
    "\n",
    "    # ❸  classifiers\n",
    "    for name, clf in make_clfs().items():\n",
    "        clf.fit(FEAT_tr, y_tr)\n",
    "        y_pred = clf.predict(FEAT_te)\n",
    "\n",
    "        row[f\"{name}_F1w\"] = f1_score(y_te, y_pred, average=\"weighted\")\n",
    "        row[f\"{name}_F1m\"] = f1_score(y_te, y_pred, average=\"macro\")\n",
    "\n",
    "    return row\n",
    "\n",
    "# ================================================================\n",
    "# 3)  Run over grid & collect\n",
    "# ================================================================\n",
    "casca_orig_results = []\n",
    "for d_c in tqdm(grid_d_c):\n",
    "    for d_hid in tqdm(grid_dhid):\n",
    "        if d_c > d_hid:                 # skip invalid combos\n",
    "            continue\n",
    "        casca_orig_results.append(run_grid_point(d_c, d_hid))\n",
    "    gc.collect()\n",
    "\n",
    "casca_orig_df = pd.DataFrame(casca_orig_results)\n",
    "casca_orig_df.to_csv('../experiment-results/human-player/original_casca.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3fd65-d998-456e-8b61-27739e635321",
   "metadata": {},
   "source": [
    "### PurePCA and PureCCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "699c86ec-fdf7-42cb-bb3d-8f8d0ec66bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b463721bc30d47538d804e1b49dfe082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lag_set = [0]               # ignore lag scan in this experiment\n",
    "grid_d_hid = [2, 4, 6, 8, 10, 12, 14, 16, 30, 50, 70, 90]\n",
    "\n",
    "\n",
    "def make_clfs():\n",
    "    return {\n",
    "        \"LogReg\": LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \"kNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        \"CatBoost\": CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            devices='0',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    }\n",
    "\n",
    "# helper: run one grid point\n",
    "def run_grid_point(d_hid, dim_red_model_cls):\n",
    "    # ❶  fit CaSCA on *train* only\n",
    "    dr_model = dim_red_model_cls(d_hid)\n",
    "    dr_model.fit(X_eeg_tr, X_imu_tr)\n",
    "\n",
    "    Ztr, Wtr = dr_model.transform(X_eeg_tr, X_imu_tr)\n",
    "    Zte, Wte = dr_model.transform(X_eeg_te, X_imu_te)\n",
    "\n",
    "    FEAT_tr = np.hstack([Ztr, Wtr])\n",
    "    FEAT_te = np.hstack([Zte, Wte])\n",
    "\n",
    "    # ❷  embedding diagnostics (train set)\n",
    "    vif   = compute_vif(FEAT_tr)\n",
    "    cnum  = compute_cond_number(FEAT_tr)\n",
    "    Xrec  = dr_model.reconstruct_x(X_eeg_tr)\n",
    "    rmse  = compute_rmse(X_eeg_tr, Xrec)\n",
    "    ev    = compute_explained_variance(X_eeg_tr, Xrec)\n",
    "\n",
    "    row = dict(d_hid=d_hid,\n",
    "               VIF=vif, log_cond_XTX=cnum,\n",
    "               RMSE_recon=rmse, ExplVar=ev)\n",
    "\n",
    "    # ❸  classifiers\n",
    "    for name, clf in make_clfs().items():\n",
    "        clf.fit(FEAT_tr, y_tr)\n",
    "        y_pred = clf.predict(FEAT_te)\n",
    "\n",
    "        row[f\"{name}_F1w\"] = f1_score(y_te, y_pred, average=\"weighted\")\n",
    "        row[f\"{name}_F1m\"] = f1_score(y_te, y_pred, average=\"macro\")\n",
    "\n",
    "    return row\n",
    "\n",
    "# ================================================================\n",
    "# 3)  Run over grid & collect\n",
    "# ================================================================\n",
    "orig_pure_results = []\n",
    "for d_hid in tqdm(grid_d_hid):\n",
    "    for dim_red_model_cls in (PurePCA, PureCCA):\n",
    "        row = run_grid_point(d_hid, dim_red_model_cls)\n",
    "        row['din_red_model'] = type(dim_red_model_cls(1)).__name__\n",
    "        orig_pure_results.append(row)\n",
    "    gc.collect()\n",
    "\n",
    "orig_pure_df = pd.DataFrame(orig_pure_results)\n",
    "orig_pure_df.to_csv('../experiment-results/human-player/original_other.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad423c07-a102-4c1e-ac83-0ab4beec66b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Trajectory space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d91986b7-1c0a-4901-9564-b7efe2245f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "del eeg_feat, imu_feat, X_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "22753a20-2c8b-4084-8a72-d3b4604a550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 40s, sys: 2min 53s, total: 5min 34s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sfreq = 250\n",
    "win_len = int(0.3 * sfreq)\n",
    "step = int(0.2 * sfreq)\n",
    "ends = sfreq - np.arange(0, eeg_total.shape[2] - win_len + 1, step)\n",
    "n_win = len(ends)\n",
    "\n",
    "# (N, n_win, C, win_len)  ->  (N*n_win, C, win_len)\n",
    "eeg_subwin = np.stack(\n",
    "    [eeg_total[:, :, e-win_len:e] for e in ends],\n",
    "    axis=1\n",
    ").reshape(-1, eeg_total.shape[1], win_len)\n",
    "y_subwin = np.repeat(y_total, n_win)    # same label for every sub-window\n",
    "\n",
    "xd = XdawnCovariances(nfilter=5, estimator='oas')\n",
    "ts = TangentSpace(metric='riemann')\n",
    "cov = xd.fit_transform(eeg_subwin, y_subwin)           # (N*n_win, P, P)\n",
    "eeg_ts = ts.fit_transform(cov)                         # (N*n_win, D_ts)\n",
    "eeg_feat = eeg_ts.reshape(-1, n_win * eeg_ts.shape[1]) # (N, 4·D_ts)\n",
    "\n",
    "imu_subwin = np.stack(\n",
    "    [imu_total[:, :, e-win_len:e] for e in ends],\n",
    "    axis=1\n",
    ").reshape(-1, imu_total.shape[1], win_len)\n",
    "imu_values = imu_stats(imu_subwin)\n",
    "imu_feat = imu_values.reshape(-1, n_win * imu_values.shape[1]) # (N, (C+1)*4)\n",
    "\n",
    "X_feat = np.hstack([eeg_feat, imu_feat])         # (N, 4·D_ts + (C+1)*4)\n",
    "\n",
    "del eeg_subwin, cov, eeg_ts, imu_subwin, imu_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3631fc0d-4491-499f-85f9-ff213d746566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18826, 1860), (18826, 192), (18826, 2052))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_feat.shape, imu_feat.shape, X_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72f9e9-dd5e-4929-864b-1edfbe28c9db",
   "metadata": {},
   "source": [
    "**Train-test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6dbf5679-296e-4826-af30-9ec245d7a29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train=14935  test=3891 windows\n"
     ]
    }
   ],
   "source": [
    "train_mask = subjects_total <= 20\n",
    "test_mask  = subjects_total > 20\n",
    "\n",
    "X_eeg_tr, X_eeg_te = eeg_feat[train_mask], eeg_feat[test_mask]\n",
    "X_imu_tr, X_imu_te = imu_feat[train_mask], imu_feat[test_mask]\n",
    "X_feat_tr, X_feat_te = X_feat[train_mask], X_feat[test_mask]\n",
    "y_tr, y_te = y_total[train_mask], y_total[test_mask]\n",
    "\n",
    "print(f\"train={len(y_tr)}  test={len(y_te)} windows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85212855-870a-496e-885e-9d0b724a3ffd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### EEG vs IMU vs both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "76df638c-5eaf-4632-8ec3-d9279c990004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4af30f44c84884be5eccb7dbc8a866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917fa7af036c47fb98a4bb6b256848c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a38cb1c75246c793165fa02c3e1f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a1036e682f4675a57770dffa19b846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 13min 46s, sys: 8min 29s, total: 1h 22min 15s\n",
      "Wall time: 4min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MODEL_NAMES = (\"LogReg\", \"kNN\", \"CatBoost\")\n",
    "\n",
    "\n",
    "def instantiate_model(model_nm):\n",
    "    assert model_nm in MODEL_NAMES\n",
    "\n",
    "    if model_nm == \"LogReg\":\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            LogisticRegression(\n",
    "                max_iter=2000,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "        )\n",
    "    elif model_nm == \"kNN\":\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        )\n",
    "    elif model_nm == \"CatBoost\":\n",
    "        return CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    'EEG_ONLY': (X_eeg_tr, X_eeg_te),\n",
    "    'IMU_ONLY': (X_imu_tr, X_imu_te),\n",
    "    'TOTAL': (X_feat_tr, X_feat_te)\n",
    "}\n",
    "traj_results_tr_te = []\n",
    "\n",
    "for dataset_nm, (temp_tr, temp_te) in tqdm(datasets.items()):\n",
    "    for model_nm in tqdm(MODEL_NAMES):\n",
    "        model = instantiate_model(model_nm).fit(temp_tr, y_tr)\n",
    "        model.fit(temp_tr, y_tr)\n",
    "        y_pred = model.predict(temp_te)\n",
    "        \n",
    "        row = {\n",
    "            'model': model_nm,\n",
    "            'data': dataset_nm,\n",
    "            'f1_macro': f1_score(y_te, y_pred, average='macro'),\n",
    "            'f1_weighted': f1_score(y_te, y_pred, average='weighted'),\n",
    "        }\n",
    "        traj_results_tr_te.append(row)\n",
    "    gc.collect()\n",
    "    \n",
    "traj_results_tr_te = pd.DataFrame(traj_results_tr_te)\n",
    "traj_results_tr_te.to_csv('../experiment-results/human-player/trajectory_tr_te.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be5b170-65c1-4e03-89e2-f82d01f27b40",
   "metadata": {},
   "source": [
    "### CasCA grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "213b27c6-9a4b-4582-9ace-a60f2add8887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3aca62a90a488fbcd3399b6dfe9253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e132e41d20774dcd93618d07f3fb9a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 1  d_hid= 2\n",
      "  ➜ d_c= 1  d_hid= 4\n",
      "  ➜ d_c= 1  d_hid= 6\n",
      "  ➜ d_c= 1  d_hid= 8\n",
      "  ➜ d_c= 1  d_hid=10\n",
      "  ➜ d_c= 1  d_hid=12\n",
      "  ➜ d_c= 1  d_hid=14\n",
      "  ➜ d_c= 1  d_hid=16\n",
      "  ➜ d_c= 1  d_hid=30\n",
      "  ➜ d_c= 1  d_hid=50\n",
      "  ➜ d_c= 1  d_hid=70\n",
      "  ➜ d_c= 1  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc62f13df5fd4e7fbd9fab2e91931c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 3  d_hid= 4\n",
      "  ➜ d_c= 3  d_hid= 6\n",
      "  ➜ d_c= 3  d_hid= 8\n",
      "  ➜ d_c= 3  d_hid=10\n",
      "  ➜ d_c= 3  d_hid=12\n",
      "  ➜ d_c= 3  d_hid=14\n",
      "  ➜ d_c= 3  d_hid=16\n",
      "  ➜ d_c= 3  d_hid=30\n",
      "  ➜ d_c= 3  d_hid=50\n",
      "  ➜ d_c= 3  d_hid=70\n",
      "  ➜ d_c= 3  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01767b330b7d44e0948399103e9b2ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 5  d_hid= 6\n",
      "  ➜ d_c= 5  d_hid= 8\n",
      "  ➜ d_c= 5  d_hid=10\n",
      "  ➜ d_c= 5  d_hid=12\n",
      "  ➜ d_c= 5  d_hid=14\n",
      "  ➜ d_c= 5  d_hid=16\n",
      "  ➜ d_c= 5  d_hid=30\n",
      "  ➜ d_c= 5  d_hid=50\n",
      "  ➜ d_c= 5  d_hid=70\n",
      "  ➜ d_c= 5  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7608ad73034ccfbe961af26984bc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 7  d_hid= 8\n",
      "  ➜ d_c= 7  d_hid=10\n",
      "  ➜ d_c= 7  d_hid=12\n",
      "  ➜ d_c= 7  d_hid=14\n",
      "  ➜ d_c= 7  d_hid=16\n",
      "  ➜ d_c= 7  d_hid=30\n",
      "  ➜ d_c= 7  d_hid=50\n",
      "  ➜ d_c= 7  d_hid=70\n",
      "  ➜ d_c= 7  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522013839f26453fa6e7a2cfd401d00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c= 9  d_hid=10\n",
      "  ➜ d_c= 9  d_hid=12\n",
      "  ➜ d_c= 9  d_hid=14\n",
      "  ➜ d_c= 9  d_hid=16\n",
      "  ➜ d_c= 9  d_hid=30\n",
      "  ➜ d_c= 9  d_hid=50\n",
      "  ➜ d_c= 9  d_hid=70\n",
      "  ➜ d_c= 9  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c6c1bc67984550ae875877fd6009e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=11  d_hid=12\n",
      "  ➜ d_c=11  d_hid=14\n",
      "  ➜ d_c=11  d_hid=16\n",
      "  ➜ d_c=11  d_hid=30\n",
      "  ➜ d_c=11  d_hid=50\n",
      "  ➜ d_c=11  d_hid=70\n",
      "  ➜ d_c=11  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4234f04eb58494c9a5fa47f0b44abc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=13  d_hid=14\n",
      "  ➜ d_c=13  d_hid=16\n",
      "  ➜ d_c=13  d_hid=30\n",
      "  ➜ d_c=13  d_hid=50\n",
      "  ➜ d_c=13  d_hid=70\n",
      "  ➜ d_c=13  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f44b02d8e24c109ee982269eb4cf33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=15  d_hid=16\n",
      "  ➜ d_c=15  d_hid=30\n",
      "  ➜ d_c=15  d_hid=50\n",
      "  ➜ d_c=15  d_hid=70\n",
      "  ➜ d_c=15  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463736123cd646259a10b32fad1269ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=20  d_hid=30\n",
      "  ➜ d_c=20  d_hid=50\n",
      "  ➜ d_c=20  d_hid=70\n",
      "  ➜ d_c=20  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8daf285bf4df48ce838273722778cb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=30  d_hid=30\n",
      "  ➜ d_c=30  d_hid=50\n",
      "  ➜ d_c=30  d_hid=70\n",
      "  ➜ d_c=30  d_hid=90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa6dc72f8c94e6db62a49ec1fcffaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ➜ d_c=40  d_hid=50\n",
      "  ➜ d_c=40  d_hid=70\n",
      "  ➜ d_c=40  d_hid=90\n"
     ]
    }
   ],
   "source": [
    "lag_set   = [0]               # ignore lag scan in this experiment\n",
    "grid_d_c  = [1, 3, 5, 7, 9, 11, 13, 15, 20, 30, 40]\n",
    "grid_dhid = [2, 4, 6, 8, 10, 12, 14, 16, 30, 50, 70, 90]\n",
    "\n",
    "# ================================================================\n",
    "# 1)  classifier factory (unchanged except seeds)\n",
    "# ================================================================\n",
    "def make_clfs():\n",
    "    return {\n",
    "        \"LogReg\": LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \"kNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        \"CatBoost\": CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------- helper: run one grid point\n",
    "def run_grid_point(d_c, d_hid):\n",
    "    print(f\"  ➜ d_c={d_c:2d}  d_hid={d_hid:2d}\")\n",
    "\n",
    "    # ❶  fit CaSCA on *train* only\n",
    "    casca = CaSCA(lag_set=[0], d_c=d_c, d_hid=(d_hid, min(d_hid, X_imu_tr.shape[1])))\n",
    "    casca.fit(X_eeg_tr, X_imu_tr)\n",
    "\n",
    "    Ztr, Wtr = casca.transform(X_eeg_tr, X_imu_tr)\n",
    "    Zte, Wte = casca.transform(X_eeg_te, X_imu_te)\n",
    "\n",
    "    FEAT_tr = np.hstack([Ztr, Wtr])\n",
    "    FEAT_te = np.hstack([Zte, Wte])\n",
    "\n",
    "    # ❷  embedding diagnostics (train set)\n",
    "    vif   = compute_vif(FEAT_tr)\n",
    "    cnum  = compute_cond_number(FEAT_tr)\n",
    "    Xrec  = casca.reconstruct_x(X_eeg_tr)\n",
    "    rmse  = compute_rmse(X_eeg_tr, Xrec)\n",
    "    ev    = compute_explained_variance(X_eeg_tr, Xrec)\n",
    "\n",
    "    row = dict(d_c=d_c, d_hid=d_hid,\n",
    "               VIF=vif, log_cond_XTX=cnum,\n",
    "               RMSE_recon=rmse, ExplVar=ev)\n",
    "\n",
    "    # ❸  classifiers\n",
    "    for name, clf in make_clfs().items():\n",
    "        clf.fit(FEAT_tr, y_tr)\n",
    "        y_pred = clf.predict(FEAT_te)\n",
    "\n",
    "        row[f\"{name}_F1w\"] = f1_score(y_te, y_pred, average=\"weighted\")\n",
    "        row[f\"{name}_F1m\"] = f1_score(y_te, y_pred, average=\"macro\")\n",
    "\n",
    "    return row\n",
    "\n",
    "# ================================================================\n",
    "# 3)  Run over grid & collect\n",
    "# ================================================================\n",
    "casca_traj_results = []\n",
    "for d_c in tqdm(grid_d_c):\n",
    "    for d_hid in tqdm(grid_dhid):\n",
    "        if d_c > d_hid:                 # skip invalid combos\n",
    "            continue\n",
    "        casca_traj_results.append(run_grid_point(d_c, d_hid))\n",
    "        gc.collect()\n",
    "\n",
    "casca_traj_df = pd.DataFrame(casca_traj_results)\n",
    "casca_traj_df.to_csv('../experiment-results/human-player/trajectory_casca.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c95c89e-438c-40fa-b220-c6deca77d0a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PurePCA and PureCCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "55c15896-2f78-4a74-8ba1-8434efb398ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b906d991f84f8baa043bdc8517b32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lag_set = [0]               # ignore lag scan in this experiment\n",
    "grid_d_hid = [2, 4, 6, 8, 10, 12, 14, 16, 30, 50, 70, 90]\n",
    "\n",
    "\n",
    "def make_clfs():\n",
    "    return {\n",
    "        \"LogReg\": LogisticRegression(\n",
    "            max_iter=2000,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        \"kNN\": KNeighborsClassifier(n_neighbors=5, n_jobs=4),\n",
    "        \"CatBoost\": CatBoostClassifier(\n",
    "            iterations=200,\n",
    "            verbose=False,\n",
    "            task_type='CPU',\n",
    "            devices='0',\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='MultiClass',\n",
    "            random_state=SEED\n",
    "        )\n",
    "    }\n",
    "\n",
    "# helper: run one grid point\n",
    "def run_grid_point(d_hid, dim_red_model_cls):\n",
    "    # ❶  fit CaSCA on *train* only\n",
    "    dr_model = dim_red_model_cls(d_hid)\n",
    "    dr_model.fit(X_eeg_tr, X_imu_tr)\n",
    "\n",
    "    Ztr, Wtr = dr_model.transform(X_eeg_tr, X_imu_tr)\n",
    "    Zte, Wte = dr_model.transform(X_eeg_te, X_imu_te)\n",
    "\n",
    "    FEAT_tr = np.hstack([Ztr, Wtr])\n",
    "    FEAT_te = np.hstack([Zte, Wte])\n",
    "\n",
    "    # ❷  embedding diagnostics (train set)\n",
    "    vif   = compute_vif(FEAT_tr)\n",
    "    cnum  = compute_cond_number(FEAT_tr)\n",
    "    Xrec  = dr_model.reconstruct_x(X_eeg_tr)\n",
    "    rmse  = compute_rmse(X_eeg_tr, Xrec)\n",
    "    ev    = compute_explained_variance(X_eeg_tr, Xrec)\n",
    "\n",
    "    row = dict(d_hid=d_hid,\n",
    "               VIF=vif, log_cond_XTX=cnum,\n",
    "               RMSE_recon=rmse, ExplVar=ev)\n",
    "\n",
    "    # ❸  classifiers\n",
    "    for name, clf in make_clfs().items():\n",
    "        clf.fit(FEAT_tr, y_tr)\n",
    "        y_pred = clf.predict(FEAT_te)\n",
    "\n",
    "        row[f\"{name}_F1w\"] = f1_score(y_te, y_pred, average=\"weighted\")\n",
    "        row[f\"{name}_F1m\"] = f1_score(y_te, y_pred, average=\"macro\")\n",
    "\n",
    "    return row\n",
    "\n",
    "# ================================================================\n",
    "# 3)  Run over grid & collect\n",
    "# ================================================================\n",
    "traj_pure_results = []\n",
    "for d_hid in tqdm(grid_d_hid):\n",
    "    for dim_red_model_cls in (PurePCA, PureCCA):\n",
    "        row = run_grid_point(d_hid, dim_red_model_cls)\n",
    "        row['din_red_model'] = type(dim_red_model_cls(1)).__name__\n",
    "        traj_pure_results.append(row)\n",
    "    gc.collect()\n",
    "\n",
    "traj_pure_df = pd.DataFrame(traj_pure_results)\n",
    "traj_pure_df.to_csv('../experiment-results/human-player/trajectory_other.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948ebff-5930-4ecf-972c-41ac7acb98f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
