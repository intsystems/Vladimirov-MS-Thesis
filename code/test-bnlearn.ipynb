{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0a79de",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4eb71810-de7e-4cdb-bb37-375598001fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, re, collections, statistics, math, lzma, tempfile, shutil\n",
    "import zstandard as zstd\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.independence_tests import cmiknn, parcorr\n",
    "from tigramite.pcmci import PCMCI\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "from causallearn.search.ScoreBased.GES import ges\n",
    "from causallearn.search.ScoreBased.ExactSearch import bic_exact_search\n",
    "from causallearn.utils.GraphUtils import GraphUtils\n",
    "\n",
    "from pyEDM import EmbedDimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bbea348",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'DejaVu Serif'\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['lines.markersize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 24\n",
    "plt.rcParams['ytick.labelsize'] = 24\n",
    "plt.rcParams['legend.fontsize'] = 24\n",
    "plt.rcParams['axes.titlesize'] = 30\n",
    "plt.rcParams['axes.labelsize'] = 24\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 7)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96bbac-afc4-4e38-a1e8-86d6fb0060b3",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ee59b1-c47e-4fad-ad69-3c4338aa3f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arth150_edges.csv      magic-irri_edges.csv\t mehra-original.rds\n",
      "arth150.rds\t       magic-irri.rds\t\t mehra-original_sim5000.csv\n",
      "arth150_sim5000.csv    magic-irri_sim5000.csv\t motor_ccm_df.csv\n",
      "gen_ci_signals_01.npz  mehra-original_edges.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf30dcf-c809-40dc-8cdb-461e58d62723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/magic-irri_sim5000.csv')\n",
    "edges_true = pd.read_csv('../data/magic-irri_edges.csv')\n",
    "G_true = nx.from_pandas_edgelist(edges_true, \"from\", \"to\", create_using=nx.DiGraph())\n",
    "vertices = list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c6df6e-296c-4ce2-8d90-a6c02cf6d0b0",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddede98-5531-493e-b1af-fbe0a78f16b7",
   "metadata": {},
   "source": [
    "### PCMCI with partial corr. ind. tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f41c4b1-4a46-44d6-b1b8-f668dc4bf269",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcmci = PCMCI(dataframe=pp.DataFrame(df.values),\n",
    "              cond_ind_test=parcorr.ParCorr(significance=\"analytic\"))\n",
    "pcmci_res = pcmci.run_pcmci(tau_max=0, pc_alpha=0.01)\n",
    "\n",
    "lag0_edges  = pcmci_res['graph'][..., 0]\n",
    "\n",
    "edges_pcmci = [\n",
    "    (vertices[p], vertices[c])\n",
    "    for p in range(len(vertices))\n",
    "    for c in range(len(vertices))\n",
    "    if lag0_edges[p, c]                     # True ⇒ directed edge p → c\n",
    "]\n",
    "\n",
    "G_pcmci = nx.DiGraph(edges_pcmci)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b850f0-90ec-47f3-8031-e966c6402ed4",
   "metadata": {},
   "source": [
    "### PCMCI with MI cond. ind. tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f026b7-41d2-4c9f-868d-22bceb810040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-04-25 18:52:00 [numba.core.transforms] > INFO     > finding looplift candidates\n"
     ]
    }
   ],
   "source": [
    "pcmci_mi = PCMCI(dataframe=pp.DataFrame(df.values),\n",
    "              cond_ind_test=cmiknn.CMIknn())\n",
    "pcmci_res_mi = pcmci_mi.run_pcmci(tau_max=0, pc_alpha=0.01)\n",
    "\n",
    "lag0_edges_mi  = pcmci_res_mi['graph'][..., 0]\n",
    "\n",
    "edges_pcmci_mi = [\n",
    "    (vertices[p], vertices[c])\n",
    "    for p in range(len(vertices))\n",
    "    for c in range(len(vertices))\n",
    "    if lag0_edges_mi[p, c]                     # True ⇒ directed edge p → c\n",
    "]\n",
    "\n",
    "G_pcmci_mi = nx.DiGraph(edges_pcmci_mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829534a-13e3-4d86-b812-fa7f07d7205e",
   "metadata": {},
   "source": [
    "### PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db2b256d-4657-40b5-8a82-68a170144c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_out  = pc(df.values, alpha=0.01, stable=True)\n",
    "pc_out.to_nx_graph()\n",
    "\n",
    "mapping = {i: name for i, name in enumerate(vertices)}\n",
    "pc_graph = nx.relabel_nodes(pc_out.nx_graph, mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92961f43-d595-45b9-980c-55f31fb21d0f",
   "metadata": {},
   "source": [
    "### GEM [dropped due to long time execution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6298537-57bd-4e2b-adc5-3705c65dce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ges_out = ges(df.values, score_func=\"local_score_BIC\", node_names=vertices)\n",
    "# dag_est, search_stats = bic_exact_search(df.values, max_parents=10)\n",
    "\n",
    "# G_gem   = GraphUtils.to_nx_graph(ges_out[\"G\"], node_order=vertices, label_edges=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab262a-59a8-439e-b0af-e313a5c343a5",
   "metadata": {},
   "source": [
    "### ICA -> SSR -> MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c75a80a1-a1f1-4226-8b7f-a224f5978bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastICA → (5000, 10) (64, 10)\n",
      "SSR   → (4998, 30)\n",
      "MI matrix shape: (30, 30)\n"
     ]
    }
   ],
   "source": [
    "def run_fastica(df, n_components=None, random_state=0):\n",
    "    \"\"\"\n",
    "    FastICA on standardized data.\n",
    "    Returns:\n",
    "        S : pd.DataFrame, shape (T, n_components)  – independent sources\n",
    "        A : mixing matrix, shape (p, n_components)\n",
    "    \"\"\"\n",
    "    X = (df - df.mean()) / df.std()           # z-score\n",
    "    ica = FastICA(n_components=n_components, random_state=random_state)\n",
    "    S = ica.fit_transform(X.values)           # independent comps\n",
    "    cols = [f\"S{i}\" for i in range(S.shape[1])]\n",
    "    return pd.DataFrame(S, columns=cols, index=df.index), ica.mixing_\n",
    "\n",
    "\n",
    "def build_ssr(df_sources, max_lag):\n",
    "    \"\"\"\n",
    "    State-space reconstruction (Takens embedding) for each source.\n",
    "    Returns a lagged DataFrame with columns S{k}_t-{lag}.\n",
    "    \"\"\"\n",
    "    lagged = {}\n",
    "    for col in df_sources.columns:\n",
    "        for lag in range(max_lag + 1):\n",
    "            lagged[f\"{col}_t-{lag}\"] = df_sources[col].shift(lag)\n",
    "    return pd.DataFrame(lagged).dropna()\n",
    "\n",
    "\n",
    "def mi_matrix(df_lagged, n_neighbors=8, random_state=0):\n",
    "    \"\"\"\n",
    "    Pairwise MI (Kraskov estimator via sklearn) – symmetric matrix.\n",
    "    Returns:\n",
    "        MI : pd.DataFrame, shape (m, m)\n",
    "    \"\"\"\n",
    "    cols = df_lagged.columns\n",
    "    m    = len(cols)\n",
    "    M    = np.zeros((m, m))\n",
    "    for i, y in enumerate(cols):\n",
    "        M[i, :] = mutual_info_regression(\n",
    "            df_lagged[cols], df_lagged[y],\n",
    "            n_neighbors=n_neighbors,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    return pd.DataFrame(M, index=cols, columns=cols)\n",
    "\n",
    "\n",
    "def mi_to_edge_list(mi_df, thresh):\n",
    "    \"\"\"\n",
    "    Turn MI matrix into an edge list above a threshold.\n",
    "    \"\"\"\n",
    "    edges = [(i, j, mi_df.loc[i, j])\n",
    "             for i, j in it.combinations(mi_df.columns, 2)\n",
    "             if mi_df.loc[i, j] >= thresh]\n",
    "    return sorted(edges, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "\n",
    "S, A = run_fastica(df, n_components=10, random_state=SEED)  # auto = p comps\n",
    "print(\"FastICA →\", S.shape, A.shape)\n",
    "\n",
    "# 2. Takens embedding (lag 0..3)  ----------------------------------\n",
    "SSR = build_ssr(S, max_lag=2)\n",
    "print(\"SSR   →\", SSR.shape)\n",
    "\n",
    "# 3. pairwise MI matrix  ------------------------------------------\n",
    "MI = mi_matrix(SSR, n_neighbors=8)\n",
    "print(\"MI matrix shape:\", MI.shape)\n",
    "\n",
    "# # 4. choose a threshold and get edges\n",
    "# tau = 0.05                                  # pick manually or via FDR\n",
    "# edges = mi_to_edge_list(MI, thresh=tau)\n",
    "# print(f\"{len(edges)} edges with MI ≥ {tau}\")\n",
    "# print(edges[:10])                           # top-10 strongest links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f3d74a6-448d-4d0b-ac57-f845fa348474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(MI);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f979de-cd16-4693-b308-34344ae92bbf",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8670740b-f073-4567-9592-e3466ea9a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyEDM import EmbedDimension, Embed, MakeBlock\n",
    "\n",
    "\n",
    "best_emb_mapping = dict.fromkeys(S.columns, 0)\n",
    "for col in S.columns.values:\n",
    "    for trg in S.columns.values:\n",
    "        if col == trg:\n",
    "            continue\n",
    "            \n",
    "        emb_to_rho = EmbedDimension(\n",
    "            S,\n",
    "            columns=col,\n",
    "            target=trg,\n",
    "            maxE=7,\n",
    "            embedded=False,\n",
    "            lib='1 4500',\n",
    "            pred='4501 5000',\n",
    "            showPlot=False\n",
    "        )\n",
    "        emb_to_rho = emb_to_rho.round(3)\n",
    "        best_emb_dim = emb_to_rho.loc[emb_to_rho['rho'].argmax(), 'E']\n",
    "        best_emb_mapping[col] += best_emb_dim\n",
    "\n",
    "    best_emb_mapping[col] = int(round(best_emb_mapping[col] / len(best_emb_mapping)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e20b2716-80dc-4e35-a7d6-8f9d3ce14996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S0': 3,\n",
       " 'S1': 3,\n",
       " 'S2': 3,\n",
       " 'S3': 3,\n",
       " 'S4': 4,\n",
       " 'S5': 3,\n",
       " 'S6': 3,\n",
       " 'S7': 4,\n",
       " 'S8': 3,\n",
       " 'S9': 3}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_emb_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "832940f1-3b34-4e9d-8ac5-7b4fc6718f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prf1(pred, true):\n",
    "    TP = len(set(pred.edges) & set(true.edges))\n",
    "    FP = len(set(pred.edges) - set(true.edges))\n",
    "    FN = len(set(true.edges) - set(pred.edges))\n",
    "    prec = TP/(TP+FP) if TP+FP else 0.\n",
    "    rec  = TP/(TP+FN) if TP+FN else 0.\n",
    "    f1   = 2*prec*rec/(prec+rec) if prec+rec else 0.\n",
    "    return prec, rec, f1\n",
    "\n",
    "\n",
    "def shd(pred, true):\n",
    "    undirected_diff = set(map(frozenset, pred.edges)) ^ set(map(frozenset, true.edges))\n",
    "    misdir = set(pred.edges) & set((v,u) for u,v in true.edges)\n",
    "    return len(undirected_diff) + len(misdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8323c46e-dfe6-4f1a-9999-269fe15cd600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCMCI   PR=0.182  RC=0.990  F1=0.307  SHD=279\n",
      "PC      PR=0.673  RC=0.706  F1=0.689  SHD=40\n"
     ]
    }
   ],
   "source": [
    "for name, G_hat in {\"PCMCI\": G_pcmci, \"PC\": pc_graph}.items():\n",
    "    p,r,f = prf1(G_hat, G_true)\n",
    "    print(f\"{name:<6}  PR={p:5.3f}  RC={r:5.3f}  F1={f:5.3f}  SHD={shd(G_hat,G_true)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72400054-da04-43ef-8545-2cfab7ad32b8",
   "metadata": {},
   "source": [
    "For the dataset MAGIC-IRRI PC algorithm works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ac12c-99c8-44af-a474-f58a4189b4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248cfaa0-edae-4173-989c-31ae7ecf562b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da9242a3-cb70-45c6-800c-a06a07532ad2",
   "metadata": {},
   "source": [
    "\n",
    "Great. I’ll write a PhD-level academic review of causal identification methods focused on DAG-based frameworks. The review will emphasize mathematical formulations, algorithmic complexity, assumptions (e.g., faithfulness, causal sufficiency), and classification (e.g., score-based, constraint-based, hybrid). It will include BibTeX-style citations.\n",
    "\n",
    "I’ll let you know once the review is ready.\n",
    "\n",
    "# Causal Identification in DAG-based Frameworks: A Review\n",
    "\n",
    "## Introduction  \n",
    "Causal discovery seeks to infer cause-effect relationships from data by leveraging assumptions encoded in directed acyclic graphs (DAGs). A **DAG-based causal framework** represents variables as nodes and causal influences as directed edges, providing a graphical model for the data-generating process ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=d,same%20as%20those%20that%20can)) ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=Several%20DAGs%20can%20describe%20exactly,a%20partially%20directed%20acyclic%20graph)). Unlike purely associative models, DAGs capture *structural* information: missing edges imply conditional independence, and directed paths encode causal ordering. **Causal identification** refers to determining causal structure or effects from observational data under appropriate assumptions. This review surveys major DAG-based causal identification methods, analyzing their mathematical foundations, computational complexity, algorithmic type (constraint-based, score-based, hybrid, etc.), assumptions (e.g. Markov and faithfulness conditions), and key invariance properties. We cover classical algorithms (PC, FCI, GES), recent advances (GSP, NOTEARS, ICP), and others, highlighting how each balances identifiability and practical estimation. We also distinguish *identifiability* (what can be determined in principle from infinite data and assumptions) from *estimation* (what is learned from finite samples). Throughout, we use formal definitions and representative formulas to ensure rigor, and we cite foundational works such as Pearl’s *do*-calculus for causal effects ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=d,same%20as%20those%20that%20can)), Spirtes et al.’s constraint-based discovery algorithms, and Peters et al.’s contributions on invariance and causal inference.\n",
    "\n",
    "## Preliminaries: Causal DAGs and Assumptions  \n",
    "**Causal DAGs:** A DAG $G=(V,E)$ consists of nodes $V=\\{X_1,\\dots,X_d\\}$ and directed edges $E$ with no cycles. Each node $X_i$ has a set of parents $\\text{Pa}_i$ (its direct causes in the graph). A causal DAG can be associated with a Structural Equation Model (SEM) where each variable is a function of its parents plus an independent noise: \n",
    "$$X_i = f_i(\\text{Pa}_i) + \\varepsilon_i,$$ \n",
    "with $\\varepsilon_i$ noise terms (assumed independent). The **causal Markov condition** holds: each variable is independent of its non-descendants given its parents ([CSCE580 GuestLecture1 -- BNLearning](https://pooyanjamshidi.github.io/csce580/lectures/CSCE580-GuestLecture--BNLearning.pdf#:~:text=Markov%20Condition%20%26%20Faithfulness%20Assumption,specified%20by%20the%20Markov%20condition)) ([CSCE580 GuestLecture1 -- BNLearning](https://pooyanjamshidi.github.io/csce580/lectures/CSCE580-GuestLecture--BNLearning.pdf#:~:text=%E2%80%A2%20Implication%3A%20faithfulness%20is%20assumed,the%20learned%20graph%20is%20correct)). Formally, for every $X_i$, \n",
    "$$X_i \\perp\\!\\!\\!\\perp \\text{NonDescendants}(X_i) \\mid \\text{Pa}_i,$$ \n",
    "which implies a set of conditional independence (CI) relations in any distribution that factorizes according to DAG $G$ ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=d,same%20as%20those%20that%20can)). These CI relations can be read from $G$ via **d-separation** (Pearl, 2000). If a set $S$ of nodes d-separates $X$ and $Y$ in $G$, then $X \\perp\\!\\!\\!\\perp Y \\mid S$ in every distribution that respects $G$ ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=d,same%20as%20those%20that%20can)). \n",
    "\n",
    "**Markov Equivalence and Identifiability:** Multiple DAGs can encode the same CI relations. Such DAGs are **Markov equivalent** and form an equivalence class, often represented by a completed partially directed acyclic graph (CPDAG) ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=Several%20DAGs%20can%20describe%20exactly,a%20partially%20directed%20acyclic%20graph)). All DAGs in a Markov equivalence class share the same skeleton (undirected structure) and the same v-structures (induced colliders). This fundamental ambiguity implies that **observational data alone (with only CI tests) can at best identify an equivalence class of DAGs**, not a unique DAG, unless further assumptions are made. In particular, without additional assumptions or interventions, causal structure is only *partially identifiable*: one can recover invariant features (like the CPDAG) but not distinguish DAGs within the same class. Key additional assumptions can improve identifiability, as discussed below.\n",
    "\n",
    "**Causal Sufficiency:** A standard assumption is that the observed set of variables is *causally sufficient*, meaning there are no unobserved confounders (common causes) among them ([CSCE580 GuestLecture1 -- BNLearning](https://pooyanjamshidi.github.io/csce580/lectures/CSCE580-GuestLecture--BNLearning.pdf#:~:text=)). Under causal sufficiency, every d-separation in the true DAG corresponds to an observable conditional independence in the data. If this fails (latent variables exist), specialized algorithms (e.g. FCI) are needed to correctly identify causal relations. We will highlight which methods assume causal sufficiency and which relax it.\n",
    "\n",
    "**Faithfulness:** Another crucial assumption is **faithfulness**, which posits that the only conditional independencies in the data are those implied by the DAG’s d-separation; i.e. no cancellations or coincidences lead to spurious independencies ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=according%20to%20G,separation)). Formally, a distribution $P$ is faithful to DAG $G$ if for every disjoint sets $X, Y, S$: \n",
    "$$X \\perp\\!\\!\\!\\perp Y \\mid S \\implies X \\text{ and } Y \\text{ are d-separated by } S \\text{ in } G,$$ \n",
    "and vice versa ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=according%20to%20G,separation)). Faithfulness is stronger than the Markov condition and essentially ensures a one-to-one correspondence between graphical d-separations and true independencies. Nearly all constraint-based methods (and many score-based ones) require faithfulness (or variants like *restricted-faithfulness*) to be consistent ([](https://pages.stat.wisc.edu/~raskutti/publication/SPAlgorithm.pdf#:~:text=independence%20testing,SP%29%20algorithm%20are)). If faithfulness holds and we have infinite data (an “independence oracle”), the DAG’s Markov equivalence class is identifiable from observational data (Spirtes et al., 2000). Violations of faithfulness (e.g. nearly canceling paths) can degrade algorithms’ performance ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=whether%20their%20inferences%20can%20be,short%2C%20and%20can%20be%20violated)) ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=Many%20algorithms%20for%20causal%20inference,violations%20of%20faithfulness)), a point we revisit when discussing robust methods like GSP.\n",
    "\n",
    "**Identifiability vs. Estimation:** It is important to distinguish theoretical identifiability from practical estimation. **Identifiability** means that a causal quantity (structure or effect) is *in principle uniquely determined* by the population distribution under given assumptions. For example, with faithfulness, the CPDAG of the true DAG is identifiable from an infinite sample of the joint distribution. Likewise, a causal effect $P(Y\\mid do(X))$ is *identifiable* from observational data if it can be expressed in terms of the observed distribution (Pearl’s *do*-calculus provides criteria for this ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=d,same%20as%20those%20that%20can))). By contrast, **estimation** refers to the finite-sample procedure of inferring these quantities from data, which involves statistical uncertainty, search heuristics, and error control. An algorithm can be *consistent* (converging to the correct structure as sample size grows) even if finite-sample estimates might be wrong due to noise. Throughout the review, we note when a method is provably consistent (usually under faithfulness and sufficiency) and how it handles the gap between infinite-data identifiability and finite-data estimation (e.g. through scoring penalties or conservative tests).\n",
    "\n",
    "## Constraint-Based Algorithms\n",
    "\n",
    "Constraint-based causal discovery algorithms infer the graph structure by testing conditional independence constraints implied by the causal Markov and faithfulness assumptions. They typically have two phases: (1) **Skeleton identification** – determine which pairs of variables are adjacent in the graph by finding a separating set $S$ such that $X \\perp\\!\\!\\!\\perp Y \\mid S$ for each non-adjacent pair $(X,Y)$. (2) **Edge orientation** – orient the undirected skeleton into a DAG (or pattern) by identifying v-structures and applying logical orientation rules. The output is a graph (often a CPDAG or a partial ancestral graph) that represents the Markov equivalence class of the causal model consistent with the observed independencies.\n",
    "\n",
    "### PC Algorithm (Peter–Clark Algorithm)  \n",
    "The **PC algorithm** (named after its authors Peter Spirtes and Clark Glymour) is a prototypical constraint-based method ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=Many%20algorithms%20for%20causal%20inference,violations%20of%20faithfulness)). PC assumes causal sufficiency and faithfulness. It starts with a complete undirected graph on all $n$ variables and proceeds to remove edges that are rejected by conditional independence (CI) tests ([Challenges and Opportunities with Causal Discovery Algorithms: Application to Alzheimer’s Pathophysiology | Scientific Reports](https://www.nature.com/articles/s41598-020-59669-x#:~:text=FCI%20constructs%20a%20causal%20graph,in%20detail%20elsewhere9%20%2C%2036)). In detail, PC performs CI tests in increasing order of conditioning set size: first remove edges for marginal independencies, then for conditional independencies given one variable, then two, etc. ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=algorithm%20consists%20of%20three%20steps,subset%20S%20of%20the%20remaining)). When an edge $X\\text{---}Y$ is removed, the conditioning set that led to independence is recorded as a separating set for $(X,Y)$. After obtaining a skeleton (undirected graph) with no spurious edges, PC identifies **unshielded triples** (configurations $X - Z - Y$ where $X$ and $Y$ are not directly connected) and orients $X \\rightarrow Z \\leftarrow Y$ if $Z$ was not in the separating set of $X$ and $Y$. Such oriented triples are *v-structures* (colliders) and are guaranteed by faithfulness to be present in the true DAG ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=d,same%20as%20those%20that%20can)). Finally, PC applies a series of orientation propagation rules (e.g. arrow propagation, avoiding cycles or new v-structures) to direct as many remaining edges as possible without contradicting earlier results (Spirtes et al., 2000). The output of PC is a CPDAG representing the equivalence class of DAGs consistent with the tests.\n",
    "\n",
    "*Mathematical Properties:* Under the assumptions of causal Markov, acyclicity, causal sufficiency, and faithfulness, the PC algorithm is **sound and complete**: given perfect CI information, it will recover the true Markov equivalence class (the correct skeleton and v-structures) ([[PDF] Order-Independent Constraint-Based Causal Structure Learning](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=Learning%20jmlr.org%20%20The%20PC,be%20faithful%20to%20a)). In the large-sample limit, with an asymptotically valid CI test, PC is **consistent** (Spirtes et al., 2000), meaning the probability that PC outputs the true CPDAG approaches 1 as sample size $\\to\\infty$. However, if the distribution nearly violates faithfulness (almost independencies), PC’s decisions can become unreliable, motivating variants like the “Conservative PC” (CPC) algorithm that avoids overconfident orientations in such cases.\n",
    "\n",
    "*Complexity:* A key drawback of PC is its computational complexity. In the worst case (a fully connected graph), PC may need to test independence for conditioning sets up to size $n-2$, which is combinatorially many. The worst-case time complexity is super-exponential in $n$ (NP-hard in general). However, complexity can be parameterized by the graph’s maximum degree $k$: the number of CI tests grows roughly as $O(n^{k})$ ([CSCE580 GuestLecture1 -- BNLearning](https://pooyanjamshidi.github.io/csce580/lectures/CSCE580-GuestLecture--BNLearning.pdf#:~:text=Complexity%20%E2%80%A2%20The%20complexity%20of,case%20running%20time%20of)) (exponential in $k$ but polynomial in $n$ if $k$ is moderate). In practice, if the true graph is sparse, PC is feasible; for dense graphs, PC becomes very slow. There are improvements like parallel PC and order-independent PC (PC-stable) to address variable ordering effects and efficiency ([CSCE580 GuestLecture1 -- BNLearning](https://pooyanjamshidi.github.io/csce580/lectures/CSCE580-GuestLecture--BNLearning.pdf#:~:text=%E2%80%A2%20PC%20algorithm%20has%20a,case%20running%20time%20of)) ([CSCE580 GuestLecture1 -- BNLearning](https://pooyanjamshidi.github.io/csce580/lectures/CSCE580-GuestLecture--BNLearning.pdf#:~:text=Complexity%20%E2%80%A2%20The%20complexity%20of,case%20running%20time%20of)).\n",
    "\n",
    "*Assumptions:* **Type:** Constraint-based. **Required assumptions:** (i) *Causal Markov* and *Faithfulness* – ensures a one-to-one link between separations in the DAG and independencies in data ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=according%20to%20G,separation)). (ii) *Causal Sufficiency* – no hidden confounders; violations require using algorithms like FCI instead. (iii) Typically assumes i.i.d. sampling and possibly general position (no exact cancelling of independencies beyond faithfulness). **Key invariances:** Under these assumptions, the CI relations used by PC are *invariant properties of the underlying causal DAG*. In particular, if the distribution changes by interventions on other variables but the causal structure remains, those invariances (independencies) would persist. The PC algorithm’s output (a CPDAG) thus represents all DAGs that share the same invariant independence structure. PC is also known to have an order-dependence issue (the result can depend on the order in which variables are considered for testing), which is remedied by the *PC-stable* modification ensuring invariance under variable ordering.\n",
    "\n",
    "- **Type:** Constraint-based structure learning (CI testing).  \n",
    "- **Complexity:** Worst-case $O(n^{k})$ CI tests (exponential in max degree $k$); polynomial time for sparse graphs; typically exponential in general.  \n",
    "- **Assumptions:** Causal sufficiency (no latent confounders); Causal Markov condition; Faithfulness (or a variant, e.g. strong-faithfulness) ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=according%20to%20G,separation)); i.i.d. data.  \n",
    "- **Invariances:** Leverages invariant conditional independencies implied by the DAG. Identifies the Markov equivalence class (invariant under rotations within that class). Output is a CPDAG capturing all invariant causal features (e.g. v-structures) shared by equivalent DAGs.\n",
    "\n",
    "### Fast Causal Inference (FCI) Algorithm  \n",
    "The **FCI algorithm** (Spirtes et al., 1995; Richardson & Spirtes, 2002) extends the PC approach to relax the causal sufficiency assumption. FCI can handle hidden variables (latent confounders) and even selection bias (in a general version called *Selection-Biased FCI*). The output of FCI is not a DAG or CPDAG, but a **Partial Ancestral Graph (PAG)**, which represents an equivalence class of DAGs *with latent variables and/or selection bias*. Edges in a PAG can be directed ($\\to$), bidirected ($\\leftrightarrow$, indicating the presence of a latent confounder between two nodes), or partially oriented (marked with a circle $\\circ$ at one end to denote ambiguity). The PAG encodes conditional independence relations of a **maximally extended DAG model** (allowing latent variables) that fits the data.\n",
    "\n",
    "*How FCI works:* FCI’s first phase mirrors PC’s skeleton identification but uses a broader notion of separation (called m-separation in an augmented graph) appropriate for latent variable models ([Comparison of strategies for scalable causal discovery of latent ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC6096780/#:~:text=The%20standard%20constraint,However%2C%20the%20word)). Starting from a complete graph on observed variables, FCI removes edges by testing conditional independencies just like PC, but it must consider that an independence could be due to a path blocked by an unobserved confounder. After obtaining an initial skeleton among observed variables, FCI enters a more complex orientation phase. It first orients obvious colliders (unshielded triples $X\\circ\\!\\!-\\!\\!\\circ Z \\circ\\!\\!-\\!\\!\\circ Y$ where $Z$ was not in the separating set for $X$ and $Y$, so $X \\to Z \\leftarrow Y$). Next, a series of rules (called *orientation rules* R1–R10 in the literature) propagate directions taking into account the possibility of latent variables. For example, if $X - Y$ remains undirected but $X$ is d-separated from $Y$ by some set that does **not** include one of them, then that edge is covered by a latent structure and is marked with $\\leftrightarrow$. The logic is intricate but ensures that if an edge cannot be oriented definitively as $X\\to Y$ or $Y\\to X$, it remains ambiguous (circle ends). The final PAG encodes all invariant causal features discernible despite latent confounding. Notably, FCI can discover the existence of a latent common cause (by producing a bidirected $X \\leftrightarrow Y$) even though neither the latent nor the exact causal direction between $X$ and $Y$ is identified.\n",
    "\n",
    "*Complexity:* FCI has similar complexity to PC in terms of the order of CI tests (also exponential in worst case). Each CI test in FCI is analogous to PC’s, though the conditioning sets considered can be larger due to the presence of possible latent structures. The additional orientation rules add polynomial overhead. In practice, FCI is slower than PC on the same number of variables, but still polynomial for bounded-degree graphs ([](https://jmlr.org/papers/volume15/colombo14a/colombo14a.pdf#:~:text=FCI%2C%20in%20the%20sense%20that,but%20is%20slightly%20less)). There is also a “Really Fast Causal Inference” (RFCI) variant that simplifies some tests to improve speed at the cost of some ambiguity in the output.\n",
    "\n",
    "*Assumptions:* **Type:** Constraint-based (with extended independence tests for hidden variables). **Assumptions:** Causal Markov and faithfulness still hold but in the sense of a maximal ancestral graph (MAG) which accounts for latents. No assumptions of causal sufficiency – FCI explicitly allows latent confounders and selection bias (if using a further generalized FCI). Still assumes no cycles (DAG underlying), and typically assumes the distribution is *semi-faithful* (no independencies beyond those implied by some DAG with latents). **Key invariances:** FCI leverages the fact that even with unobserved confounders, certain conditional independencies are invariant and can be detected. For example, if $U$ is an unobserved cause of $X$ and $Y$, then $X$ and $Y$ will be marginally dependent but conditionally independent given any variable that “separates” the path (like a collider on that path). FCI finds invariant features like “Y-structures” (a pattern indicating no confounding on a collider) ([Challenges and Opportunities with Causal Discovery Algorithms: Application to Alzheimer’s Pathophysiology | Scientific Reports](https://www.nature.com/articles/s41598-020-59669-x#:~:text=Other%20than%20the%20searching%20strategies,it%20encounters%20a%20%E2%80%9CY%E2%80%9D%20structure17)). The PAG output by FCI represents all causally compatible models, so any edge oriented as $X\\to Y$ in the PAG means *in every DAG with some latents that fits the data, $X$ is an ancestor of $Y$*. This invariance makes FCI powerful: it only commits to causal directions that are reliably identified even in presence of hidden variables ([Challenges and Opportunities with Causal Discovery Algorithms: Application to Alzheimer’s Pathophysiology | Scientific Reports](https://www.nature.com/articles/s41598-020-59669-x#:~:text=Other%20than%20the%20searching%20strategies,it%20encounters%20a%20%E2%80%9CY%E2%80%9D%20structure17)).\n",
    "\n",
    "- **Type:** Constraint-based (latent-variable-capable).  \n",
    "- **Complexity:** Similar order to PC (exponential in high connectivity); often slower due to more complex orientation rules.  \n",
    "- **Assumptions:** No causal sufficiency needed (handles hidden variables); requires extended faithfulness (no unaccounted independencies) and causal Markov for a DAG with possibly hidden nodes; acyclicity and no selection bias (unless extended FCI with bias handling).  \n",
    "- **Invariances:** Identifies structural features invariant under unobserved confounding. Independence and separation are considered in a MAG/PAG context. The output PAG encodes equivalence classes of DAGs with latents that share the same invariant independencies.\n",
    "\n",
    "## Score-Based Algorithms\n",
    "\n",
    "Score-based methods approach causal discovery as an optimization problem. They define a **score** (usually related to the goodness-of-fit of a DAG to data, penalized for complexity) and then search for the DAG structure that maximizes this score. Typically, the score is a log-likelihood (or marginal likelihood) of the data under a Bayesian network model given the DAG, combined with a penalty to avoid overfitting (e.g. BIC or MDL penalty). Unlike constraint-based methods that rely on local CI decisions, score-based algorithms consider global model fit. They implicitly encode the Markov condition in the scoring function: a candidate DAG $G$ gets a high score if it can describe the joint distribution efficiently. A major advantage is the ability to incorporate **soft** measures of trade-off (via the score) and handle statistical uncertainty more naturally. However, the search space of DAGs is super-exponential in $n$, so algorithms must employ heuristics (greedy search, dynamic programming, MCMC, etc.) since exact optimization is NP-hard.\n",
    "\n",
    "### Greedy Equivalence Search (GES)  \n",
    "**GES** (Greedy Equivalence Search) is a classic score-based algorithm introduced by Chickering (2002). It searches through the space of DAG *equivalence classes* (represented as CPDAGs) by adding or removing edges in a greedy hill-climbing fashion to optimize a score (typically BIC score). GES consists of two phases ([Challenges and Opportunities with Causal Discovery Algorithms: Application to Alzheimer’s Pathophysiology | Scientific Reports](https://www.nature.com/articles/s41598-020-59669-x#:~:text=,algorithm)): a **forward phase** that starts from an empty graph and greedily adds the edge that most increases the score (maximizes data likelihood with penalty) until no addition improves the score; and a **backward phase** that starts from the result of forward phase and greedily removes an edge that most improves the score (usually, further increases BIC by removing unnecessary connections) until no removal improves it. This forward-backward strategy aims to find a local optimum of the score that is a good approximation to the true structure. Importantly, each addition or removal is actually performed on the current *equivalence class* (CPDAG) rather than a specific DAG, ensuring that GES respects the score equivalence property (two DAGs in the same Markov equivalence class have the same score, given certain scoring criteria like BIC or BDeu). Thus, GES moves among equivalence classes with guaranteed score improvement and outputs a CPDAG as the final result.\n",
    "\n",
    "*Theoretical properties:* GES is proven to be **asymptotically consistent** under standard assumptions (Chickering, 2002). If the data distribution is generated by a DAG $G^*$ that satisfies the score’s assumptions (e.g. a joint Gaussian or multinomial distribution for BIC) and if the scoring criterion is *score-equivalent* and *consistent*, then in the limit of infinite data, GES will find the true equivalence class $G^*$ (more precisely, a global optimum of the score which corresponds to $G^*$). The score-equivalence means if two DAGs are Markov equivalent, they have the same score (BIC and Bayesian scores have this property), allowing the search to be well-defined on equivalence classes. GES is also *complete* in the sense that its forward-backward greedy strategy can reach the global optimum (it’s an **inclusion optimal** algorithm: adding all true edges then removing false ones can achieve the true structure (Meek, 1997; Chickering, 2002)). However, the greedy nature means in finite samples it might get stuck in a local optimum that is not global, although in practice using equivalence classes mitigates some local traps.\n",
    "\n",
    "*Complexity:* The worst-case time complexity of naive GES is super-exponential since the space is huge. But heuristically, each step of adding or removing tests a number of candidate edges polynomial in $n$ (checking score deltas for each possible edge addition or removal). The number of steps can be large, but in many cases GES is faster than PC for moderate $n$ because it doesn’t explicitly test high-order conditional independencies — instead, it fits local scores. In high-dimensional settings, an optimized version called FGES (Fast GES) uses clever data structures and parallelization to handle thousands of variables ([Challenges and Opportunities with Causal Discovery Algorithms: Application to Alzheimer’s Pathophysiology | Scientific Reports](https://www.nature.com/articles/s41598-020-59669-x#:~:text=,algorithm)). FGES maintains the same theoretical output as GES but scales better in practice.\n",
    "\n",
    "*Assumptions:* **Type:** Score-based (greedy search in model space). **Assumptions:** (i) The data are generated by a DAG model within the considered class (usually discrete or Gaussian BN). (ii) Causal sufficiency (no latents) is assumed by vanilla GES, as hidden confounders would violate the DAG model’s assumptions (though extensions exist to handle latent structure by scoring e.g. maximal DAGs or using specially designed scores). (iii) The true distribution satisfies the *faithfulness* or *noise-free* assumption implicitly, so that the highest-scoring DAG corresponds to the true causal structure (no parameter degeneracies that make a wrong structure fit as well). (iv) Score equivalence: the scoring function should not prefer one DAG over an equivalent DAG, which holds for many standard choices (BIC, BDe). **Key invariances:** GES relies on *likelihood score invariances*: all DAGs in the same Markov equivalence class will (in the large sample limit) have equal scores if the score is truly score-equivalent. Therefore, the greedy search in equivalence class space is valid. Moreover, like constraint methods, GES will always identify correct v-structures under infinite data (because only the true orientation yields a better score due to likelihood being higher for the collider than for either causal chain) ([Challenges and Opportunities with Causal Discovery Algorithms: Application to Alzheimer’s Pathophysiology | Scientific Reports](https://www.nature.com/articles/s41598-020-59669-x#:~:text=the%20BIC,higher%20likelihood%20than%20other%20structures)). In fact, GES typically orients colliders correctly because a v-structure (A→C←B) allows a better fit than a chain (A→C→B) when both A and B influence C independently. Another invariance is that GES’s result does not depend on the variable ordering (it searches globally). However, it *does* depend on the chosen score; if the score is misspecified, the output can be incorrect despite data.\n",
    "\n",
    "- **Type:** Score-based (greedy hill-climbing in DAG space using BIC/score).  \n",
    "- **Complexity:** NP-hard in theory; greedy heuristic with forward-backward phases. Each step requires evaluating score change for $O(n^2)$ candidate edge operations; often efficient in practice for tens to low-hundreds of variables (FGES scales to larger $n$ with parallelism).  \n",
    "- **Assumptions:** Causal sufficiency (no hidden variables); data follows a parametric model (e.g. linear Gaussian or discrete) that matches scoring assumptions; faithfulness (so the true causal DAG maximizes the chosen score); large sample or appropriate penalty to avoid overfitting.  \n",
    "- **Invariances:** Score-equivalence (score depends only on equivalence class structure, not arbitrary orientation of immorality-free edges); invariance of v-structures detection via score; produces a CPDAG capturing invariant causal features as in constraint methods.\n",
    "\n",
    "### Greedy Sparsest Permutation (GSP)  \n",
    "The **Greedy Sparsest Permutation (GSP)** is a more recent algorithm that combines elements of score-based search with constraint-based rationale by searching over variable orderings (permutations) to find a minimal causal model. The idea, introduced by Solus et al. (2017) and further developed by Raskutti & Uhler (2018), is that if one can find an ordering of variables such that each variable has only a few predecessors as parents, then one has effectively identified a sparse DAG that fits the data. **Sparsest Permutation (SP)** algorithms consider *all permutations* of variables and select the DAG (consistent with each permutation) that has the fewest edges consistent with the observed conditional independencies. Since searching all permutations ($n!$ possibilities) is infeasible for even moderate $n$, GSP is a greedy heuristic that incrementally builds a permutation aiming to keep the graph sparse ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=might%20be%20improved%20by%20relaxing,that%20if%20the%20data%20generating)).\n",
    "\n",
    "*Mechanism:* GSP works by iteratively inserting variables into an ordering and assigning parents from those earlier in the order such that adding an edge explains dependencies (typically using CI tests to decide if an edge is needed). In essence, for a given order $\\pi$, one can determine an optimal DAG by letting each node have as parents **all** earlier nodes in $\\pi$ that are not independent of it conditional on some subset (this is akin to the algorithm of Verma & Pearl (1990) for minimal I-maps). The total number of edges for that DAG is then minimized when each variable’s parents are the minimal set explaining its dependencies. GSP attempts to find the permutation that yields the least total number of edges (hence “sparsest”). It uses greedy local swaps or other local moves in permutation space to improve an initial guess. Solus et al. (2021) propose operations like *swap* and *tuck* to efficiently traverse permutation space and find a good candidate ordering ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=directed%20acyclic%20causal%20models%2C%20including,art%20causal%20search)).\n",
    "\n",
    "The motivation behind GSP is to weaken the faithfulness requirement: instead of requiring that *all* conditional independencies are captured (which can fail in nearly-unfaithful cases), one seeks a model with minimal complexity (edges) that still explains the major independencies. In fact, GSP and related permutation methods require a strictly weaker assumption than faithfulness, often called *ordered connectivity* or *adjacency-faithfulness*, which essentially means no false edges are needed to explain independencies ([](https://pages.stat.wisc.edu/~raskutti/publication/SPAlgorithm.pdf#:~:text=independence%20testing,SP%29%20algorithm%20are)). Raskutti and Uhler (2018) proved that if the true distribution satisfies a certain graphoid property and a mild assumption (called *stable sparseness*), the sparsest permutation will yield the correct Markov equivalence class with high probability ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=might%20be%20improved%20by%20relaxing,that%20if%20the%20data%20generating)) ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=distribution%20is%20a%20graphoid%2C%20then,super%20exponential%20in%20the%20number)). Thus, GSP can be consistent under weaker conditions than PC, addressing some cases where PC fails due to nearly violated faithfulness ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=whether%20their%20inferences%20can%20be,short%2C%20and%20can%20be%20violated)).\n",
    "\n",
    "*Complexity:* Exhaustive search over permutations is $O(n!)$, but GSP’s greedy relaxations make it manageable. Each step of evaluating a candidate permutation’s score (number of edges or a score function) requires checking parent sets for each node (which can be done in polynomial time if using conditional independence tests or a decomposable score). The overall complexity is not rigorously polynomial – it depends on how many permutations the greedy procedure explores. Empirically, GSP and its relaxations (like GRaSP) have shown good performance up to $n \\approx 100$ or more, especially for dense graphs where PC/GES struggle ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=and%20develop%20a%20class%20of,1%20INTRODUCTION)) ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=consistent%20under%20increasingly%20weaker%20assumptions,for%20dense%20graphs%20and%20graphs)). It remains a heuristic without a worst-case guarantee better than brute force, but it often finds a very sparse candidate early, pruning much of the search.\n",
    "\n",
    "*Assumptions:* **Type:** Hybrid (permutation-based search using both independence tests and a sparsity score). It’s not purely score-based in the likelihood sense, but it is an optimization over an objective (minimize edges) subject to fitting independence constraints, thus combining both paradigms. **Assumptions:** Requires the causal Markov condition. Does not require full faithfulness; instead uses a weaker condition that there is a unique minimal DAG explaining the independencies (no indistinguishable alternatives with equal sparsity). It assumes causal sufficiency (no latents) in its basic form (like PC). It can incorporate CI tests, so distribution-free in principle except needing an independence oracle. **Key invariances:** GSP exploits the invariance that in the true causal order, each variable is independent of all non-descendants that come after it. If one guesses the correct ordering (topological sort of the DAG), then for each variable $X_i$, any variable that comes after $X_i$ in the order is either its descendant or independent of it given some earlier variables. The true ordering will yield a very *sparse* connectivity pattern (each node only connects to earlier ones that are its causes). This sparsity is an invariant property of the true causal order. GSP’s edge decisions are based on conditional independencies which are invariant; by prioritizing fewer edges, it seeks an invariant minimal representation. GSP is also robust to certain unfaithful configurations since it won’t add two canceling edges if one edge can suffice to explain the dependency pattern.\n",
    "\n",
    "- **Type:** Hybrid/permutation-based (combines CI testing with a combinatorial sparsity search).  \n",
    "- **Complexity:** Greedy heuristic over $n!$ search space; in practice polynomial-time per permutation evaluation, but overall runtime can grow quickly with $n$. Works well with $n$ up to hundreds with optimized moves (GRaSP).  \n",
    "- **Assumptions:** Markov condition; adjacency-faithfulness or similar (weaker than full faithfulness) ([](https://proceedings.mlr.press/v180/lam22a/lam22a.pdf#:~:text=might%20be%20improved%20by%20relaxing,that%20if%20the%20data%20generating)); no latent confounders (or extend MAG concept); typically assumes a dense enough sampling of distribution to reliably test CI.  \n",
    "- **Invariances:** Leverages the invariant ordering of causes before effects. Minimal edge count is used as a proxy for the true ordering’s property. Independence relationships (separators) are invariant and used to include/exclude edges. Returns a CPDAG (or a set of sparse DAGs) that represents invariant causal connections under the identified ordering.\n",
    "\n",
    "## Invariance-Based Methods\n",
    "\n",
    "A different approach to causal discovery is to exploit *invariance properties* of causal mechanisms across different environments or subsets of data. These methods go beyond single-distribution observational data, instead leveraging heterogeneity (from experiments or nature) to identify causality.\n",
    "\n",
    "### Invariant Causal Prediction (ICP)  \n",
    "**Invariant Causal Prediction (ICP)** is an approach introduced by Peters, Bühlmann, and Meinshausen (2016) that identifies causal relationships by leveraging the invariance of causal mechanisms across multiple environments. Assume we observe data from a set of environments (or experimental conditions) $E = \\{1,2,\\dots,m\\}$ – for example, different experimental settings, subsets of data where certain variables may be intervened on, or different subpopulations. The *core idea* of ICP is that if a variable $X_j$ is a direct cause of an outcome $Y$, then the conditional distribution $P(Y \\mid X_j)$ (or more generally $P(Y \\mid \\text{Parents}_Y)$) remains the same across all environments **as long as $X_j$ is not itself intervened on**. Causal predictors have *invariant* relationships with the target, whereas spurious associations may change when background conditions change.\n",
    "\n",
    "ICP formalizes this by testing invariances: one considers a subset $S \\subseteq \\{X_1,\\dots,X_{d}\\}$ of predictors and asks, “Is the conditional law of $Y$ given $S$ the same in all environments?” If yes, $S$ is a candidate set of causal parents of $Y$. In practice (for continuous data), one often performs a regression of $Y$ on $S$ pooling all environments and then checks if the regression residuals have equal distribution (or at least equal expectation) across environments. Formally, for each environment $e$, let $Y = g_S^e(S) + \\varepsilon^e$ be a predictive model. $S$ is *invariant* if one can find a model such that $g_S^1 = g_S^2 = \\cdots = g_S^m$ (the relationship does not change with $e$) and the residual $\\varepsilon$ distribution is identical across $e$. Under mild assumptions, the intersection of all such invariant sets contains the true direct causes of $Y$ (with high probability) (Peters et al., 2016). One typically takes the *intersection* of all subsets found to be invariant – this yields a set $\\hat{S}$ which is a consistent estimator of $\\text{Pa}_Y$ (direct causes of $Y$), even in presence of other (possibly hidden) variables.\n",
    "\n",
    "ICP is not a full structure discovery method for all variables at once; rather it focuses on identifying causes of a given target $Y$. Repeating ICP for each variable as target could in principle recover the whole graph, though with high computational cost. The strength of ICP is that it does not require faithfulness in the same way as PC/GES since it uses environmental changes as a proxy for interventions. It does assume that the set of environments is rich enough that at least one environment changes the distribution of non-causes while leaving the causal mechanism of $Y$ intact (this is sometimes called *Independent Causal Mechanisms* and *modularity*: causal modules do not change unless directly intervened upon).\n",
    "\n",
    "*Complexity:* Checking all subsets $S$ of predictors for invariance is combinatorial ($2^{d}$ subsets). In practice, ICP can be implemented with a forward-selection or other search strategy to avoid brute force. The method is most feasible when the true parent set of $Y$ is not too large, or one can restrict the search (e.g. using domain knowledge). For moderate $d$ (tens of variables) or focusing on a specific target, ICP is tractable. Recent advances (heuristics and more efficient tests) have extended ICP ideas to larger scales and to non-linear models.\n",
    "\n",
    "*Assumptions:* **Type:** Invariance-based (uses multiple environments; essentially a form of constraint-based across datasets). **Assumptions:** (i) *Causal invariance:* The conditional distribution of $Y$ given its true parents $\\text{Pa}_Y$ is invariant across environments (no matter which environment, if we condition on the actual causes of $Y$, the distribution of $Y$ is the same). (ii) In each environment, the set of variables that might have changed (e.g. whose mechanism or distribution is different) does not include $Y$’s causal mechanism itself. This is like saying environments act like interventions on some variables other than $Y$ (or on $Y$’s parents perhaps), but *not* directly on $Y$ in a way that changes $Y$’s response to its causes. (iii) i.i.d. samples within each environment; enough data per environment to test for distributional equality. ICP does **not** require causal sufficiency – it can handle hidden common causes because those would typically violate invariance if they were influencing $Y$ (the method’s consistency proofs account for potential confounders by showing that only true parents give full invariance). It also does not require faithfulness in terms of independencies; instead it requires a form of *stability/transportability* of mechanisms.\n",
    "\n",
    "**Key invariances:** By design, the invariance of $P(Y \\mid S)$ across environments is the criterion. If $S$ includes all true parents of $Y$, then indeed $P(Y \\mid S)$ remains the same even if other variables or external conditions change, because $S$ “shield” $Y$ from those changes. If $S$ is missing one of $Y$’s parents, then when that missing parent’s distribution changes across environments, the distribution of $Y$ given $S$ will also change (detectably) – thus $S$ fails the test. If $S$ contains an extraneous variable that is not a parent, that variable’s relationship with $Y$ might not remain invariant when conditions change (it could create a false dependency that varies). Therefore, the true parent set is the unique set (under appropriate conditions) that yields perfect invariance. This approach is robust to certain confounders: even if some $Z$ confounds $X$ and $Y$, unless $Z$ also confounds the relationship in all environments, the set $\\{X\\}$ might fail invariance (if $X$ is not true cause). In summary, **invariance across environments is used as a signature of causality**. This principle is closely related to the concept of *autonomy of causal mechanisms* in Pearl’s framework and has connections to causal transfer learning and robustness (Schölkopf et al., 2021).\n",
    "\n",
    "- **Type:** Invariance-based, multi-environment (neither purely score nor purely single-distribution constraint).  \n",
    "- **Complexity:** Combinatorial subset search (exponential in worst case), but feasible for moderate targets or guided search.  \n",
    "- **Assumptions:** Existence of multiple environments (experimental or observational perturbations); causal mechanism for $Y$ is invariant; no direct intervention on $Y$ across environments; enough data to test equality of conditional distributions. Does not assume faithfulness or sufficiency explicitly.  \n",
    "- **Invariances:** Identifies features (parent sets) whose predictive relationship to $Y$ is invariant across environment changes. Causal parent-child relationships are stable (module unchanged) under interventions on other parts of the system – this is the invariance leveraged. \n",
    "\n",
    "## Continuous Optimization Methods\n",
    "\n",
    "With advances in machine learning, a new class of causal discovery methods formulates structure learning as a continuous optimization problem. These methods encode the acyclicity constraint with a differentiable penalty or constraint and then optimize a loss (usually based on reconstruction error or likelihood) over real-valued adjacency matrices. They effectively turn the combinatorial search over DAGs into a continuous parameter estimation problem.\n",
    "\n",
    "### NOTEARS (Non-combinatorial Optimization via Trace Exponential)  \n",
    "**NOTEARS** (Zheng et al., 2018) is a seminal algorithm in this category. It considers a linear SEM: $X = XW + N$ where $W$ is a weighted adjacency matrix of the DAG and $N$ is a matrix of independent noise variables. The goal is to find a matrix $W$ that fits the data well (e.g. minimizes sum of squared errors $\\|X - XW\\|^2$) while enforcing that $W$ corresponds to a DAG (no cycles). To avoid the combinatorial DAG constraints, NOTEARS introduces a smooth function $h(W)$ that is zero if and only if $W$ is acyclic. Specifically, one formulation is:\n",
    "$$ h(W) = \\operatorname{tr}\\!\\Big(\\exp(W \\circ W)\\Big) - d = 0, $$ \n",
    "where $W \\circ W$ is the elementwise square and $\\exp(\\cdot)$ is the matrix exponential ([DAGs with NO TEARS: Continuous Optimization for Structure Learning](https://papers.neurips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf#:~:text=Theorem%201,Moreover%2C%20h%28W%29%20has%20a)). This function $h(W)$ is a differentiable equality constraint; intuitively, $\\operatorname{tr}(\\exp(W \\circ W))$ expands to $d +$ (sum of cycles weights). The equation $h(W)=0$ characterizes acyclic graphs ([DAGs with NO TEARS: Continuous Optimization for Structure Learning](https://papers.neurips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf#:~:text=Theorem%201,Moreover%2C%20h%28W%29%20has%20a)). NOTEARS then solves the optimization:\n",
    "$$ \\min_{W \\in \\mathbb{R}^{d\\times d}} F(W) \\quad \\text{s.t.} \\quad h(W) = 0, $$\n",
    "where $F(W)$ is a loss function, for example \n",
    "$$F(W) = \\frac{1}{2n}\\|X - XW\\|_2^2 + \\lambda \\|W\\|_1,$$ \n",
    "a least-squares with $\\ell_1$ penalty to encourage sparsity (so it’s akin to fitting a sparse weighted adjacency matrix). This is solved with standard continuous optimization techniques (augmented Lagrangian, gradient-based optimizers) ([DAGs with NO TEARS: Continuous Optimization for Structure Learning](https://papers.neurips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf#:~:text=learning%20of%20DAGs%20so%20that,with%20a%20smooth%20equality%20constraint)) ([DAGs with NO TEARS: Continuous Optimization for Structure Learning](https://papers.neurips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf#:~:text=are%20only%20guaranteed%20to%20find,com%2Fxunzheng)). The result is a weight matrix $\\hat{W}$ which is then thresholded to obtain a binary adjacency (structure).\n",
    "\n",
    "*Advantages:* NOTEARS can efficiently handle fairly large graphs (e.g. $d=50$–100) by leveraging gradient-based methods and is easily extendable (the authors also did logistic regression for binary data, etc.). It finds both the structure and parameters simultaneously by solving a single optimization problem ([DAGs with NO TEARS: Continuous Optimization for Structure Learning](https://papers.neurips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf#:~:text=G%202%20D%20in%20,quick%20illustration%20and%20Section%205)). One of the surprising findings was that **NOTEARS does not require the faithfulness assumption** in the same way constraint-based methods do – it can, for instance, recover correct DAGs even when the true distribution has near-zero partial correlations (a near-violation of faithfulness), as long as the exact optimum of the optimization corresponds to the true structure (which it will if the model class is correct) ([[PDF] Learning DAGs with Continuous Optimization Xun Zheng](https://www.ml.cmu.edu/research/phd-dissertation-pdfs/thesis-zheng-xun.pdf#:~:text=,does%20NOTEARS%20work%3F%20One)). In other words, by focusing on minimizing reconstruction error, NOTEARS bypasses the need for explicit CI tests that might fail when correlations are very small but nonzero. This makes it robust in scenarios where PC might misinterpret a tiny correlation as independence. \n",
    "\n",
    "*Complexity:* Each evaluation of $h(W)$ involves a matrix exponential, which can be computed in $O(d^3)$ time ([DAGs with NO TEARS: Continuous Optimization for Structure Learning](https://papers.neurips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf#:~:text=A%20key%20conclusion%20from%20Theorem,appeared%20in%20the%20DAG%20learning)). Gradient-based optimization will require many such evaluations, but typically the number of iterations is manageable (tens to hundreds). Empirically, NOTEARS solves problems up to $d \\approx 100$ in seconds to minutes on modern hardware. The overall complexity is difficult to characterize in Big-O because it depends on convergence of the solver (which is not worst-case guaranteed polynomial). In practice, it scales much better than combinatorial search for moderate $d$. For very large $d$ (hundreds+), the cubic scaling of the matrix exponential can be a bottleneck, but there are iterative and approximate methods to handle it. In summary, NOTEARS turns structure learning into solving a series of matrix operations and gradient steps, which is far more efficient than naive search, though the problem is still non-convex (so one must be careful about local minima).\n",
    "\n",
    "*Assumptions:* **Type:** Continuous score-based optimization (with differentiable acyclicity constraint). **Assumptions:** The standard NOTEARS assumes a **linear SEM with additive noise** – essentially a linear Gaussian (or at least sub-Gaussian) model. The causal sufficiency assumption holds (no latent confounders) because a single $W$ is fit to all variables. It assumes the data truly follows a DAG structure (i.e. there exists some weighted acyclic $W$ generating the data). While it doesn’t explicitly require faithfulness, it does assume that the optimization problem has a unique minimum at the true $W$ (this can fail if two different DAGs produce exactly the same covariance, which is the faithfulness violation case; however in linear models that usually implies some parameter degeneracy which would mean the optimum is not unique). For identifiability, one often assumes either no degeneracies in $W$ or some additional condition to avoid flipping edges with equal error. In practice, a small $\\ell_2$ weight on $W$ or thresholding helps ensure a stable solution. **Key invariances:** NOTEARS exploits the fact that in a correctly specified linear model, the independence structure is encoded in zero entries of $W$. It enforces acyclicity invariantly through $h(W)$. One might say the invariant here is the *acyclic constraint itself* and the minimality of the SEM: if an edge is not present in the true DAG, setting that entry of $W$ to zero should (in the optimum) not worsen the loss, whereas adding a nonzero weight where no true edge exists would only add spurious covariance that doesn’t help fit the data. Thus, the zero pattern of the optimal $W$ is an invariant indicator of the absence of causal links. Another invariance: because it’s optimizing a global score, NOTEARS by design yields a model that is as good as a full ML estimate for that structure, so it shares the likelihood invariances with score-based methods (i.e. it will correctly identify v-structures by the same mechanism of improved fit). Also, being essentially a parametric ML method, it inherits any invariances of the ML estimator (for example, in linear Gaussian models, the correct model is identifiable up to Markov equivalence; NOTEARS will output one member of the class, often the true DAG if there is any slight edge weights asymmetry to distinguish it).\n",
    "\n",
    "- **Type:** Continuous optimization (score-based with equality constraint).  \n",
    "- **Complexity:** Polynomial per iteration ($O(d^3)$ for matrix exponential); non-convex optimization but usually fast convergence for moderate $d$. Scales to tens or hundreds of variables.  \n",
    "- **Assumptions:** Linear SEM (or specified differentiable model); no hidden confounders; noise independent and typically identical distribution; true DAG generates data (model correct); in practice, requires data sufficient to estimate covariance accurately. Does not strictly need global faithfulness, but relies on a unique optimum.  \n",
    "- **Invariances:** Encodes the global acyclicity invariant via $h(W)=0$ ([DAGs with NO TEARS: Continuous Optimization for Structure Learning](https://papers.neurips.cc/paper/8157-dags-with-no-tears-continuous-optimization-for-structure-learning.pdf#:~:text=Theorem%201,Moreover%2C%20h%28W%29%20has%20a)). Respects score-equivalence in the sense that the final loss depends only on the equivalence class (though optimization might pick one DAG). Captures invariant sparsity pattern of true $W$ (zero entries correspond to missing edges in all equivalent DAGs). The approach is robust to small perturbations in data relationships, focusing on overall fit.\n",
    "\n",
    "## Other Notable Methods and Extensions  \n",
    "\n",
    "Beyond the above, numerous other DAG-based causal discovery methods exist, often combining ideas or adding new assumptions for identifiability:\n",
    "\n",
    "- **LiNGAM (Linear Non-Gaussian Acyclic Model):** Shimizu et al. (2006) showed that if the data follow a linear DAG model *but with non-Gaussian independent noises*, the causal DAG becomes *fully identifiable*, not just up to Markov equivalence ([[PDF] A Linear Non-Gaussian Acyclic Model for Causal Discovery](https://jmlr.org/papers/volume7/shimizu06a/shimizu06a.pdf#:~:text=How,2006)) ([A pooling-LiNGAM algorithm for effective connectivity analysis of ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC4186480/#:~:text=The%20LiNGAM%20algorithm%20utilizes%20higher,2006)). The non-Gaussianity breaks the symmetry that plagues Gaussian DAGs. LiNGAM uses Independent Component Analysis (ICA) to exploit higher-order statistics: it finds an unmixing matrix that corresponds to the causal ordering. In brief, if $X = B X + N$ (with $N$ non-Gaussian, independent components), then one can recover $B$ (and hence the directed graph) by an ICA algorithm. LiNGAM is a **constraint-based** method in spirit (it finds an ordering such that variables become independent after appropriate subtraction of parent effects) but also uses a scoring (likelihood of independent components). It assumes no latent variables and that each variable is a linear mixture of its parents plus a non-Gaussian noise; under these assumptions it is provably consistent and can even determine the exact ordering of variables (not just CPDAG). Variants like DirectLiNGAM (Shimizu et al., 2011) improve computational efficiency.\n",
    "\n",
    "- **Nonlinear Additive Noise Models (ANM):** Hoyer et al. (2009) extended LiNGAM to nonlinear relationships: if $Y = f(X) + N$ with $N$ independent of $X$, and both $f$ and distribution of $N$ are “generic” (no fine-tuned cancellation), one can often distinguish $X \\to Y$ from $Y \\to X$ because in the correct causal direction the noise is independent of the cause, while in the wrong direction the noise term will be dependent on the cause. ANM methods perform independence tests between residuals and predictors to find the correct orientation. These methods assume causal sufficiency and that the function and noise are such that the independence holds only in the causal direction. This breaks Markov equivalence in many cases, allowing full DAG discovery in bivariate cases and beyond by pairwise checks or combining with graph search. Constraint- and score-based methods can be augmented with such independence-of-noise tests to orient edges that would otherwise be undetermined.\n",
    "\n",
    "- **Hybrid Methods (e.g. MMHC):** The **Max-Min Hill-Climbing (MMHC)** algorithm (Tsamardinos et al., 2006) is an example of a hybrid approach that first uses a constraint-based phase to limit the search space (identify a skeleton via a series of statistical tests, like the Max-Min Parents and Children algorithm) and then applies a score-based hill-climb to orient edges and fine-tune the structure. This way, it leverages the strengths of both approaches: the speed of constraint-based pruning and the robustness of score-based search in the reduced graph space. Many practical implementations use such hybrid strategies for efficiency.\n",
    "\n",
    "- **Causal Bayesian Networks with Interventions:** When some experimental (interventional) data is available, one can extend algorithms (e.g. GES has an interventional variant that can combine observational and experimental data). In the presence of interventions that break certain dependencies, identifiability improves. For example, **IGSP** (Invariant GSP) extends GSP to intervention settings (Wang et al., 2018), and **Joint-IDA** (Maathuis et al., 2010) leverages multiple intervention experiments to fully orient the CPDAG. While not purely observational, these DAG-based methods are crucial in practical causal discovery and use similar ideas of score or constraint but with modified tests accounting for interventions.\n",
    "\n",
    "- **Dynamic/Time-Series Models:** If the data have a time component and one can assume a causal ordering over time (no instantaneous feedback loops), algorithms like PCMCI (Runge et al., 2019) or time-series adaptations of PC/FCI can exploit temporal separation to reduce the search complexity. Time series causal discovery often assumes a partial ordering given by time lags, which simplifies identifiability (causes precede effects in time).\n",
    "\n",
    "Each of these methods makes certain trade-offs in assumptions vs. applicability. For instance, LiNGAM and ANM assume special noise or functional forms (strong assumptions, but then identifiability is achieved without needing multiple environments or prior knowledge). Hybrid methods sacrifice theoretical purity for improved finite-sample performance. Interventional methods step outside pure observation but provide gold-standard identification when experiments are feasible. Dynamic methods utilize domain-specific structure (time) to aid discovery.\n",
    "\n",
    "## Identifiability vs. Estimation Revisited  \n",
    "Across all these methods, a recurring theme is the balance between *identifiability* (what the method can guarantee with infinite data under its assumptions) and *estimation complexity* (how it performs with finite data and computational resources). Constraint-based methods like PC/FCI are **identifiability-focused**: under their assumptions, they output a provably correct equivalence class, but they can suffer in estimation if CI tests are unreliable at finite samples or high dimensions. Score-based methods like GES sacrifice a bit of identifiability (they might return a suboptimal local maximum in finite sample) for more robust estimation (aggregating evidence via a score). Methods like NOTEARS or LiNGAM introduce stronger modeling assumptions which, when correct, yield full identifiability (e.g. exact DAG recovery, not just CPDAG) and often good estimation, but if assumptions are violated, they can give misleading results. Invariance-based ICP directly targets causal identifiability by using multiple datasets; its estimation depends on having enough data across those environments and careful statistical testing.\n",
    "\n",
    "It is also important to note the distinction between **causal structure identification** and **causal effect identification**. All the above methods primarily aim to identify the structure (the DAG or equivalence class). Once the structure (or partial structure) is known, one can then estimate causal effects (e.g. using regression or adjustment formulas). In some cases, a causal effect might be identifiable even if the full structure is not – for example, under the back-door criterion, one can identify $P(Y\\mid do(X))$ by adjusting for any valid back-door set of covariates (Pearl, 2009). Algorithms exist to directly find causal effects (e.g. using reinforcement learning or pattern recognition of conditional distributions) without fully identifying the DAG. However, in DAG-based frameworks, effect identification is typically done after or in conjunction with structure learning. Pearl’s do-calculus gives graphical conditions under which an effect is identifiable from a given partially known graph. In an academic review context, we focus on structure because these algorithms yield the knowledge necessary to compute arbitrary causal effects (provided they are identifiable). \n",
    "\n",
    "## Conclusion  \n",
    "Causal identification in DAG-based frameworks is a rich field at the intersection of statistics, computer science, and philosophy. We reviewed key algorithms – constraint-based (PC, FCI), score-based (GES), hybrid and ordering-based (GSP), invariance-based (ICP), and continuous optimization (NOTEARS) – and discussed their assumptions and properties. A unifying principle is that **causation leaves statistical footprints**: whether in conditional independencies, optimal score structure, invariances across environments, or functional asymmetries, each method leverages certain invariant features of the data that arise from the underlying causal DAG. The trade-offs in algorithm design often reflect which invariants are exploited and which assumptions are made to ensure those invariants are reliable. As research progresses, we see methods relaxing assumptions (e.g. allowing some latent variables or weaker faithfulness) and scaling up computationally, often by borrowing techniques from machine learning (like gradient-based optimization in NOTEARS or neural network approaches) while retaining theoretical guarantees where possible. \n",
    "\n",
    "A fundamental limitation remains that without either strong assumptions, multiple environments, or interventions, causal discovery from observational data cannot be fully resolved beyond Markov equivalence. Thus, a PhD-level understanding of this topic entails recognizing the conditions under which causal identification *is* possible and choosing the appropriate method that aligns with those conditions. The methods covered here form the toolkit for a broad range of scientific problems, from biology to economics, where understanding causality is paramount. Ongoing developments continue to expand this toolkit, aiming for algorithms that are more computationally efficient, more statistically powerful in finite samples, and more flexible in the face of real-world complexities (like latent structure, feedback loops, and non-stationarity). The rigorous study of these causal identification methods not only advances theoretical knowledge but also paves the way for practical discovery of causal relations in data-rich fields.\n",
    "\n",
    "## References  \n",
    "\n",
    "- Pearl, J. (2009). *Causality: Models, Reasoning, and Inference*. 2nd Ed. Cambridge University Press.  \n",
    "- Spirtes, P., Glymour, C., & Scheines, R. (2000). *Causation, Prediction, and Search*. MIT Press.  \n",
    "- Chickering, D. (2002). “Optimal structure identification with greedy search.” *JMLR*, **3**(Nov), 507–554.  \n",
    "- Peters, J., Janzing, D., & Schölkopf, B. (2017). *Elements of Causal Inference: Foundations and Learning Algorithms*. MIT Press. (Contains the 2016 ICP method details.)  \n",
    "- Peters, J., Bühlmann, P., & Meinshausen, N. (2016). “Causal inference using invariant prediction: identification and confidence intervals.” *Journal of the Royal Statistical Society: Series B*, **78**(5), 947–1012.  \n",
    "- Shimizu, S., et al. (2006). “A Linear Non-Gaussian Acyclic Model for causal discovery.” *JMLR*, **7**, 2003–2030.  \n",
    "- Hoyer, P., et al. (2009). “Nonlinear causal discovery with additive noise models.” *NIPS*.  \n",
    "- Zheng, X., Aragam, B., Ravikumar, P., & Xing, E. (2018). “DAGs with NO TEARS: Continuous optimization for structure learning.” *NeurIPS*.  \n",
    "- Solus, L., Wang, Y., & Uhler, C. (2021). “Consistent causal estimation with feedback.” (for GSP algorithm details).  \n",
    "- Tsamardinos, I., Brown, L., & Aliferis, C. (2006). “The Max-Min Hill-Climbing Bayesian Network Structure Learning Algorithm.” *Machine Learning*, **65**(1), 31–78.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d038884-80f5-4fc5-baa9-977be148678f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53c8a396-462e-4529-ad55-e696d1c83e9d",
   "metadata": {},
   "source": [
    "Great. I’ll write a PhD-level review on recent advances in causal discovery and causal inference applied to brain-computer interfaces (BCIs), focusing on non-invasive modalities like EEG, fMRI, IMU, and MRI. I’ll organize it into sections covering the introduction, methodology, recent application-driven advances, challenges, and future directions. The review will include BibTeX-formatted references and use LaTeX for mathematical expressions with \"$\" and \"$$\" formatting only.\n",
    "\n",
    "I'll let you know as soon as the review is ready.\n",
    "\n",
    "# Causal Discovery and Inference in Non-Invasive BCI: Recent Advances and Applications\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Brain-computer interfaces (BCIs) enable direct communication between the brain and external devices by decoding brain signals into control commands. Non-invasive BCI modalities such as electroencephalography (EEG), functional magnetic resonance imaging (fMRI), inertial measurement units (IMUs for motion sensing), and structural MRI provide complementary ways to observe brain activity without surgery. Traditional BCI approaches often rely on correlational pattern recognition (e.g. classifying EEG features), which can be brittle in the face of non-stationarity or unaccounted confounders. **Causal discovery and causal inference** methods offer a principled framework to move beyond correlation and toward understanding *cause-effect* relationships in brain signals, potentially improving the robustness and interpretability of BCI systems. In a causal framework, one seeks to identify directed influences (i.e. which neural events *cause* others) and predict the effect of interventions (e.g. stimulating a brain region or providing feedback) rather than just measure statistical associations ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=In%20the%20past%20two%20decades%2C,Acyclic%20Models%2C%20Patel%E2%80%99s%20Tau%2C%20Structural)) ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=Equation%20Modelling%2C%20and%20Transfer%20Entropy,future%20directions%20in%20this%20area)). This is especially important for BCIs used in closed-loop scenarios (like neurofeedback) or for gaining neuroscientific insight into how brain activity drives behavior.\n",
    "\n",
    "Recent years have seen significant advances in applying causal discovery algorithms and inference techniques to neural data, enabled by better computational methods and richer multimodal datasets. This review provides a PhD-level overview of these advances, focusing on practical applications in BCIs using EEG, fMRI, IMU, and MRI. We first present a methodological overview of causal discovery/inference techniques relevant to BCIs. Next, we discuss recent application-driven findings – for example, how causality-based methods have improved decoding of motor intentions from EEG or mapping of brain networks with fMRI. We then examine key challenges (such as nonstationary signals, latent confounders, and real-time constraints) and consider future research directions. Throughout, we emphasize clarity and depth, highlighting contributions from the last five years and citing relevant studies. Mathematical formulations are included where useful, with notation consistent across sections.\n",
    "\n",
    "## Methodological Overview of Causal Discovery in Brain Signals\n",
    "\n",
    "### Causal Graphs and Structural Equation Models\n",
    "\n",
    "A common formalism for causal discovery is the **directed acyclic graph (DAG)**, where nodes represent variables (such as neural signals from different channels or regions) and directed edges represent putative causal influences ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=so%20far%2C%20each%20method%20has,future%20directions%20in%20this%20area)). Underlying a DAG is usually a set of *structural equations* describing how each variable is generated from its parents (direct causes) and some independent noise. For example, if $X_j$ and $X_k$ are two EEG source signals, a simple structural causal model might be: \n",
    "\n",
    "$$\n",
    "X_k(t) = f(X_j(t-\\tau),\\, \\epsilon_k(t)),\n",
    "$$ \n",
    "\n",
    "meaning $X_j$ at an earlier time causes changes in $X_k$ (with some delay $\\tau$) plus noise $\\epsilon_k$. In vector form, a linear instantaneous model for a set of variables $\\mathbf{X} = (X_1,\\ldots,X_n)$ can be written as $\\mathbf{X} = B^\\top \\mathbf{X} + \\mathbf{N}$, where $B$ is a matrix of directed connection weights (usually assumed acyclic by appropriate ordering) ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=so%20far%2C%20each%20method%20has,future%20directions%20in%20this%20area)). Estimating the causal graph from data entails finding the structure (which entries of $B$ are non-zero) that best explains the observed statistical dependencies. This is the goal of *causal discovery algorithms*.\n",
    "\n",
    "Two major categories of causal discovery algorithms are **constraint-based** and **score-based** methods ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=so%20far%2C%20each%20method%20has,future%20directions%20in%20this%20area)). Constraint-based approaches (exemplified by the PC algorithm and its variants) systematically test conditional independence relationships in the data to infer causal links. In EEG or fMRI connectivity analysis, this could involve testing whether activity in region $A$ and $B$ are independent given activity in region $C$, etc. Score-based approaches, on the other hand, define a goodness-of-fit score (e.g. based on likelihood) for a candidate graph and then search for the highest-scoring graph structure. Recent years have also seen **continuous optimization** approaches like NOTEARS (which formulates DAG search as a differentiable problem) – these have potential to scale to higher dimensions (hundreds of nodes) by leveraging gradient-based optimization, though they have seen limited application in BCI so far.\n",
    "\n",
    "A key assumption in many classical causal discovery methods is that there are no hidden confounders and that causal relations are acyclic. In brain data, these assumptions can be problematic: There may be unobserved common causes (e.g. an unmeasured brain region influencing two EEG channels) and feedback loops in neural circuits. Advanced algorithms such as FCI (Fast Causal Inference) can infer some causal structure even in the presence of latent confounders, at the cost of more indeterminate results. Another assumption is often linearity or specific noise distributions. Methods like LiNGAM (Linear Non-Gaussian Acyclic Model) exploit non-Gaussian independent noise to identify causal ordering even in purely observational data ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=so%20far%2C%20each%20method%20has,future%20directions%20in%20this%20area)). LiNGAM has been applied to fMRI time series to detect directed connectivity by assuming hemodynamic signals have non-Gaussian distributions ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=so%20far%2C%20each%20method%20has,future%20directions%20in%20this%20area)). Nonlinear and nonparametric extensions (e.g. using kernel methods or ensemble methods) have been proposed to capture the complex dynamics of brain signals, though their application is computationally heavy.\n",
    "\n",
    "In practice, for BCI data, one often restricts the variable set based on domain knowledge to make causal discovery tractable. For example, one might model causal links among a handful of major EEG sources (e.g. frontal, motor, occipital regions) rather than dozens of electrodes, or among fMRI regions of interest rather than thousands of voxels. **Dynamic causal modeling (DCM)** is a prominent Bayesian framework in neuroscience that embodies this strategy: it assumes a known structural graph and uses Bayesian inference to estimate coupling parameters and latent neural states, particularly for fMRI or EEG evoked responses. DCM explicitly models how neural activity causes observed signals via a biophysical forward model (e.g. the hemodynamic response for fMRI), and it can compare different hypothesized connectivity structures ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=so%20far%2C%20each%20method%20has,future%20directions%20in%20this%20area)). While DCM is hypothesis-driven (the user specifies plausible networks to test), data-driven discovery methods aim to infer networks with minimal prior assumptions.\n",
    "\n",
    "### Time-Series Causality: Granger Causality and Extensions\n",
    "\n",
    "Many BCI modalities generate time-series data, so temporal causal inference is crucial. A widely used concept is **Granger causality (GC)**, originally from econometrics, which operationalizes causality in the time domain: if including the past of signal $X$ improves the prediction of signal $Y$, beyond using $Y$’s own past, then $X$ is said to *Granger-cause* $Y$. Formally, let $Y_t$ denote the value of $Y$ at time $t$. $X$ Granger-causes $Y$ if \n",
    "$$\\text{Var}(Y_t \\mid Y_{t-1}, Y_{t-2}, \\ldots) > \\text{Var}(Y_t \\mid Y_{t-1}, Y_{t-2}, \\ldots, X_{t-1}, X_{t-2}, \\ldots),$$ \n",
    "meaning the predictive variance of $Y$ decreases when including past $X$ terms. In practice one fits autoregressive models: e.g., for bivariate signals $(X,Y)$, compare the model $Y_t = \\sum_{k=1}^p a_k Y_{t-k} + \\varepsilon_t$ with the model including $X$ terms $Y_t = \\sum_{k=1}^p a_k Y_{t-k} + \\sum_{k=1}^p b_k X_{t-k} + \\varepsilon'_t$. If the latter significantly improves goodness-of-fit (e.g. lowers AIC or BIC, or an $F$-test indicates $\\{b_k\\}$ not all zero), one concludes $X \\to Y$ causal influence. GC has been extremely popular for inferring **effective connectivity** in brain signals because it is relatively straightforward and captures directed *temporal* influence ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=so%20far%2C%20each%20method%20has,future%20directions%20in%20this%20area)). Notably, GC is an *operational* definition of causality reliant on predictability; it does not guarantee a true causal relationship (common drivers can induce false GC), but it aligns with how neuroscientists often define directed functional connections.\n",
    "\n",
    "In EEG-based BCIs, GC is often computed between channels or sources to yield a directed brain network during a particular cognitive task or mental state. For example, researchers have extracted **Granger causality brain network features** to distinguish different mental tasks or emotions from EEG ([Frontiers | EEG emotion recognition based on cross-frequency granger causality feature extraction and fusion in the left and right hemispheres](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.974673/full#:~:text=maintaining%20the%20best%20emotion%20recognition,frequency%20band%20GC%20features)) ([Frontiers | EEG emotion recognition based on cross-frequency granger causality feature extraction and fusion in the left and right hemispheres](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.974673/full#:~:text=Considering%20the%20functional%20asymmetric%20of,proposed%20under%20the%20condition%20of)). Extensions of GC handle various complexities: *frequency-domain GC* examines causal influence at specific oscillatory frequencies (useful since EEG has distinct frequency bands like alpha, beta, etc.), and *conditional GC* (multivariate GC) accounts for multiple interacting sources to avoid spurious connectivity. **Nonlinear Granger causality** methods have also been developed, since neural interactions can be nonlinear. One approach is to use nonlinear predictors (such as neural networks or kernel methods) in place of linear autoregression. For instance, a *nonlinear GC* analysis was recently used to extract features from multi-channel EEG for motor imagery BCI, outperforming linear GC in classification accuracy ([[2409.10374] Nonlinear Causality in Brain Networks: With Application to Motor Imagery vs Execution](https://arxiv.org/abs/2409.10374#:~:text=brain%20dynamics,representing%20causal%20interactions%20in%20autoregressive)) ([[2409.10374] Nonlinear Causality in Brain Networks: With Application to Motor Imagery vs Execution](https://arxiv.org/abs/2409.10374#:~:text=terms%2C%20a%20concept%20underlying%20Granger,in%20brain%20connectivity%20across%20subjects)). Another approach introduced a robust statistical formulation of GC using heavy-tailed noise (Student-t distribution) to reduce the influence of outliers in EEG effective connectivity analysis ([A novel robust Student's t-based Granger causality for EEG based ...](https://www.sciencedirect.com/science/article/abs/pii/S1746809422007753#:~:text=,in%20directed%20brain%20network%20analysis)) ([Bayesian Based Granger Causality Analysis for Positive Emotional ...](https://www.researchgate.net/publication/354438285_Bayesian_Based_Granger_Causality_Analysis_for_Positive_Emotional_EEGs#:~:text=Bayesian%20Based%20Granger%20Causality%20Analysis,Biomedical%20Signal%20Processing%20and)) – this improved the discriminative power of network features for emotion recognition in an EEG-based affective BCI.\n",
    "\n",
    "Beyond GC, **information-theoretic measures** like *transfer entropy* (TE) are widely used for causal inference in neural time-series. Transfer entropy measures the reduction in uncertainty of future $Y$ given past $X$, conditioned on past $Y$, and can capture nonlinear dependencies. Unlike GC which often assumes a parametric model, TE is model-free. A recent study computed transfer entropy between EEG channels to characterize directed connectivity for motor imagery tasks ([\n",
    "            Classification of Right/Left Hand Motor Imagery by Effective Connectivity Based on Transfer Entropy in Electroencephalogram Signal - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10719976/#:~:text=electroencephalogram%20,right%20and%20lefthand%20MI%20tasks)) ([\n",
    "            Classification of Right/Left Hand Motor Imagery by Effective Connectivity Based on Transfer Entropy in Electroencephalogram Signal - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10719976/#:~:text=TE%20is%20calculated%20among%20EEG,methods%20are%20used%20for%20classification)). They identified robust TE-based features that, after feature selection, allowed classification of left vs. right hand motor imagery with over 90% accuracy ([\n",
    "            Classification of Right/Left Hand Motor Imagery by Effective Connectivity Based on Transfer Entropy in Electroencephalogram Signal - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10719976/#:~:text=Results%3A)) ([\n",
    "            Classification of Right/Left Hand Motor Imagery by Effective Connectivity Based on Transfer Entropy in Electroencephalogram Signal - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10719976/#:~:text=Results%20show%20that%20the%20best,accuracy)) – a notable improvement, suggesting that causal features carry discriminative information for BCI control. **Directed information** and *conditional mutual information* are related measures also applied to quantify causal drive between neural signals, sometimes with methods to correct for finite sample bias.\n",
    "\n",
    "### Causal Inference in Multimodal and Spatially-Distributed Data\n",
    "\n",
    "Each non-invasive modality provides a different window into brain activity, and combining modalities can enrich causal analysis. **EEG** has high temporal resolution and directly measures electrical activity, but source localization is uncertain and signals are mixtures due to volume conduction (an electrical field from one source propagates to multiple electrodes). **fMRI** has high spatial resolution and can cover the whole brain, but is indirect (measuring blood flow changes) and much slower (on the order of seconds). **IMUs** measure motion/kinematics and are not neural per se, but in BCI contexts they can capture limb or head movements that may either result from brain commands or feed back to brain (proprioceptive feedback). **Structural MRI** (e.g. diffusion MRI for structural connectivity) provides anatomical constraints on possible neural pathways.\n",
    "\n",
    "Causal analysis in such multimodal settings often leverages one modality to inform another. For example, anatomical connectivity from diffusion MRI (DTI) can be used as a prior for causal graph discovery on fMRI data. A recent *graph neural network (GNN) framework for causal inference in brain networks* combined fMRI time-series with the brain’s anatomical graph structure ([\n",
    "            Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11763835/#:~:text=Greenlee%20M,Google%20Scholar)). In this approach, each node (brain region) integrates information from its neighbors on the anatomical graph, and the GNN is trained to predict directed functional influences, effectively learning a mapping from structural connectivity to causal functional connectivity ([A graph neural network framework for causal inference in brain ...](https://www.nature.com/articles/s41598-021-87411-8#:~:text=,on%20the%20structural%20anatomical%20layout)) ([\n",
    "            Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11763835/#:~:text=Greenlee%20M,Google%20Scholar)). *Greenlee and Lang (2021)* implemented this idea and demonstrated improved accuracy in inferring causal dependencies between brain regions when including structural connectivity constraints ([\n",
    "            Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11763835/#:~:text=Greenlee%20M,Google%20Scholar)). Such methods hint that integrating MRI (structural) and fMRI (functional) data can yield more plausible causal brain networks, which is valuable for BCI applications like stroke rehabilitation where knowing the structural pathways can guide neurofeedback targeting.\n",
    "\n",
    "In EEG, one major challenge for causal discovery is the mixing of sources at the sensor level. Techniques like Independent Component Analysis (ICA) or source localization are often applied first, to get estimates of underlying neural source signals that can serve as variables in a causal graph. Even then, **volume conduction** (instantaneous mixing) can induce zero-lag correlations that trick algorithms into false causality. Methods like *partial directed coherence* and *direct transfer function* have been developed in the frequency-domain to mitigate this by focusing on lagged interactions. More recently, researchers proposed combining EEG with peripheral sensors (like IMUs or EMG) to **separate neural causation from artifacts**. For instance, head movements (captured by IMU) can cause EEG changes due to electrode motion or muscle activity; by modeling the causal influence of IMU signals on EEG, one can identify and subtract artifact influences, isolating true brain-to-computer commands. While studies explicitly doing *causal* integration of IMU with EEG are still emerging, the concept of hybrid BCIs supports this – e.g. a hybrid BCI combining EEG with eye tracking or head motion can discern whether an EEG pattern was caused by a user’s actual motion or by internal intent ([An EEG-/EOG-Based Hybrid Brain-Computer Interface - Frontiers](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2019.01243/full#:~:text=An%20EEG,grade%20wearable%20device)) ([The hybrid BCI - Frontiers](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnpro.2010.00003/full#:~:text=A%20simultaneous%20hybrid%20BCI%20can,associated%20with%20two)). The causal approach here would treat the peripheral signals as additional variables and attempt to infer the direction of influence (brain -> movement vs. movement -> brain).\n",
    "\n",
    "**fMRI causal discovery** has to contend with the sluggish and indirect nature of the BOLD signal. Advanced methods incorporate the hemodynamic response function (HRF) into the model; for example, one may deconvolve the BOLD signal to approximate neural activity before applying causal discovery, or use DCM which includes the HRF forward model. Constraint-based causal discovery has been attempted on fMRI time-series (e.g. using the PC algorithm on ROI-averaged fMRI data), but results can be sensitive to preprocessing choices ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C5%3A%20Spatial%20smoothing,to%20explain%20these%20correlations%20with)) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=level%20of%20parcellations%20of%20voxels,Individual%20parcels)). Recent efforts address fMRI-specific issues: **sampling rate and delays** (the slow sampling can cause misidentification of temporal order ([\n",
    "            Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC6370462/#:~:text=lags%20,These%20slow))), spatial noise correlations from smoothing ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C5%3A%20Spatial%20smoothing,Performing%20analysis%20at%20the)), and very high dimensionality (hundreds of brain regions) with limited timepoints ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C6%3A%20High%20Dimensionality,million%20neurons%20in%20an%208)) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=therefore%20scale%20to%20hundreds%20or,Lam%20et%20al)). Some solutions include working at coarser parcellations to reduce dimensionality and using regularization or priors to favor sparse or physiologically plausible networks ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C7%3A%20High%20Density,Strogatz%2C%201998%2C%20Rawls%20et%20al)) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=interconnected%20with%20other%20subcortical%20and,%C2%B4)). Notably, *Ramsey et al.* and others have developed algorithms that scale to whole-brain fMRI by incorporating such strategies, discovering directed networks that align with known anatomical hubs ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C7%3A%20High%20Density,Strogatz%2C%201998%2C%20Rawls%20et%20al)) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=2022%5D.%20This%20small,and%20sending%20information%20about%20the)).\n",
    "\n",
    "Finally, it is worth noting the difference between **causal *discovery*** and **causal *effect inference***. The former aims to learn the causal graph (who causes whom), whereas the latter assumes a causal model and estimates the magnitude of an effect of interest, often via *counterfactual* or *interventional* analysis. In BCI contexts, effect inference might be used to answer questions like “if we stimulate region X, how much change in output Y (e.g. a cursor movement) should we expect?” or “was the change in the patient’s mood caused by the neurofeedback intervention or just chance?”. While discovery has been the focus in recent literature (since uncovering brain connectivity is of high interest), some works also apply *potential outcomes* frameworks or **do-calculus** for specific experimental designs. For example, in cognitive neuroscience, researchers use causal inference to analyze the effect of experimentally perturbing a stimulus on EEG responses, controlling for pre-stimulus brain state (an application of **causal marginal models** in event-related potentials). These techniques often rely on assumptions of no unmeasured confounders (which can be approximately met in controlled experiments). In summary, a toolkit of methods – from graphical models and Granger causality to deep learning-based causal prediction – is now available to probe cause-effect relations in non-invasive neurodata. The next section illustrates how these methods have recently advanced *practical BCI applications*.\n",
    "\n",
    "## Recent Advances and Applications in Causal BCI Systems\n",
    "\n",
    "### EEG-Based BCIs: Causal Features for Classification and Control\n",
    "\n",
    "One of the most active areas is applying causal discovery to **EEG-based BCIs**, which are widely used for communication (spellers), motor control (prosthetics or wheelchairs via motor imagery), and cognitive monitoring (fatigue or emotion detection). Recent studies demonstrate that incorporating causal features can enhance BCI performance and provide neurophysiological insights:\n",
    "\n",
    "- **Motor Imagery BCI Improvements**: Motor imagery (MI) tasks (imagining limb movements) are a cornerstone of BCIs for paralyzed patients. Traditional classification uses power spectral features from specific frequency bands (like mu rhythms) to distinguish left-hand vs right-hand imagery. New approaches extract *effective connectivity* patterns during MI to boost classifier robustness. For example, Rezaei and Shalbaf (2023) computed a network of directed information flow between EEG channels using transfer entropy for each trial of left- or right-hand imagery ([\n",
    "            Classification of Right/Left Hand Motor Imagery by Effective Connectivity Based on Transfer Entropy in Electroencephalogram Signal - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10719976/#:~:text=electroencephalogram%20,right%20and%20lefthand%20MI%20tasks)) ([\n",
    "            Classification of Right/Left Hand Motor Imagery by Effective Connectivity Based on Transfer Entropy in Electroencephalogram Signal - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10719976/#:~:text=TE%20is%20calculated%20among%20EEG,methods%20are%20used%20for%20classification)). They then selected the most significant connectivity features and fed them to an SVM classifier. This causal feature approach achieved 91% accuracy on a 29-subject dataset, substantially higher than using conventional features ([\n",
    "            Classification of Right/Left Hand Motor Imagery by Effective Connectivity Based on Transfer Entropy in Electroencephalogram Signal - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10719976/#:~:text=Results%3A)) ([\n",
    "            Classification of Right/Left Hand Motor Imagery by Effective Connectivity Based on Transfer Entropy in Electroencephalogram Signal - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10719976/#:~:text=Results%20show%20that%20the%20best,accuracy)). The directed network revealed physiologically meaningful connections (e.g. causal influence from motor cortex channels to parietal areas during imagined movement), suggesting that the classifier was leveraging true neural interaction patterns rather than idiosyncratic noise. Similarly, nonlinear Granger causality has been used to extract MI features: an approach combining a polynomial-kernel Granger causality analysis with swarm optimization for feature selection improved MI task recognition rates, indicating that nonlinear causal relationships carry discriminative information even when linear methods fail ([Motor imagery EEG task recognition using a nonlinear Granger ...](https://www.researchgate.net/publication/377877304_Motor_imagery_EEG_task_recognition_using_a_nonlinear_Granger_causality_feature_extraction_and_an_improved_Salp_swarm_feature_selection#:~:text=Motor%20imagery%20EEG%20task%20recognition,text%20available)) ([Motor imagery EEG task recognition using a nonlinear Granger ...](https://www.researchgate.net/publication/377877304_Motor_imagery_EEG_task_recognition_using_a_nonlinear_Granger_causality_feature_extraction_and_an_improved_Salp_swarm_feature_selection#:~:text=Motor%20imagery%20EEG%20task%20recognition,text)).\n",
    "\n",
    "- **Affective and Cognitive State Recognition**: BCI applications have expanded to detecting user mental states such as stress or emotion for adaptive interfaces. EEG connectivity networks are being used as features for these purposes. Zhang *et al.* (2022) proposed an EEG emotion recognition scheme based on **cross-frequency Granger causality (CFGC)** features ([Frontiers | EEG emotion recognition based on cross-frequency granger causality feature extraction and fusion in the left and right hemispheres](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.974673/full#:~:text=on%20cross,which%20comprehensively%20considers%20the%20influence)). They computed GC not only within the same frequency band but also from one band to another (e.g. delta-band activity causing changes in alpha-band activity), separated by hemisphere. Using a novel feature decorrelation and fusion strategy, they achieved about 84.9% accuracy on a four-class emotion recognition task, a significant 8.4% improvement over using same-frequency GC features alone ([Frontiers | EEG emotion recognition based on cross-frequency granger causality feature extraction and fusion in the left and right hemispheres](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.974673/full#:~:text=feature%20number%20of%20each%20GC,frequency%20band%20GC%20features)) ([Frontiers | EEG emotion recognition based on cross-frequency granger causality feature extraction and fusion in the left and right hemispheres](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.974673/full#:~:text=on%20the%20DEAP%20emotion%20dataset,frequency%20band%20GC%20features)). This suggests cross-frequency causal interactions (potentially reflecting top-down influences or cross-region coupling) provide additional information about emotional states. Beyond GC, other causal measures like directed phase transfer entropy have been applied to EEG to classify cognitive workload levels, showing that brain network causality can serve as a reliable index of how “hard” the brain is working, more robust than raw EEG power which varies greatly across individuals.\n",
    "\n",
    "- **Neurofeedback and Closed-Loop Control**: In EEG neurofeedback training, users modulate their brain activity with real-time feedback. Causal analysis plays a role in both the design and assessment of such systems. For instance, to enhance **motor rehabilitation BCIs**, researchers have started to identify causal motor networks that could be targeted by feedback. A 2021 study introduced a causal channel selection method for MI-based neurofeedback: they enforced a “causal constraint” such that only channels that causally influence the central motor channels (as per GC analysis) were chosen for feedback and decoding. This led to a reduced channel set (improving convenience) while maintaining or improving performance, presumably because channels with only spurious correlations (and no causal effect on motor areas) were dropped. Although specifics vary, the common theme is that understanding the causal flow of information in the brain can inform which EEG features or channels are truly task-relevant.\n",
    "\n",
    "- **Hybrid BCIs and Artifact Separation**: EEG is often contaminated by artifacts like eye blinks, muscle activity, and movement – all of which can be measured by other sensors (EOG for eye movements, EMG for muscle, IMU for motion). Causal inference can help disentangle these mixed signals. For example, eye movements (EOG) may “cause” certain EEG patterns that mimic intentional signals. By modeling EOG and EEG together in a causal framework, one can attribute variance to ocular causes versus brain causes. In practice, regression or adaptive filters are used for artifact removal, but recent research has begun framing artifact removal as a causal intervention: *remove the effect of the unwanted cause*. One could compute the portion of EEG explained by EOG (using Granger causality or cross-prediction) and subtract it, effectively performing a *do-operator* intervention to set the artifact source to zero. While still a developing area, this perspective is gaining interest as BCIs move to real-world environments where artifacts are rampant. A tangible example is in *mobile BCIs*: when users walk or turn their head while wearing an EEG-driven BCI, IMU sensors capturing head kinematics can be used to causally filter out movement-induced EEG changes, leaving the true neural intention signals. Early multimodal BCIs combining EEG with gait measurements have shown improved decoding of user state (e.g. better differentiation of mild cognitive impairment patients from healthy controls by combining EEG and motion data), highlighting the benefit of multimodal causal understanding in practical settings.\n",
    "\n",
    "### fMRI and fNIRS in BCI: Mapping and Utilizing Causal Brain Networks\n",
    "\n",
    "Functional MRI is not commonly used for real-time BCIs due to its slow temporal resolution and bulky setup, but it provides rich information about brain networks that can inform BCI design and neuroscience-driven BCIs (like training a user to modulate specific brain regions). Moreover, real-time fMRI *is* used in specialized BCI paradigms such as communication with completely locked-in patients or neurofeedback for treating disorders. Causal inference methods have recently been applied to fMRI in these contexts:\n",
    "\n",
    "- **Whole-Brain Causal Discovery for Network Identification**: Understanding large-scale brain network interactions can guide which signals a BCI should monitor or modulate. In a significant advance, Smith *et al.* and others tackled *whole-brain causal discovery* from fMRI data, something long deemed intractable due to high dimensionality. By combining dimension reduction (parcellating the brain into regions) with optimized search algorithms, they uncovered directed networks at the scale of hundreds of regions ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C6%3A%20High%20Dimensionality,million%20neurons%20in%20an%208)) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=therefore%20scale%20to%20hundreds%20or,Lam%20et%20al)). The resulting *causal connectome* revealed hub nodes that send or receive disproportionate causal influence (e.g. anterior cingulate cortex acting as a hub in cognitive control networks) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C7%3A%20High%20Density,Strogatz%2C%201998%2C%20Rawls%20et%20al)) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=2022%5D.%20This%20small,and%20sending%20information%20about%20the)). These methods, validated on simulated and real fMRI datasets, set the stage for data-driven hypothesis generation – for instance, identifying a causal loop between the motor cortex and cerebellum from resting-state fMRI could suggest a new BCI feedback pathway to exploit. While these studies are not BCIs per se, they provide *maps of influence* that advanced BCIs (especially those aiming for cognitive enhancement or therapy) might target. We can imagine a future rehabilitative BCI that not only detects activity in an isolated region but monitors a network’s causal dynamics (e.g. frontal to parietal connectivity) as a biomarker of recovery.\n",
    "\n",
    "- **Real-Time fMRI Neurofeedback and Causality**: Neurofeedback with fMRI typically trains subjects to control the activity of a particular region (e.g. amygdala for emotion regulation) by providing a real-time measure of that region’s activation. Recent work indicates that considering network connectivity, not just local activation, can enhance the efficacy of neurofeedback. Hershaw and colleagues (2021) applied **dynamic causal modeling (DCM)** analysis to fMRI neurofeedback sessions, finding that successful learners exhibited strengthened top-down connectivity from frontal regions to the target region during feedback, whereas poor learners did not ([[EPUB] dynamic causal modeling of an fMRI neurofeedback task - Frontiers](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1212549/epub#:~:text=Frontiers%20www,hierarchical%20cognitive%20control%2C%20which)) ([[EPUB] dynamic causal modeling of an fMRI neurofeedback task - Frontiers](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1212549/epub#:~:text=The%20observed%20stronger%20top,hierarchical%20cognitive%20control%2C%20which)). This suggests a causal network reorganization happened as a result of the training. Similarly, some studies have used Granger causality on the fMRI signals during neurofeedback to ensure that the training not only changes the target region but also its causal influence on other task-relevant areas. For example, real-time fMRI feedback to enhance attention was shown to increase the directed influence of dorsal attention network regions on visual cortex, as measured by GC ([Using connectivity-based real-time fMRI neurofeedback to modulate ...](https://pmc.ncbi.nlm.nih.gov/articles/PMC7013190/#:~:text=,enhancing%20DLPFC%20%E2%80%93%20ACC)) ([Real-time fMRI-based neurofeedback reinforces causality of ...](https://www.sciencedirect.com/science/article/abs/pii/S016801021200003X#:~:text=Real,EC%29%20using%20Granger)). These findings are application-driven: in therapy, one might want to *reinforce a specific causal pathway* (like top-down control from prefrontal to amygdala in anxiety patients). By measuring causal connectivity changes, the BCI can be tuned—potentially even adaptively—to focus on causal circuit modifications rather than raw activation.\n",
    "\n",
    "- **fNIRS and optical imaging**: fNIRS (functional near-infrared spectroscopy) is another non-invasive modality, essentially a portable optical version of fMRI that measures cortical blood flow. It has slower dynamics like fMRI but can be worn in more natural settings. Causal analyses similar to those in EEG have been applied to fNIRS and multimodal EEG-fNIRS. For instance, a hybrid BCI using EEG and fNIRS for motor imagery found that combining features, including directed connectivity features, from both modalities improved accuracy over either modality alone ([Hybrid EEG-fNIRS brain-computer interface based on the non-linear ...](https://www.sciencedirect.com/science/article/pii/S0208521623000256#:~:text=Hybrid%20EEG,EL%29%20approach)) ([The hybrid BCI - Frontiers](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnpro.2010.00003/full#:~:text=A%20simultaneous%20hybrid%20BCI%20can,associated%20with%20two)). They used methods to ensure the EEG and fNIRS signals were temporally aligned and then computed cross-modal Granger influences (whether EEG activity could predict subsequent fNIRS changes or vice versa), hypothesizing that a true neural activation would cause both an EEG change and a delayed fNIRS change. Indeed, trials with consistent EEG→fNIRS causal patterns corresponded to clearer motor imagery attempts. Such cross-modal causal checks can improve confidence that a detected EEG event is genuinely brain-originated (since it had a downstream hemodynamic effect), which is a practical way to reduce false positives in BCIs.\n",
    "\n",
    "- **Using Structural Connectivity Constraints**: An application of causal inference is in personalized BCI tuning. MRI diffusion tractography can map an individual’s structural brain network. Recent research shows that seeding causal models with individual structural connectivity can personalize effective connectivity estimates ([Graph Neural Networks in Brain Connectivity Studies](https://pmc.ncbi.nlm.nih.gov/articles/PMC11763835/#:~:text=Graph%20Neural%20Networks%20in%20Brain,11%5D%2C%202024%2C%20GAT)) ([A Benchmark for Brain Network Analysis with Graph Neural Networks](https://pmc.ncbi.nlm.nih.gov/articles/PMC10079627/#:~:text=Apart%20from%20fMRI%20and%20DTI%2C,DOI%5D)). In practice, one could take a person’s DTI connectivity matrix as a prior graph for causal discovery on their EEG or fMRI data, yielding a personalized causal network. This approach was applied in schizophrenia diagnosis, where integrating fMRI with structural MRI via graph neural networks improved classification of patients vs controls by focusing on connectivity patterns supported by anatomy ([\n",
    "            Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11763835/#:~:text=match%20at%20L1562%20,1%E2%80%936)). For BCI, this could translate to tailored neurofeedback: for example, if a patient’s motor network has weak structural links from premotor to motor cortex, the BCI might especially monitor and encourage causal signals along that pathway to compensate functionally. These ideas are still nascent but exemplify the trend of *application-driven causal modeling* – using whatever information (multimodal data, subject-specific priors) available to enhance the relevance of causal analysis for a given person or use-case.\n",
    "\n",
    "### IMU and Peripheral Data: Causal Context in BCI Control\n",
    "\n",
    "While EEG and fMRI measure brain activity, **IMUs and other sensors** provide context such as user motion or environment changes. Incorporating these into causal models can greatly benefit BCIs that operate in the real world:\n",
    "\n",
    "- **Prosthetic and Exoskeleton Control**: For BCI-driven prosthetic limbs, often EEG or ECoG (invasive) provide motor intentions, but feedback from limb movement (sensed by IMUs or joint sensors) also enters the loop. A challenge is distinguishing whether EEG activity is truly instructive or if the limb moving (via reflex or other cause) generates EEG responses. By building a bidirectional causal model (EEG signals and limb kinematics), one can detect directed causation: EEG should *cause* limb movement commands, and limb movement may *cause* sensory cortex responses. Ensuring the decoder emphasizes the former direction (and not the latter) is vital for stability (to avoid a feedback loop where the BCI accidentally amplifies sensory-evoked EEG as if it were a command). Recent BCI prosthetic studies have started to incorporate state-space models where brain signals and device states co-evolve. In these, explicit causal links from brain to device (forward control) and device to brain (feedback) are modeled, and interventions like perturbing the device can test the learned causality. Although much of this work is in simulation or early trials, it demonstrates a principled use of causal inference to design BCIs that remain robust when the user’s actions influence their brain (closing the loop).\n",
    "\n",
    "- **Mobile Brain/Body Imaging**: As BCIs move to mobile use (e.g. an EEG cap feeding commands to AR glasses while the user walks), **causal inference helps maintain performance across contexts**. If an EEG pattern that triggers an action is actually caused by the user tripping or turning (detected by accelerometer), a causal filter can flag that and prevent a false activation. One example is brain-controlled wheelchair navigation: the user’s EEG might issue a “go” command, but if the wheelchair hits a bump, the resultant jolt causes EEG artifacts that could falsely be interpreted as a stop command. A causal model including the wheelchair’s IMU or speed sensor could attribute the EEG change to the bump (wheelchair -> EEG causation) rather than user intent, and the BCI could ignore it. Experimental prototypes with hybrid EEG-IMU systems indeed show fewer false commands during motion, although formal causal modeling in these systems is just beginning. The concept aligns with **causal inference for reliability**: by explicitly modeling external causes of brain signal changes, the BCI can become context-aware and safer.\n",
    "\n",
    "- **Inferring Cognitive Load and Errors**: IMUs can capture subtle head nods, posture shifts, or reaction time which often correlate with user states like confusion or error recognition. Causal analysis between brain signals and such behavioral measures can enrich passive BCIs (systems that monitor user state). For instance, if a user is performing a task and EEG indicates an error-related potential, one might also see a reflexive head shake captured by IMU. By correlating and timing these, one might deduce a causal chain: the brain’s error detection center fired, causing an involuntary shake. Conversely, if the head movement preceded the EEG, maybe a physical distraction caused both the movement and the EEG pattern secondarily. Understanding these sequences can help a BCI decide whether to intervene (perhaps give assistance if the error was genuine). Recent human factors research with simultaneous EEG and motion capture is exploring these relationships, often using structural equation modeling or Granger analysis to parse out influence directions.\n",
    "\n",
    "In summary, across EEG and fMRI domains, causal methods are yielding practical gains: higher classification accuracies by using causal features, more interpretable models of what brain processes a BCI engages, and improved robustness by accounting for external influences. The advances of the last five years especially show a trend of **integrating multiple signals and leveraging complex models (like deep learning and GNNs) to handle the rich data**, bringing causal analysis from controlled lab data toward real-world BCI deployment. However, significant challenges remain before causal BCIs become mainstream, as we discuss next.\n",
    "\n",
    "## Challenges in the Field\n",
    "\n",
    "Despite the promising advances, applying causal discovery and inference in BCI contexts faces numerous challenges:\n",
    "\n",
    "- **Noisy, Non-Stationary Data:** Non-invasive brain signals are notoriously noisy and variable. EEG signals can have low signal-to-noise ratio, and statistical relationships can change over time (non-stationarity) due to fatigue, learning, or context. This violates assumptions of many causal algorithms which presume a fixed underlying model generating the data. For instance, a causal graph learned in the first 10 minutes of a BCI session might become partially invalid after an hour of use if the user’s brain shifts strategies. One approach to tackle this is using *sliding-window* causal analysis or online adaptation: continually re-estimating causal influences as the session progresses. However, real-time causal discovery is computationally intensive. Another approach is to find *causal invariants* – features of the data generating process that remain stable. Recent research in machine learning suggests using *Invariant Causal Prediction* to find brain features that have stable relationships with outcomes across sessions or subjects. Applying such methods to BCIs could improve robustness to non-stationarity, but doing so in high-dimensional neural data is an open challenge.\n",
    "\n",
    "- **High Dimensionality vs. Limited Samples:** BCI datasets often have many channels or regions (tens to hundreds) but only limited trials or samples (especially for fMRI, where a few hundred time points may cover a session). This makes reliable causal discovery difficult – the search space of possible graphs is enormous (super-exponential in number of variables), and overfitting is a risk. While modern approaches like GNN-based inference ([\n",
    "            Graph Neural Networks in Brain Connectivity Studies: Methods, Challenges, and Future Directions - PMC\n",
    "        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC11763835/#:~:text=Greenlee%20M,Google%20Scholar)) and regularized optimization can handle larger networks than before, scaling to whole-brain or high-density EEG with statistical rigour remains tough. Investigators often reduce the dimensionality (e.g. focusing on known networks or doing PCA/ICA first), which can bias results. Moreover, dense connections in the brain (each region having many neighbors) violate the sparsity assumption of some algorithms, which expect relatively tree-like graphs ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C7%3A%20High%20Density,Strogatz%2C%201998%2C%20Rawls%20et%20al)) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=2022%5D.%20This%20small,and%20sending%20information%20about%20the)). As noted in a recent review, most causal discovery methods see **degraded performance on densely connected graphs** and in small-sample regimes ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=C7%3A%20High%20Density,Strogatz%2C%201998%2C%20Rawls%20et%20al)) ([](https://www.cmu.edu/dietrich/causality/UAI23-accepted%20papers/Causal%20Discovery%20for%20fMRI%20data-%20Challenges,%20Solutions,%20and%20a%20Case%20Study.pdf#:~:text=typically%20connected%20to%20at%20least,Strogatz%2C%201998%2C%20Rawls%20et%20al)). Developing methods that can learn in this “small-n large-p” regime, possibly by incorporating strong priors from neuroscience (like known anatomical connections or hierarchical organization), is an ongoing challenge.\n",
    "\n",
    "- **Latent Confounders and Common Inputs:** In brain networks, we can rarely measure all relevant variables. Unobserved neural sources or external factors can confound the relationships among recorded signals. For example, EEG electrodes might all pick up a strong oscillation caused by a deep brain source (which is unrecorded); this creates spuriously high connectivity among the electrodes. Techniques like FCI can leave some connections undetermined (marking them as potentially confounded) but do not solve the issue. Another example is when analyzing brain-behavior causality: a change in EEG and a change in behavior could both be driven by an unmeasured cognitive event. One mitigation strategy is to include as many relevant measurements as possible (hence multimodal recordings). Still, some variables (like certain cognitive states) are intangible. Researchers sometimes use *graphical models with hidden variables* or treat certain components as latent nodes to at least model the possibility of confounding. This area would benefit from domain-specific tricks; for instance, using fMRI to inform EEG causality (if fMRI sees a deep source, that knowledge could adjust the EEG graph). But integrating these formally is complex. Ensuring causal claims from BCI data are not due to confounds often requires follow-up experiments (e.g. deliberately perturbing one node and seeing if the predicted effect occurs, as a validation).\n",
    "\n",
    "- **Temporal Resolution Mismatches:** Combining modalities like EEG and fMRI or IMU means dealing with very different time scales and delays. Causal inference in asynchronous or event-based data is non-trivial. If an EEG spike causes a behavior within 200 ms, but fMRI signals related to that EEG spike peak 5 seconds later, standard algorithms might struggle to link them properly. Methods like upsampling or using time-delay embedding can help align data, but they introduce uncertainty. Additionally, in fMRI, the hemodynamic delay varies across brain regions and individuals ([Six problems for causal inference from fMRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/19747552/#:~:text=Neuroimaging%20%28e,dependent%20variations%20in%20BOLD)) ([Six problems for causal inference from fMRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/19747552/#:~:text=among%20activity%20in%20these%20regions,of%20procedures%20that%20under%20these)). Mismodeled delays can lead to wrong causality direction (e.g. one region’s signal lags another simply due to slower HRF, not true influence). Some causal discovery methods now incorporate unknown lag as parameters to solve (e.g. algorithms that treat every edge as having a possible lag and optimize those). These are computationally heavy and can overfit, so a key challenge is handling unknown temporal dynamics robustly.\n",
    "\n",
    "- **Feedback Loops and Adaptation:** A BCI in operation is a *closed-loop system*: the brain affects the computer and the computer (through feedback or actions in the world) affects the brain. This means the data is generated by a system with feedback, violating the acyclic assumption. In a closed-loop, we have *cyclic causality* (the user’s brain state causes an action, which in turn causes a new brain state). Standard DAG-based formalisms break down here. Techniques from control theory and causal inference under feedback need to be employed. One approach is to model the loop as a dynamical system (with state-space equations) rather than a static graph. Another is to cut the feedback by design in analysis – e.g. analyze only one direction at a time by providing stimuli or using “open-loop” segments. But ideally, we want to analyze full closed-loop behavior to improve BCI adaptivity. This is an emerging area: recent neurofeedback studies explicitly modeled the brain-computer loop and used that to adjust neurofeedback protocols on the fly, but the math is challenging and usually simplified (assuming linear dynamics or known forms). The challenge is to extend causal inference to *cyclic* interactions, which likely requires blending with reinforcement learning or control optimization algorithms.\n",
    "\n",
    "- **Validation of Causal Findings:** In an engineering sense, the value of causal models for BCI is ultimately measured by improved performance or new capabilities, but in a scientific sense, we also want to know if the discovered causal relations reflect true neurophysiology. Validating causality is inherently hard without doing interventions. Some validation approaches include: comparing against known anatomical connectivity (for fMRI, does the learned causal graph only connect regions that have anatomical links?), stimulation studies (if algorithm says region A causes B, then experimentally stimulating A should change B’s activity), or replication across tasks (if a connection is truly causal for a function, it should appear in different tasks involving that function). Few BCI studies can do invasive validations, but some non-invasive validations exist. For example, transcranial magnetic stimulation (TMS) is sometimes used to perturb a brain area during EEG recordings to test predicted influences. If a causal model predicted A → B, one can apply TMS to A and see if B shows the effect. Incorporating such interventional validation into BCI research is challenging due to complexity and participant burden, but it’s important for separating algorithmic artifacts from real brain causality. \n",
    "\n",
    "- **Computational Complexity and Real-Time Requirements:** Many causal discovery methods are far too slow to run in real-time or even run offline on high-dimension data without supercomputing resources. This creates a gap between algorithm development and practical BCI usage. Simplified or incremental algorithms (that update the causal graph with each new data point) are being explored. Some work uses heuristic approximations (like assuming sparse changes or using ensemble of local causality tests) to speed up processing. There is also interest in **hardware-accelerated or parallel implementations** of causal algorithms to meet real-time demands of BCIs (for example, GPU implementations of conditional independence tests for PC algorithm). Bridging this gap is necessary if we want BCIs that *actively* reason about causality on the fly (e.g. to detect when a causal relationship changes and adapt the decoder accordingly).\n",
    "\n",
    "In summary, applying causal inference in BCI is a double challenge: the data violate many standard assumptions, and the use-case demands (often real-time, personalized, high reliability) push the limits of current methods. Nonetheless, recognizing these challenges has spurred creative solutions and is driving methodological research. As one 2023 review noted, causal inference in cognitive neuroscience must contend with complexity at multiple levels – from experimental design to model selection – and success will likely come from hybrid approaches that combine data-driven learning with insights from neural mechanisms ([Causal inference in cognitive neuroscience - Danks - 2023](https://wires.onlinelibrary.wiley.com/doi/10.1002/wcs.1650#:~:text=2023%20wires,neuroscience%3B%20both%20problems%20have)) ([Cognitive science - David Danks](https://www.daviddanks.org/cognitive-science#:~:text=Cognitive%20science%20,perception%20and%20causal%20inference)). In the final section, we look ahead to how the field might address these challenges and expand the role of causality in BCIs.\n",
    "\n",
    "## Future Directions\n",
    "\n",
    "The convergence of BCI technology with advanced causal learning methods is opening exciting avenues for both fundamental neuroscience and practical device improvement. We highlight several promising future directions:\n",
    "\n",
    "**1. Causal Deep Learning for BCI:** As deep neural networks become common in BCIs (for decoding complex patterns in EEG/fMRI), embedding causal reasoning into these networks is a frontier topic. One idea is *causal representation learning*: designing deep models that learn latent features corresponding to underlying causal factors (e.g. separating EEG components that are causally related to a task from those that are noise or confounds). For example, variational autoencoders with structured latent spaces could enforce that one latent influences another only as a DAG, thereby discovering a causal factorization of EEG data. Recent work by Barmpas *et al.* (2024) takes a **“causal perspective on brainwave modeling”**, arguing for architectures that can disentangle EEG signal sources and identify which are modulators vs responders ([A causal perspective on brainwave modeling for brain-computer interfaces - PubMed](https://pubmed.ncbi.nlm.nih.gov/38621380/#:~:text=A%20causal%20perspective%20on%20brainwave,computer%20interfaces)) ([A causal perspective on brainwave modeling for brain-computer interfaces - PubMed](https://pubmed.ncbi.nlm.nih.gov/38621380/#:~:text=Konstantinos%20Barmpas%20%C2%A0%201%20%C2%A0,30%C2%A0%201%20%C2%A0%202)). Integrating this with modern deep BCI decoders (like CNNs or Transformers used for EEG) could yield models that not only output a decision but also explain it in causal terms (“the decision was driven by activity in node X causing node Y to fire”). In addition, neural networks can be used as function approximators in causal discovery (to handle nonlinearity); *temporal convolutional networks* or *recurrent networks* might detect complex causal lag patterns in EEG automatically. However, ensuring the network’s learned connectivity is *truly causal* remains a challenge – it may require training with interventions or adversarial regularization to avoid merely capturing correlations. \n",
    "\n",
    "**2. Personalized and Adaptive Causal Models:** People have unique neural wiring and can also change with training. Future BCI systems could maintain a **user-specific causal model** of the brain-computer loop that adapts over time. On the hardware side, the increasing availability of wearable multimodal sensors (EEG, EMG, IMU, eye-trackers, etc.) for each user means we can collect rich data to inform such models. An adaptive causal BCI might start with a generic model (based on population data) and then refine connections and weights for the individual by using calibration data and even active experimentation (perhaps subtly varying stimuli to probe the user’s brain responses). This intersects with *transfer learning* in BCIs – instead of just feature transfer, one would transfer an initial causal graph and then adapt it. A potential framework is **active invariant causal learning**, where the BCI can try small *interventions* (e.g. adjust feedback parameters) to test which aspects of the decoding remain invariant, thereby homing in on causal features. With more longitudinal BCI use (e.g. at-home stroke rehab over months), the system could detect if the user’s brain connectivity is changing (maybe new pathways taking over function) and update the decoding strategy accordingly. Achieving this requires algorithms that can update causal beliefs online and handle new emerging structures (for instance, if a previously silent connection becomes active as the user learns). The benefit would be BCIs that don’t need frequent re-calibration from scratch – they continuously self-calibrate by tracking causal structures.\n",
    "\n",
    "**3. Interventional BCIs and Causal Feedback:** Most current BCIs are passive or reactive (they observe and decode). A future direction is **interventional BCI** – one that actively perturbs the brain to achieve a better outcome. This could be through neurostimulation (e.g. tCS, TMS) or through intelligent feedback modulation. Causal modeling is crucial here, because one needs to predict the effect of an intervention (the essence of causal inference). For example, if a BCI predicts a user is entering a freeze-of-gait episode (in Parkinson’s disease), it might deliver a stimulus to the motor cortex to alleviate it. Deciding *where* and *when* to stimulate is a causal decision: one must use a model of brain causality to know that “stimulating region X will lead to desired effect Y”. Research is moving in this direction with closed-loop stimulation experiments guided by models (some use simplified models, like assuming stimulating a certain oscillation will entrain a network). In the future, we might see BCIs that have an internal causal model and perform a sort of mental “counterfactual simulation” to choose an optimal intervention. For instance, a BCI treating depression via neurofeedback might simulate: “if I increase feedback gain on alpha oscillations, will that cause the desired change in limbic network connectivity?” and adjust based on a causal model of alpha’s effect on mood circuitry. Achieving reliable causal models for such use will require extensive multimodal data and probably machine learning to estimate individual differences, but it represents a powerful paradigm shift: BCIs not just reading brain signals, but *writing* to the brain in a principled way.\n",
    "\n",
    "**4. Causal Evaluation Metrics and Standards:** As causal methods become more integrated into BCI pipelines, the community will likely develop new evaluation metrics beyond classification accuracy or information transfer rate. Metrics could include *causal fidelity* (does the model’s causal graph agree with known anatomy or physiology?), *stability across domains* (does the BCI maintain performance under perturbations, indicating it captured true causal features?), and *explainability* (can the BCI provide a causal explanation for each decision, aiding clinical interpretation?). Already, the push for **explainable AI (XAI) in BCI** has drawn attention to methods like analyzing which EEG features most influence a deep net’s output ([Explainable artificial intelligence approaches for brain-computer ...](https://arxiv.org/html/2312.13033v1#:~:text=Explainable%20artificial%20intelligence%20approaches%20for,Computer%20Interfaces%20%28BCIs)). A causal slant on XAI would mean explaining decisions in terms of causal chains (“the system decided the user is imagining move left because activity in motor cortex caused a network activation that typically precedes left-hand movement”). In the next few years, we may see benchmarks or challenge datasets released where the goal is to recover known causal connections from BCI data – analogous to how EEG decoding competitions spurred advances in accuracy, these could spur advances in causal interpretability. The field might also converge on best practices for reporting causal analyses (to avoid overstated claims). For example, requiring that any causal discovery result on EEG/fMRI be supplemented with either a reproducibility check on an independent dataset or a minimal intervention test if possible.\n",
    "\n",
    "**5. Integration with Neuroscience Knowledge Graphs:** Causal discovery doesn’t happen in a vacuum – there’s decades of neuroscience knowledge about which areas connect to which. One future direction is integrating *a priori* knowledge (from atlases, literature, Bayesian priors) with data-driven methods. For instance, a causal discovery algorithm could start with a prior graph where edge probabilities are informed by a database of tract-tracing studies or previous EEG studies, then update those probabilities with the new data. This Bayesian approach would potentially yield more credible results, especially in sparse data settings. Additionally, as large brain projects (e.g. Human Connectome Project, multimodal datasets) become available, meta-analytic causal models might be built that serve as a starting point for BCI-specific models. The marriage of data-driven causal discovery with theory-driven modeling (like neural mass models or biophysical simulations) could also be fruitful: one could use a biophysical brain model to simulate data under various causal regimes, use that to validate a discovery method, and then apply to real data with more confidence. In short, future causal BCI research will likely be highly **multidisciplinary** – drawing from neuroscience theory, machine learning, signal processing, and even ethics (as interventions raise ethical considerations).\n",
    "\n",
    "In conclusion, causal discovery and inference methods are transitioning from purely analytical tools to practical components of BCI systems. They promise BCIs that are not only more accurate but also **safer, more interpretable, and adaptable**. The journey is just beginning: as we continue to surmount the technical challenges, we move closer to BCIs that truly understand the brain’s causal language, enabling more natural and powerful interactions with technology. This integration of causality could ultimately help fulfill the original dream of BCI – not just to read minds, but to interface with the brain on its own terms, for therapeutic and augmentative purposes alike.\n",
    "\n",
    "## References\n",
    "\n",
    "```bibtex\n",
    "@article{Bielczyk2019,\n",
    "  title={Disentangling causal webs in the brain using functional magnetic resonance imaging: A review of current approaches},\n",
    "  author={Bielczyk, Natalia Z. and Uithol, Sebo and van Mourik, Tim and Anderson, Peter and Glennon, Jeffrey and Buitelaar, Jan K. and Geurts, Hilde M.},\n",
    "  journal={Network Neuroscience},\n",
    "  volume={3},\n",
    "  number={2},\n",
    "  pages={237--273},\n",
    "  year={2019},\n",
    "  doi={10.1162/netn_a_00062}\n",
    "}\n",
    "\n",
    "@article{Greenlee2021,\n",
    "  title={A graph neural network framework for causal inference in brain networks},\n",
    "  author={Greenlee, Mark W. and Lang, Eric W.},\n",
    "  journal={Scientific Reports},\n",
    "  volume={11},\n",
    "  pages={8061},\n",
    "  year={2021},\n",
    "  doi={10.1038/s41598-021-87411-8}\n",
    "}\n",
    "\n",
    "@article{Danks2023,\n",
    "  title={Causal inference in cognitive neuroscience},\n",
    "  author={Danks, David and Davis, Isaac},\n",
    "  journal={Wiley Interdisciplinary Reviews: Cognitive Science},\n",
    "  volume={14},\n",
    "  number={5},\n",
    "  pages={e1650},\n",
    "  year={2023},\n",
    "  doi={10.1002/wcs.1650}\n",
    "}\n",
    "\n",
    "@article{Barmpas2024,\n",
    "  title={A causal perspective on brainwave modeling for brain–computer interfaces},\n",
    "  author={Barmpas, Konstantinos and Panagakis, Yannis and Zoumpourlis, Georgios and Adamos, Dimitrios A. and Laskaris, Nikolaos and Zafeiriou, Stefanos},\n",
    "  journal={Journal of Neural Engineering},\n",
    "  volume={21},\n",
    "  number={3},\n",
    "  pages={036018},\n",
    "  year={2024},\n",
    "  doi={10.1088/1741-2552/ad3eb5}\n",
    "}\n",
    "\n",
    "@article{Rezaei2023,\n",
    "  title={Classification of right/left hand motor imagery by effective connectivity based on transfer entropy in electroencephalogram signal},\n",
    "  author={Rezaei, Erfan and Shalbaf, Ahmad},\n",
    "  journal={Basic and Clinical Neuroscience},\n",
    "  volume={14},\n",
    "  number={2},\n",
    "  pages={213--224},\n",
    "  year={2023},\n",
    "  doi={10.32598/bcn.2021.2034.3}\n",
    "}\n",
    "\n",
    "@article{Zhang2022,\n",
    "  title={EEG emotion recognition based on cross-frequency Granger causality feature extraction and fusion in the left and right hemispheres},\n",
    "  author={Zhang, Jing and Zhang, Xueying and Chen, Guijun and Huang, Lixia and Sun, Ying},\n",
    "  journal={Frontiers in Neuroscience},\n",
    "  volume={16},\n",
    "  pages={974673},\n",
    "  year={2022},\n",
    "  doi={10.3389/fnins.2022.974673}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded39e15-bcaf-4435-8812-740b1c876661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9e77e85",
   "metadata": {},
   "source": [
    "Perfect, thank you for clarifying.\n",
    "I will compile a comprehensive overview of all known state-space reconstruction methods specifically used in the time-series domain, including both classical and modern approaches. I will structure it in formal academic style, using $$...$$ for math formatting, so you can easily adapt it for your Master's thesis.\n",
    "I will notify you once the full overview is ready.\n",
    "\n",
    "# State-Space Reconstruction Methods for Time Series\n",
    "\n",
    "## Introduction  \n",
    "**State-space reconstruction (SSR)** refers to the process of constructing a multi-dimensional phase space from time-series data such that the dynamics of an unknown system can be studied in that space. Even if only one or a few variables of a dynamical system are observed, SSR aims to recover the underlying state trajectory (or *attractor*) in a reconstructed state-space that is topologically equivalent (diffeomorphic) to the true state-space of the system ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=In%20the%20study%20of%20dynamical,of%20structures%20in%20%2052)). In essence, SSR attempts to preserve invariant properties of the original dynamical system (e.g. trajectories, Lyapunov exponents, attractor dimensions) under smooth coordinate transformations ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=In%20the%20study%20of%20dynamical,of%20structures%20in%20%2052)). This is crucial in analyzing chaotic systems where we may have limited observations but want to infer the full system’s behavior. \n",
    "\n",
    "Historically, the foundation of SSR was laid by Packard *et al.* (1980), who demonstrated that one can reconstruct an attractor's geometry from a single observed time series using time-delay coordinates ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=,cite%20journal%7D%7D%60%3A%20CS1)). This idea was formalized by **Takens’ embedding theorem** (1981), which provided rigorous conditions for when such a reconstruction is valid ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=Takens%27%20theorem%20is%20the%201981,set%20of%20arbitrary%20box%20counting)). Takens showed that, for a deterministic dynamical system with an attractor of dimension $d_A$, one can generically reconstruct the attractor from a sequence of scalar observations by embedding them in an $m$-dimensional space, as long as the embedding dimension $m$ is sufficiently large (specifically $m > 2\\,d_A$) ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=Assume%20that%20the%20dynamics%20f,dimensional%20Euclidean%20space%20with)). In practical terms, one constructs delay-coordinate vectors from the time series and Takens’ theorem guarantees these vectors form a one-to-one image of the true attractor (i.e. a smooth invertible mapping) under ideal conditions.\n",
    "\n",
    "Below we present a comprehensive overview of state-space reconstruction methods, from the **classical approaches** grounded in time-delay embeddings and linear algebraic decomposition, to **modern developments** leveraging nonlinear manifold learning, Riemannian geometry (SPD matrices), and recent innovations in machine learning. For each method, we describe the technique, underlying assumptions, and discuss strengths and limitations, with references to key literature.\n",
    "\n",
    "## Classical Methods for State-Space Reconstruction  \n",
    "\n",
    "### Time-Delay Embedding (Method of Delays)  \n",
    "The **time-delay embedding** method is the classical approach introduced by Takens. Given a scalar time series $x(t)$ (or a sequence $x_1, x_2, \\dots, x_N$ sampled at some interval), one constructs vectors in $\\mathbb{R}^m$ of the form: \n",
    "\n",
    "$$\n",
    "y(t) \\;=\\; [\\,x(t),\\; x(t+\\tau),\\; x(t+2\\tau),\\;\\dots,\\;x(t+(m-1)\\tau)\\,] \\,\\in \\mathbb{R}^m\\,,\n",
    "$$ \n",
    "\n",
    "where $m$ is the **embedding dimension** and $\\tau$ is the **time delay** (or lag). Each $y(t)$ is a point in the reconstructed state space, and as $t$ varies, these points trace out a trajectory (often called a *shadow attractor*). Takens’ theorem ensures that if $m$ is chosen larger than twice the true attractor dimension, and $\\tau$ is generic, then the mapping from the true state to $y(t)$ is one-to-one and smooth ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=In%20the%20study%20of%20dynamical,of%20structures%20in%20%2052)) ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=Assume%20that%20the%20dynamics%20f,dimensional%20Euclidean%20space%20with)). Intuitively, the delay coordinates provide independent “probes” into the system’s state – by looking at the variable’s value at different times, we capture some of the system’s internal degrees of freedom. \n",
    "\n",
    "**Parameter selection:** Although *in theory* almost any $\\tau$ works for an infinitely long noise-free series ([Delay coordinates in practice](https://www.cs.helsinki.fi/u/ahonkela/dippa/node17.html#:~:text=another%20free%20parameter%20in%20the,information%20between%20the%20consecutive%20samples)), in practice the choice of delay and embedding dimension is critical, especially with noisy or finite data ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=Takens%27%20theorem%20is%20usually%20used,attractor%20would%20be%20destroyed%20by)). Common heuristics include: (a) choosing $\\tau$ as the first minimum of the average mutual information between $x(t)$ and $x(t+\\tau)$ ([Delay coordinates in practice](https://www.cs.helsinki.fi/u/ahonkela/dippa/node17.html#:~:text=another%20free%20parameter%20in%20the,information%20between%20the%20consecutive%20samples)) (to ensure successive coordinates provide new information and are not overly correlated), or sometimes the first zero crossing of the autocorrelation. (b) Increasing $m$ until the fraction of “false neighbors” in the reconstructed space drops to zero ([Illustration of phase space reconstruction (PSR) A Illustration of how... | Download Scientific Diagram](https://www.researchgate.net/figure/Illustration-of-phase-space-reconstruction-PSR-A-Illustration-of-how-downwards_fig2_361881342#:~:text=ones%20are%20%E2%80%98false%E2%80%99%20,neighbours%20a%20state%20has%20can)). The **false nearest neighbors (FNN)** method (Kennel *et al.*, 1992) checks how points that are close in $(m)$-dimensional space become distant when embedded in $(m+1)$-dimensions; if a neighbor was “false” due to projection in too low a dimension, it will separate in higher-$m$ ([False nearest neighbor algorithm - Wikipedia](https://en.wikipedia.org/wiki/False_nearest_neighbor_algorithm#:~:text=Within%20abstract%20algebra%20%2C%20the,3)). The minimum $m$ at which nearly all neighbors are real (no false neighbors) is taken as a sufficient embedding dimension.\n",
    "\n",
    "- **Assumptions:** Requires the system to be deterministic and evolving on a sufficiently low-dimensional attractor (so that $m$ can be chosen finite). The time series should be long and reasonably clean (noise-free Takens theory; small noise is tolerable but no formal guarantee) ([Delay coordinates in practice](https://www.cs.helsinki.fi/u/ahonkela/dippa/node17.html#:~:text=Even%20though%20Takens%27%20theorem%20does,just%20a%20single%20time%20series)) ([Delay coordinates in practice](https://www.cs.helsinki.fi/u/ahonkela/dippa/node17.html#:~:text=another%20free%20parameter%20in%20the,information%20between%20the%20consecutive%20samples)). The observation function (the measured scalar) must be a “generic” observable of the system (i.e. it couples to all degrees of freedom of the attractor) ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=Takens%27%20theorem%20is%20the%201981,with%20other%20classes%20of%20functions)).  \n",
    "- **Strengths:** Simple and widely applicable – no specific model needed. Underpins many nonlinear time series analysis techniques. Takens’ theorem provides a strong theoretical guarantee that topological invariants (e.g. strange attractor structure, Lyapunov exponents, fractal dimensions) are preserved in the reconstructed space ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=In%20the%20study%20of%20dynamical,of%20structures%20in%20%2052)). With methods like FNN and mutual information, one can systematically choose embedding parameters, making the method practical and data-driven ([Delay coordinates in practice](https://www.cs.helsinki.fi/u/ahonkela/dippa/node17.html#:~:text=another%20free%20parameter%20in%20the,information%20between%20the%20consecutive%20samples)) ([False nearest neighbor algorithm - Wikipedia](https://en.wikipedia.org/wiki/False_nearest_neighbor_algorithm#:~:text=Within%20abstract%20algebra%20%2C%20the,3)).  \n",
    "- **Limitations:** **Parameter sensitivity:** Poor choices of $\\tau$ or $m$ can lead to projection overlaps or an overfolded attractor (too small $m$) or overly sparse data (too large $m$) ([Delay coordinates in practice](https://www.cs.helsinki.fi/u/ahonkela/dippa/node17.html#:~:text=The%20embedding%20dimension%20must%20also,first%20minimum%20of%20the%20mutual)). **Noise amplification:** Each additional delay coordinate includes the same noise at a later time; high-dimensional embeddings can become dominated by noise if the signal-to-noise ratio is low ([Delay coordinates in practice](https://www.cs.helsinki.fi/u/ahonkela/dippa/node17.html#:~:text=The%20embedding%20dimension%20must%20also,first%20minimum%20of%20the%20mutual)). **Computational cost:** In high $m$, data is sparse and neighbor searches or density estimates suffer (the “curse of dimensionality”). Also, Takens’ embedding is only a diffeomorphism – it preserves qualitative dynamics but not exact geometric distances or angles (the shape of trajectories can be distorted by the embedding). Finally, Takens’ theory does not strictly hold for stochastic systems (though some extensions exist); for such systems or those with driving noise, delay embeddings can fail to yield a clean attractor. \n",
    "\n",
    "### Singular Spectrum Analysis (SSA)  \n",
    "**Singular Spectrum Analysis** is a method that combines delay embedding with linear decomposition techniques to extract modes of variability from a time series. It can be seen as a data-driven, nonparametric spectral decomposition method ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=In%20practice%2C%20SSA%20is%20a,SSA%20proceeds%20by%20diagonalizing%20the)), closely related to principal component analysis (PCA) on time-delay vectors. In SSA, one first forms the **trajectory matrix** (or Hankel matrix) of the time series using a chosen window length $L$. For a series $X = (x_1, \\dots, x_N)$, the trajectory matrix is: \n",
    "\n",
    "$$ \n",
    "\\mathbf{X} = [\\,X_1 : X_2 : \\cdots : X_K\\,] \\;\\in \\mathbb{R}^{L \\times K}, \\quad \\text{where } X_i = (x_i, x_{i+1}, \\dots, x_{i+L-1})^T, \n",
    "$$ \n",
    "\n",
    "and $K = N - L + 1$ ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=Form%20the%20trajectory%20matrix%20of,displaystyle%20L%5C%21%5Ctimes%20%5C%21K%7D%20matrix)). In other words, each column $X_i$ is an $L$-length delay embedding of the series (with delay 1 between successive points). $\\mathbf{X}$ is a Hankel matrix (constant skew-diagonals) capturing all overlapping length-$L$ segments of the series ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=where%20Image%3A%20%7B%5Cdisplaystyle%20X_%7Bi%7D%3D%28x_%7Bi%7D%2C%5Cldots%20%2Cx_%7Bi%2BL,const)). Next, SSA performs a **singular value decomposition (SVD)** of this trajectory matrix: $\\mathbf{X} = \\sum_{j=1}^L \\sqrt{\\lambda_j}\\,U_j V_j^T$, equivalently diagonalizing the $L\\times L$ lag-covariance matrix $\\mathbf{S} = \\mathbf{X}\\mathbf{X}^T$ to obtain eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_L$ and eigenvectors $U_j$ ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=2nd%20step%3A%20Singular%20Value%20Decomposition,SVD)). The pair $(\\sqrt{\\lambda_j}, U_j)$ is often called an **eigentriple**. The eigenvectors $U_j$ provide an orthonormal basis of the $L$-dimensional embedding space, and projecting the trajectory matrix onto each $U_j$ yields the principal components (also called temporal EOFs in SSA literature). \n",
    "\n",
    "The final steps involve **grouping** and **reconstruction**: one groups subsets of these components (e.g. those corresponding to a signal or trend of interest) and computes a reduced-rank approximation of $\\mathbf{X}$. From this approximated trajectory matrix, the time series is reconstructed by averaging along the diagonals (each anti-diagonal corresponds to one time index) ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=where%20Image%3A%20%7B%5Cdisplaystyle%20X_%7Bi%7D%3D%28x_%7Bi%7D%2C%5Cldots%20%2Cx_%7Bi%2BL,const)). By appropriate grouping, one can separate the original series into a sum of interpretable components: e.g. a slowly varying trend, oscillatory modes (often appearing as pairs of nearly equal $\\lambda_j$ for sinusoidal components), and residual noise ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=SSA%20can%20be%20used%20as,parametric%20form%20of%20these%20components)) ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=nonparametric%20spectral%20estimation%20method,spectral%20decomposition%20of)).\n",
    "\n",
    "- **Assumptions:** SSA is *model-free* and makes minimal assumptions about the data-generating process ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=SSA%20can%20be%20used%20as,parametric%20form%20of%20these%20components)). It does not require stationarity strictly, though trend extraction is a common use. It assumes the time series has structural patterns (like trends or periodicities) that can be captured by a few dominant singular values. Long enough series (relative to $L$) are needed to get stable covariance estimates.  \n",
    "- **Strengths:** **Data-driven decomposition:** It can effectively decompose and reconstruct signals into data-adaptive basis functions, without assuming linearity or specifying frequencies ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=nonparametric%20spectral%20estimation%20method,spectral%20decomposition%20of)). It’s particularly powerful for extracting oscillatory modes (even if nonstationary or amplitude-modulated) and for denoising – truncating small singular values filters out noise. SSA is related to Karhunen-Loève decomposition, maximizing variance capture ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=nonparametric%20spectral%20estimation%20method,spectral%20decomposition%20of)), and thus often captures the “statistical dimension” of the system ([Singular spectrum analysis in nonlinear dynamics, with applications ...](https://www.sciencedirect.com/science/article/abs/pii/0167278989900778#:~:text=Singular%20spectrum%20analysis%20in%20nonlinear,phenomena%20reflected%20by%20the%20data)). Unlike a pure delay embedding, SSA provides an *optimal linear subspace* in which to project the dynamics (similar to PCA on delay vectors). This can reveal the rank of the system dynamics (e.g. periodic series yield a low-rank trajectory matrix with pairs of equal singular values). Also, SSA has a **multivariate extension (MSSA)** that can jointly embed multiple time series for coupled analysis, improving detection of common modes.  \n",
    "- **Limitations:** **Linear reconstruction:** SSA relies on linear subspaces. If the dynamics are highly nonlinear, important structure might not align with the top singular vectors. For example, SSA might require many components to represent a strange attractor if its geometry is not well-approximated by any low-dimensional linear subspace. There is no guarantee that the principal components correspond to actual state variables – they are orthogonal directions of maximal variance, which might mix various dynamical coordinates. **Parameter choices:** The window length $L$ is a crucial parameter – too small and one cannot capture long-period dynamics; too large and one might overembed and include excessive noise or unrelated variance. Guidance is often heuristic (e.g. set $L$ about half the time series length or around expected periodicities), but not always obvious. **Overlaps with AR models:** SSA results can sometimes be interpreted in terms of ARIMA models (e.g. SSA can implicitly perform something akin to fitting an AR model of order $L$, since that’s the rank of the covariance matrix) – this means it may struggle with series that require a time-varying model or truly nonlinear coupling beyond second-order correlations. Nonetheless, SSA remains a robust general tool for state-space reconstruction and signal extraction, especially for moderately noisy chaotic signals ([Singular spectrum analysis in nonlinear dynamics, with applications ...](https://www.sciencedirect.com/science/article/abs/pii/0167278989900778#:~:text=Singular%20spectrum%20analysis%20in%20nonlinear,phenomena%20reflected%20by%20the%20data)).\n",
    "\n",
    "## Modern Developments and Advanced Methods  \n",
    "\n",
    "### Manifold Learning-Based Embeddings  \n",
    "Classical delay embeddings and SSA use linear or fixed transformations. **Manifold learning** techniques, developed largely in the 2000s, enable *nonlinear* dimensionality reduction. These methods attempt to discover a low-dimensional manifold on which the high-dimensional data lie, preserving intrinsic geometric structure. In the context of time series, one can apply manifold learning either to **time-delay vectors** or to inherently high-dimensional time-series data, to find a more compact representation of the system’s state. The key idea is that if the system has an attractor of dimension $d$, the data (in some embedding space) essentially lie on an $d$-dimensional manifold $\\mathcal{M}$, and algorithms can *learn* coordinates on $\\mathcal{M}$ that flatten out the nonlinear twists of the attractor.\n",
    "\n",
    "Common manifold learning algorithms include **Locally Linear Embedding (LLE)**, **Isomap**, **Laplacian Eigenmaps**, **Diffusion Maps**, **t-SNE/UMAP**, among others. These are unsupervised algorithms that take a set of data points in a high-$D$ space and produce coordinates in a lower $d$-dimensional space. They typically construct a graph or neighborhood relations among the data points and then optimize some objective to preserve local distances or global geodesic structure. For example, *Isomap* finds the geodesic distances between all points (distances along the data manifold, approximated by shortest paths in a neighbor graph) and then uses classical MDS to embed points in $d$ dimensions preserving those distances. *LLE* constructs each data point as a weighted linear combination of its $k$ nearest neighbors (in the original space) and then finds $d$-dimensional coordinates that best reconstruct each point from the same neighbor weights ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=At%20the%20first%20step%2C%20we,an%20approximation%20to%20the%20intrinsic)). *Diffusion Maps* builds a Markov transition matrix between nearby points and uses the leading eigenvectors of this diffusion operator as coordinates (embedding points such that Euclidean distance in the new space reflects “diffusion distance” in the original data) ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=At%20the%20first%20step%2C%20we,an%20approximation%20to%20the%20intrinsic)).\n",
    "\n",
    "When applying these to time-series, a typical approach is: first, embed the time series in some space (possibly using delays) to get a point cloud $\\{y(t)\\}$ that samples the attractor. Then run a manifold learning algorithm on $\\{y(t)\\}$. The result will be a set of coordinates $\\{\\xi(t)\\}$ in $\\mathbb{R}^d$ (with $d < m$ potentially) that parametrizes the data manifold. Ideally, $d$ will equal the true attractor dimension or a useful reduced dimension. Manifold learning can thus **discover a more parsimonious state-space** by unfolding nonlinear correlations in the delay-embedded data. Recent studies use such methods to construct reduced models: for instance, using LLE or diffusion maps to first embed the dynamics, then modeling evolution in that intrinsic space ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=At%20the%20first%20step%2C%20we,an%20approximation%20to%20the%20intrinsic)) ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=Riemannian%20metric%2C%20which%20defines%20the,one%20hand%2C%20the%20existence%20of)).\n",
    "\n",
    "- **Assumptions:** Assumes the data indeed lie on or near a smooth low-dimensional manifold (the attractor) embedded in a higher-dimensional space ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=observations,their%20snapshots%20in%20time%20that)) ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=At%20the%20first%20step%2C%20we,an%20approximation%20to%20the%20intrinsic)). Requires a sufficiently large and representative set of sample points on the attractor – manifold learning is data-hungry and works best when the attractor is well-sampled. Typically considered an *unsupervised learning* approach, it doesn’t assume a specific dynamics, just the geometrical smoothness of the dataset. For algorithms like Isomap or LLE, one must choose neighborhood size or kernel scale appropriately; the results assume the manifold is roughly uniform (no extremely folded or sparse regions that violate the algorithm’s assumptions).  \n",
    "- **Strengths:** **Nonlinear dimensionality reduction:** These methods can recover curvilinear coordinates of the attractor that linear methods (PCA/SSA) might miss. They preserve *intrinsic geometry* – e.g. LLE preserves local neighbor relations, Isomap preserves global geodesic distances ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=At%20the%20first%20step%2C%20we,an%20approximation%20to%20the%20intrinsic)), Diffusion Maps preserve a notion of connectivity. This often means the learned coordinates correspond to meaningful system variables (e.g. a phase or an amplitude of oscillation, etc.) in a way that linear projections couldn’t capture. Manifold learning can thus reveal the true degrees of freedom of a complex system. It has been successfully applied to systems like human motion, climate patterns, and even empirical high-dim time series to find low-dim structure ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=the%20points%20is%20maintained%20as,based%20on%20the%20resulting%20embedding)) ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=models%20,dimensional%20%E2%80%9Csufficiently%20smooth%E2%80%9D)). Another strength is that these methods are fairly *general*: one can plug in delay vectors, frequency-domain features, or any representation of time-series segments and still apply the algorithm. For example, one can embed each time window and use Diffusion Maps to obtain a nonlinear “eigencoordinate” that often corresponds to the dominant mode of variability.  \n",
    "- **Limitations:** **Computational complexity:** Many manifold algorithms scale poorly with number of points (e.g. $O(N^2)$ or worse for computing distances/eigenvectors), making very long time series challenging without downsampling or using approximations (like Nyström extension ([[1802.08762] Diffusion Maps meet Nyström - arXiv](https://arxiv.org/abs/1802.08762#:~:text=,roughly%20two%20to%20four%20times))). **No dynamics explicit:** These methods purely capture geometry; they do not by themselves yield a dynamic model in the new coordinates (though one can fit a model afterward). Also, they often ignore the sequential nature of time-series data – they treat points as an unlabeled cloud. This can be a drawback if the data are not densely sampled on the attractor (time-series often give a highly correlated sweep through the attractor, rather than independent samples). Some adaptations, like time-coupled diffusion maps or incorporating temporal neighbors, have been proposed to respect time ordering ([Time coupled diffusion maps - ScienceDirect.com](https://www.sciencedirect.com/science/article/pii/S1063520317301276#:~:text=Time%20coupled%20diffusion%20maps%20,of%20a%20diffusion%20operator)). **Parameter tuning:** One must choose neighbor count $k$ or kernel width, which can significantly affect results (too small neighborhood – noise and fragmentation; too large – fails to capture local structure). **Inverse mapping challenge:** After embedding into $d$ dimensions, reconstructing the original time series or high-dimensional state from the manifold coordinates is non-trivial. Unlike PCA (which has a direct inverse transform), manifold methods often require interpolation or separate learning to “lift” back to the original space ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=evolution%20of%20the%20embedded%20time,Lee)). In practice, one might use radial basis functions or neural nets to map the low-d coordinates to original observables ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=evolution%20of%20the%20embedded%20time,Lee)), but this is an added layer of complexity. Despite these issues, manifold learning has opened new ways to perform SSR, such as forecasting on the manifold (e.g. building regressors in the reduced space to predict future states) ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=Riemannian%20metric%2C%20which%20defines%20the,one%20hand%2C%20the%20existence%20of)), and revealing genuine low-dimensional behavior in very complex systems that would be obscured in raw observations.\n",
    "\n",
    "### Riemannian & Geometric Approaches (SPD Matrices and Grassmann Manifolds)  \n",
    "Another modern avenue for SSR involves representing segments of time series as geometric objects like **covariance matrices** or subspaces, which lie on curved manifolds (specifically, Riemannian manifolds such as the manifold of Symmetric Positive Definite matrices or the Grassmann manifold of subspaces). The motivation is that certain features of dynamical systems – especially in high-dimensional or multivariate settings – are naturally encoded by covariance or subspace structure, and by considering the appropriate geometry one can better compare and analyze these features.\n",
    "\n",
    "**SPD matrix representation:** Given a $d$-dimensional multivariate time series or a $d$-channel signal (e.g. $d$ EEG sensors) and an embedding dimension $D$, one can form $D$-lagged vectors as before: $s_e(t) = [s_1(t), ..., s_d(t),\\; s_1(t+\\tau),...,s_d(t+\\tau),\\;\\dots,\\;s_1(t+(D-1)\\tau),...,s_d(t+(D-1)\\tau)]^T \\in \\mathbb{R}^{dD}$. This is essentially a phase-space reconstruction (PSR) applied to each channel. From a window of such vectors, one can compute a **sample covariance matrix** $R = \\frac{1}{N}\\sum_{i=1}^N s_e(t_i) s_e(t_i)^T$, which will be a $dD \\times dD$ SPD matrix (symmetric positive-definite). This covariance encapsulates both the spatial correlations between channels *and* temporal correlations up to lag $(D-1)\\tau$. In effect, it’s an “augmented covariance” that includes dynamical information. Such an approach was formalized by Barachant *et al.* (2013) in the context of Brain-Computer Interface, called the **Augmented Covariance Matrix (ACM)** method: it treats the covariance of time-delayed EEG signals as a feature that lives on the SPD manifold, and uses Riemannian geometry for classification. Because the space of SPD matrices is a Riemannian manifold (with well-defined geodesic distances, e.g. log-Euclidean metric), one can average these matrices (Fréchet mean) or compute distances in a way that respects the curvature and positive-definiteness constraints. In SSR terms, each SPD matrix can be seen as a representation of the local state dynamics. By comparing SPD matrices from different time windows, one can quantify similarity of dynamical states. For example, two chaotic states might have distinct covariance structure of variables over a short window, which could be discerned via Riemannian distance on SPD. In practice, this approach has been very successful in scenarios like EEG where the true state is high-dimensional and noisy; the covariance provides a robust signature of the state that filters out high-frequency noise.\n",
    "\n",
    "**Grassmann manifold (subspace) representation:** Instead of the full covariance, one can represent the **subspace** spanned by certain vectors associated with the time series. A prime example: in SSA or subspace system identification, we obtain an orthonormal basis of principal components (or an observability subspace) for the dynamics. The column space spanned by, say, the first $r$ singular vectors $U_1,\\dots,U_r$ of the trajectory matrix is an $r$-dimensional subspace of $\\mathbb{R}^L$. This subspace itself can be treated as a point on a Grassmann manifold $\\mathcal{G}(r, L)$ (the set of all $r$-dimensional subspaces in $\\mathbb{R}^L$) ([Analysis of Temporal Tensor Datasets on Product Grassmann Manifold](https://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Batalo_Analysis_of_Temporal_Tensor_Datasets_on_Product_Grassmann_Manifold_CVPRW_2022_paper.pdf#:~:text=a%20respective%20linear%20subspace%20it,A%20single%20manifold%20expresses%20geo%024869)). The Grassmann manifold has a natural Riemannian metric (derived from principal angles between subspaces), so one can measure distances between two subspaces (for instance, two different time series might yield two subspaces capturing their dynamics, and one can compute how “far apart” these dynamics are on $\\mathcal{G}$). This concept is used in **subspace-based clustering of time series** and in linear system identification: each linear dynamical system of order $r$ corresponds to an $r$-dimensional observability subspace. By embedding an unknown system’s data and estimating an $r$-dim subspace, one effectively reconstructs a linear state-space. Clustering on Grassmann then groups systems with similar subspaces ([Analysis of Temporal Tensor Datasets on Product Grassmann Manifold](https://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Batalo_Analysis_of_Temporal_Tensor_Datasets_on_Product_Grassmann_Manifold_CVPRW_2022_paper.pdf#:~:text=a%20respective%20linear%20subspace%20it,A%20single%20manifold%20expresses%20geo%024869)). Recent reviews categorize various Grassmannian methods for multivariate time series clustering and modeling, highlighting that many algorithms differ by how they construct the subspace (e.g., via SVD of Hankel matrix, via autoregressive model subspace, or via frequency domain) but ultimately compare subspaces on $\\mathcal{G}$ ([Multivariate Time Series Clustering with State Space Dynamical Modeling and Grassmann Manifold Learning: A Systematic Review on Human Motion Data](https://www.mdpi.com/2076-3417/15/1/43#:~:text=clustering%20challenging,systematic%20overview%20of%20Grassmann%20MTS)).\n",
    "\n",
    "To summarize these geometric approaches:\n",
    "\n",
    "- **Assumptions:** These methods assume that relevant information about the state resides in second-order statistics (for SPD) or in a linear subspace of some feature space. The SPD approach effectively assumes the process is characterized well by covariance (which is true for e.g. Gaussian signals or linearly embedded dynamics, but not always for strongly nonlinear relationships beyond covariance). Grassmann/subspace methods often assume an approximate linear model, or at least that the subspace spanned by a few delay vectors is meaningful and can be compared between systems. Both approaches assume one can accumulate enough data in a window to estimate the covariance or subspace reliably (requiring ergodicity or stationarity over that window).  \n",
    "- **Strengths:** **Robustness and geometry:** Representing states as SPD matrices can be robust to noise (since covariance is an aggregation, it averages out zero-mean noise to an extent). Using Riemannian geometry on SPD yields more meaningful distances than, say, flattening the matrix and using Euclidean norm. For example, the Riemannian distance between covariances $R_1$ and $R_2$ can be defined as $\\|\\log(R_1^{-1/2} R_2 R_1^{-1/2})\\|_F$, which accounts for eigenstructure differences without violating positive-definiteness. Such metrics have led to state-of-the-art classification in EEG, as different mental states produce subtly different covariance patterns. In SSR context, this means we can distinguish dynamical regimes or behaviors by their “footprint” in covariance. The **ACM method** explicitly combines Takens embedding with SPD features, enriching the covariance with dynamical information (time-lagged correlations). Grassmannian representations allow the use of powerful tools from differential geometry to interpolate or cluster subspaces; one can compute geodesic mean subspace (Karcher mean) which gives an average dynamical model from multiple trials, for instance. **Reduced complexity:** By focusing on covariance or subspace, one reduces dimensionality dramatically (e.g. an $r$-dim subspace vs potentially a high-dimensional raw embedding). This sidesteps some curse of dimensionality and allows use of well-defined global coordinates (principal angles) to compare states.  \n",
    "- **Limitations:** **Information loss:** A covariance (SPD) captures only second-order moments; it will miss any phase information or higher-order structure. Two very different attractors could conceivably have similar covariance structure, so SPD features might alias distinct dynamics. For highly nonlinear or non-Gaussian dynamics, second-order statistics might be insufficient. Also, computing covariance over a sliding window introduces a timescale: too short a window and covariance is noisy, too long and it averages over dynamical changes (smearing out the trajectory details). **Not one-to-one:** Representing a state by a covariance matrix is many-to-one; you cannot reconstruct the original time-series segment uniquely from its covariance. Thus this is more useful for *recognition* or characterization (as in classification, clustering) than for reconstructing detailed trajectories. **Geometric complexity:** While the math of SPD/Grassmann is well-developed, implementing it requires care (e.g. one must use appropriate manifold optimization or distance formulas). For practitioners not familiar with Riemannian geometry, this can be a barrier. Additionally, estimating an SPD of size $dD \\times dD$ needs enough data ($N \\gg dD$); in high dimensions, one might need regularization (sometimes one uses shrinkage or assumes structure like block-Toeplitz as in the ACM which yields a further manifold structure). In summary, SPD/Grassmann approaches bring the power of geometry into SSR, excelling in scenarios where linear correlation patterns distinguish states (e.g. brain signals, oscillatory systems), but they are supplementary to, rather than replacements for, direct coordinate reconstruction.\n",
    "\n",
    "### Deep Learning and Other Recent Innovations  \n",
    "In the last decade, researchers have begun applying machine learning, especially deep neural networks, to state-space reconstruction problems. One notable direction is using **autoencoders** and **latent-variable models** to learn an embedding of the time series that captures the system’s state. For example, Gilpin (2020) introduced a deep autoencoder framework to reconstruct chaotic attractors from data ([Deep reconstruction of strange attractors from time series](https://papers.nips.cc/paper_files/paper/2020/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html#:~:text=dimensional%20time%20series%20of%20measurements,electricity%20usage%2C%20neural%20spiking%2C%20and)). In his approach, a neural network encoder takes in a segment of time series (such as a delay vector or multivariate window) and produces a low-dimensional latent representation, while a decoder tries to reconstruct the original segment. The key innovation was adding a **latent-space loss** inspired by traditional SSR techniques (specifically, a loss to penalize false neighbors in the latent space) ([Deep reconstruction of strange attractors from time series](https://papers.nips.cc/paper_files/paper/2020/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html#:~:text=dimensional%20time%20series%20of%20measurements,electricity%20usage%2C%20neural%20spiking%2C%20and)). This regularization forces the autoencoder to produce a latent representation that is topologically consistent with the true attractor – points that are nearby in the latent space correspond to nearby states in the original system. The result is that the learned latent variables form a faithful reconstruction of the strange attractor, often outperforming classical delay embeddings in preserving the structure ([Deep reconstruction of strange attractors from time series](https://papers.nips.cc/paper_files/paper/2020/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html#:~:text=general%20embedding%20technique%20for%20univariate,technique%20for%20exploratory%20data%20analysis)). Such learned embeddings can also handle noise and complex observation functions by leveraging the approximation power of neural networks.\n",
    "\n",
    "Another class of deep methods are **Recurrent Neural Networks (RNNs)** or **Variational State Space Models** that essentially learn a state-space internally. For instance, an LSTM trained to predict future values of a time series will internally maintain a hidden state that, in theory, encodes the necessary information from the past. In some cases, this hidden state corresponds to a form of state-space reconstruction (though typically not interpretable without further analysis). Variational approaches, like variational autoencoders for time series or deep Markov models, explicitly model a latent state that evolves through time, trained by maximizing likelihood of the observations. These models (e.g. the **Deep Variational Bayes Filter** or the **Latent ODE** models) combine neural networks with assumptions of a latent dynamical system. They are powerful in handling noise and uncertainty (since they naturally incorporate a probabilistic state), and they can model highly nonlinear dynamics by learning complex transition and observation functions. However, they may not guarantee the learned latent space has any simple relationship (like diffeomorphism) to the true physical state – it’s a black-box representation optimized for prediction.\n",
    "\n",
    "On a different front, **Koopman operator theory** has re-emerged as a modern tool for SSR. The Koopman operator is an infinite-dimensional linear operator that advances observable functions of the state. Modern techniques try to approximate Koopman operators with finite-dimensional linear models by choosing appropriate observables (features of the time series). For example, **Dynamic Mode Decomposition (DMD)** and its variants can be seen as ways to find a linear representation of nonlinear dynamics by analyzing time-shifted data matrices (related to delay embedding). Extended DMD uses time-delay coordinates as features (which implicitly are observables of the system at different times) and computes a best-fit linear map, yielding approximate Koopman eigenmodes. This approach can embed the dynamics into a space of dimension equal to the number of chosen observables, where the evolution is (approximately) linear. Kutz *et al.* (2018) combined delay embeddings with DMD for spatio-temporal systems, essentially using the Hankel (delay) matrix of a PDE simulation, performing SVD and regression to approximate the Koopman operator; this provided a reduced-order linear model of the dynamics on the manifold, and then original states were reconstructed via a linear mapping from the reduced space ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=Kuramoto,molecular%20enhanced%20sampling%20method%20based)) ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=the%20high,The)). The benefit of such approaches is that one can leverage linear systems theory (eigenvalues give growth/decay rates, etc.) even for a nonlinear system, and sometimes only a few modes are needed (Koopman “modes” corresponding to coherent structures in the system). The limitation is that one has to guess a suitable set of observables (time delays are a generic choice by Takens, or one can use polynomial features, radial basis functions, etc.). If the chosen observables span a Koopman-invariant subspace, the method works beautifully; if not, the linear model will be poor.\n",
    "\n",
    "**Reservoir computing** is another innovation relevant to SSR: it projects the time series into the high-dimensional state of a fixed (random) nonlinear dynamical system (the “reservoir”) and then trains a linear readout to predict the next step or some output. Intriguingly, reservoir computers have been shown to internally emulate the state-space of the input system; for instance, a well-trained reservoir can continue a chaotic sequence, effectively having reconstructed an equivalent state. This is more a modeling tool, but it highlights how even random embeddings coupled with learning can perform SSR.\n",
    "\n",
    "We also mention **Convergent Cross Mapping (CCM)** as an application of SSR in causality inference. CCM (Sugihara *et al.*, 2012) uses Takens’ embedding on two time series to test for causal interactions: if $X$ causes $Y$, then the reconstructed state-space from $Y$ (its shadow manifold) can be used to estimate states of $X$ (cross mapping). The method rests on the idea that if two variables belong to the same dynamical system, their attractor reconstructions are diffeomorphic; one attractor’s nearest neighbors correspond to neighbors on the other’s attractor ([Deep reconstruction of strange attractors from time series](https://papers.nips.cc/paper_files/paper/2020/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html#:~:text=dimensional%20time%20series%20of%20measurements,electricity%20usage%2C%20neural%20spiking%2C%20and)). CCM thus leverages SSR to detect a directional influence, something not possible with linear correlation alone.\n",
    "\n",
    "- **Assumptions:** **Deep learning approaches** assume availability of sufficient training data and that a neural network can be structured to capture the dynamics. Autoencoders assume the existence of a relatively low-dimensional latent representation that can generate the observations (similar to manifold assumption). RNN-based models assume some form of Markovian dynamics in latent space. These approaches typically need stationary dynamics or at least a well-defined generating process. **Koopman/DMD approaches** assume the dynamics can be embedded in a (possibly higher-dimensional) linear system – effectively that the observables live in a linear span closed under the system’s Koopman operator. This usually implies the system has some approximately linearizable behavior or one uses rich observables. **Reservoir computing** assumes the input can drive a generic dynamical system in a way that its state becomes a proxy for the true state – a rather loose assumption but often empirically valid.  \n",
    "- **Strengths:** **Flexibility and power:** Deep neural networks can approximate very complex nonlinear mappings, making them capable of reconstructing dynamics where traditional methods struggle (e.g. systems with discontinuities or strongly non-smooth behavior). The incorporation of domain-based loss functions (like the FNN-based loss) combines the best of both worlds: theoretical guarantees and neural flexibility ([Deep reconstruction of strange attractors from time series](https://papers.nips.cc/paper_files/paper/2020/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html#:~:text=dimensional%20time%20series%20of%20measurements,electricity%20usage%2C%20neural%20spiking%2C%20and)). These methods also naturally handle **multivariate and high-dimensional observations** – for instance, images or video sequences (one can train an autoencoder to find a low-dim dynamical description of video of a physical process). Koopman and DMD methods provide **interpretable linear models** (eigenvalues relate to oscillatory frequencies or decay rates, modes to spatial patterns), making analysis of complex systems more interpretable in some cases. Reservoir computing and deep RNNs have been extremely successful in forecasting chaotic systems (even outperforming classic methods) by implicitly learning the state update rule. **Stochastic and control-ready:** Many modern methods integrate probabilistic frameworks, handling process noise and measurement noise (e.g. deep state-space models can naturally estimate uncertainty in the state). This makes SSR applicable to not just deterministic chaos but also to real-world systems with stochastic influences.  \n",
    "- **Limitations:** **Data and training requirements:** Deep learning models often require large datasets and careful training to avoid overfitting or trivial solutions. They may not extrapolate well beyond the range of training data (whereas traditional SSR is non-parametric in a sense and might generalize better on principle). The learned representation might lack *guaranteed* correspondence to the true state variables – it is possible to learn a technically valid latent space that is a complicated function of the true state (still useful for prediction but hard to analyze scientifically). **Less theory:** Unlike Takens’ theorem, we don’t have a general theorem that an autoencoder will find a diffeomorphic embedding of the attractor (although adding losses and constraints can enforce this approximately). Results can be hard to reproduce and depend on network architecture and hyperparameters. **Koopman methods limitations:** Selecting observables is an art; if important nonlinear features are omitted, the model fails. Koopman approaches sometimes yield very high-dimensional embeddings (even infinite if using kernel methods), which, while linear, are not necessarily easier to work with unless truncated – truncation breaks the guarantee of linearity. **Computational cost:** Training deep models or computing Koopman eigenfunctions can be intensive (though once trained, usage is fast). Finally, many of these advanced methods are new and still being refined; they may not yet be as plug-and-play as classical delay embedding or SSA, requiring expertise in both the problem domain and machine learning. \n",
    "\n",
    "## Conclusion  \n",
    "State-space reconstruction remains a cornerstone in the analysis of nonlinear time series and dynamical systems. Classical methods like time-delay embedding and singular spectrum analysis provide the theoretical and practical foundation, allowing us to reconstruct attractors and identify dynamics from scalar observations ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=In%20the%20study%20of%20dynamical,of%20structures%20in%20%2052)) ([Singular spectrum analysis - Wikipedia](https://en.wikipedia.org/wiki/Singular_spectrum_analysis#:~:text=In%20practice%2C%20SSA%20is%20a,SSA%20proceeds%20by%20diagonalizing%20the)). Modern developments have greatly expanded the toolkit: nonlinear manifold learning preserves the true geometry of attractors in reduced coordinates ([Time Series Forecasting Using Manifold Learning](https://arxiv.org/pdf/2110.03625#:~:text=At%20the%20first%20step%2C%20we,an%20approximation%20to%20the%20intrinsic)), Riemannian approaches leverage the geometry of covariance and subspace manifolds to compare complex dynamics ([Analysis of Temporal Tensor Datasets on Product Grassmann Manifold](https://openaccess.thecvf.com/content/CVPR2022W/VDU/papers/Batalo_Analysis_of_Temporal_Tensor_Datasets_on_Product_Grassmann_Manifold_CVPRW_2022_paper.pdf#:~:text=a%20respective%20linear%20subspace%20it,A%20single%20manifold%20expresses%20geo%024869)), and deep learning offers powerful ways to automatically learn embeddings and dynamical models from data ([Deep reconstruction of strange attractors from time series](https://papers.nips.cc/paper_files/paper/2020/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html#:~:text=dimensional%20time%20series%20of%20measurements,electricity%20usage%2C%20neural%20spiking%2C%20and)). Each method comes with its assumptions, strengths, and limitations, as we have detailed. In practice, the choice of method (or a combination of methods) depends on the system characteristics (noise level, dimensionality, data length, etc.) and the analysis goal (e.g. qualitative understanding vs. forecasting vs. classification). Often, researchers will employ multiple approaches in tandem – for instance, using delay embedding to inform a neural network structure, or using SSA to initialize a nonlinear embedding. \n",
    "\n",
    "In all cases, the guiding principle is the same: even if we observe only time series, we aim to uncover the underlying *state space* in which the dynamics become clear. By doing so, we can apply the rich theory of dynamical systems to real-world data – identifying chaos, quantifying stability, inferring causal relations, and predicting future behavior – truly bridging observational data with the abstract state-space models that govern them. The ongoing innovations promise even more robust and automated ways to perform state-space reconstruction, pushing the boundaries of what we can learn from complex time-series data. \n",
    "\n",
    "**References:** (Key foundational papers and resources in raw URLs)  \n",
    "- Packard, N., Crutchfield, J., Farmer, D., & Shaw, R. (1980). *Geometry from a Time Series*. **Phys. Rev. Lett. 45**(9): 712–716 ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=,cite%20journal%7D%7D%60%3A%20CS1)).  \n",
    "- Takens, F. (1981). *Detecting Strange Attractors in Turbulence*. In **Dynamical Systems and Turbulence**, Warwick 1980, Lecture Notes in Mathematics 898, Springer ([Takens's theorem - Wikipedia](https://en.wikipedia.org/wiki/Takens%27s_theorem#:~:text=Takens%27%20theorem%20is%20the%201981,set%20of%20arbitrary%20box%20counting)).  \n",
    "- Kennel, M. B., Brown, R., & Abarbanel, H. (1992). *Determining Embedding Dimension for Phase-Space Reconstruction Using a Geometrical Construction*. **Phys. Rev. A 45**: 3403–3411 ([False nearest neighbor algorithm - Wikipedia](https://en.wikipedia.org/wiki/False_nearest_neighbor_algorithm#:~:text=Within%20abstract%20algebra%20%2C%20the,3)).  \n",
    "- Fraser, A. & Swinney, H. (1986). *Independent Coordinates for Strange Attractors from Mutual Information*. **Phys. Rev. A 33**: 1134–1140 (delay selection) ([Delay coordinates in practice](https://www.cs.helsinki.fi/u/ahonkela/dippa/node17.html#:~:text=another%20free%20parameter%20in%20the,information%20between%20the%20consecutive%20samples)).  \n",
    "- Broomhead, D. & King, G. (1986). *Extracting Qualitative Dynamics from Experimental Data*. **Physica D 20**: 217–236 (introduced SSA in dynamical systems context).  \n",
    "- Vautard, R. & Ghil, M. (1989). *Singular Spectrum Analysis in Nonlinear Dynamics, with Applications to Paleoclimatic Time Series*. **Physica D 35**: 395–424 ([Singular spectrum analysis in nonlinear dynamics, with applications ...](https://www.sciencedirect.com/science/article/abs/pii/0167278989900778#:~:text=Singular%20spectrum%20analysis%20in%20nonlinear,phenomena%20reflected%20by%20the%20data)).  \n",
    "- Coifman, R. & Lafon, S. (2006). *Diffusion Maps*. **Appl. Comput. Harmon. Anal. 21**: 5–30 (nonlinear dimensionality reduction for dynamical datasets).  \n",
    "- Barachant, A., Bonnet, S., Congedo, M., & Jutten, C. (2013). *Classification of covariance matrices using a Riemannian-based kernel for BCI applications*. **Neurocomputing 112**: 172–178 (SPD manifold for EEG).  \n",
    "- Gilpin, W. (2020). *Deep Reconstruction of Strange Attractors from Time Series*. **NeurIPS 33**, arXiv:2002.05909 ([Deep reconstruction of strange attractors from time series](https://papers.nips.cc/paper_files/paper/2020/hash/021bbc7ee20b71134d53e20206bd6feb-Abstract.html#:~:text=dimensional%20time%20series%20of%20measurements,electricity%20usage%2C%20neural%20spiking%2C%20and)).  \n",
    "- Brunton, S., Proctor, J., & Kutz, J. (2016). *Discovering Governing Equations from Data: Sparse Identification of Nonlinear Dynamics (SINDy)*. **PNAS 113**(15): 3932–3937 (modern equation-learning after SSR).  \n",
    "- Brunton, S., & Kutz, J. (2019). *Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control*. Cambridge Univ. Press (a textbook covering many of these SSR techniques in modern setting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44742f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8249ab81",
   "metadata": {},
   "source": [
    "### Revised Pipeline — *SSR → CCM → MI*\n",
    "\n",
    "We replace the explicit “ICA → SSR” front–end of **CAICSSR** by a *single* projection step that delivers an **interpretable state space**.  Linear source–separation (ICA, PCA, NMF) can still be used inside that step, but is no longer an indispensable first block. The updated three–stage architecture therefore reads\n",
    "\n",
    "$$\n",
    "\\boxed{\\; \\text{Raw multivariate signal } \\mathbf X(t)\n",
    "\\;\\xrightarrow{\\textbf{S S R}}\\;\n",
    "\\mathbf y_i(t)\n",
    "\\;\\xrightarrow{\\textbf{C C M}}\\;\n",
    "\\widehat{\\mathcal{G}}\n",
    "\\;\\xrightarrow{\\textbf{M I}}\\;\n",
    "W_{i\\to j}\\;}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1  State–Space Reconstruction (SSR)\n",
    "\n",
    "Let\n",
    "\n",
    "$$\n",
    "\\mathbf{X}(t)=\\begin{bmatrix}X_1(t),\\dots,X_p(t)\\end{bmatrix}^{\\!\\top},\\qquad t=1,\\dots,T .\n",
    "$$\n",
    "\n",
    "We form an **embedding** for each observable (or for each latent component obtained through any desired projection \\$\\mathbf P\\$):\n",
    "\n",
    "$$\n",
    "\\boxed{\\;\n",
    "\\mathbf y_i(t)=\n",
    "\\bigl[\n",
    "\\widetilde X_i(t),\\,\n",
    "\\widetilde X_i(t-\\tau_i),\\,\n",
    "\\dots,\\,\n",
    "\\widetilde X_i(t-(E_i-1)\\tau_i)\n",
    "\\bigr]^{\\!\\top}\\in\\mathbb R^{E_i}\n",
    "\\;}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* \\$\\widetilde X\\_i = (\\mathbf P,\\mathbf X)\\_i\\$ is either a measured channel or a projected source,\n",
    "* \\$E\\_i\\$ and \\$\\tau\\_i\\$ satisfy Takens-type conditions (\\$E\\_i>2d\\_{A,i}\\$).\n",
    "\n",
    "The collection\n",
    "\n",
    "$$\n",
    "\\mathcal M_i=\\{\\mathbf y_i(t)\\}_{t=1}^{T}\n",
    "$$\n",
    "\n",
    "is a smooth, one–to–one image of the underlying attractor of (latent) subsystem \\$i\\$.\n",
    "\n",
    "---\n",
    "\n",
    "## 2  Causal Direction via Convergent Cross Mapping (CCM)\n",
    "\n",
    "Given two reconstructed manifolds \\$\\mathcal M\\_i\\$ and \\$\\mathcal M\\_j\\$ the **cross–map skill**\n",
    "\n",
    "$$\n",
    "\\rho_{i\\to j}(\\tau)=\n",
    "\\operatorname{corr}\\!\\bigl(\n",
    "C_{i\\to j}^{(\\tau)}(t),\\,\n",
    "\\widetilde X_j(t)\n",
    "\\bigr),\\qquad\n",
    "C_{i\\to j}^{(\\tau)}(t)=\\sum_{k=1}^{K}\\! w_k\\,\\widetilde X_j\\bigl(t_k-\\tau\\bigr)\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "is computed by locally weighted simplex projection (Sugihara et al., 2012).\n",
    "\n",
    "CCM theory states\n",
    "\n",
    "$$\n",
    "\\rho_{i\\to j}(\\tau)\\;\\xrightarrow[T\\to\\infty]{}\\;\n",
    "\\rho_{i\\to j}^\\ast(\\tau)>0\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "(i\\!\\to\\!j)\\text{ is dynamically causal at lag }\\tau .\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "**Edge set \\$\\widehat{\\mathcal G}\\$** is therefore estimated as\n",
    "\n",
    "$$\n",
    "(i\\!\\to\\!j)\\in\\widehat{\\mathcal G}\n",
    "\\quad\\Longleftrightarrow\\quad\n",
    "\\exists\\,\\tau\\in\\{1,\\dots,\\tau_{\\max}\\}:\\\n",
    "\\rho_{i\\to j}(\\tau)>\\rho_{\\text{surrogate},\\;1-\\alpha}.\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "(The surrogate distribution uses seasonal– or phase–randomised nulls; \\$\\alpha\\$ is the desired family–wise error rate.)\n",
    "\n",
    "---\n",
    "\n",
    "## 3  Causal Strength by Mutual Information (MI)\n",
    "\n",
    "For every directed pair detected by CCM we **quantify** the information flow as a *conditional* mutual information on embedded coordinates:\n",
    "\n",
    "$$\n",
    "W_{i\\to j}:=\\max_{\\,\\tau\\in\\mathcal T_{i\\to j}}\n",
    "I\\!\\Bigl(\\mathbf y_i(t)\\,;\\,\\widetilde X_j(t+\\tau)\\;\\big|\\;\\mathbf y_j(t)\\Bigr),\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "estimated with k–nearest–neighbour entropy (Kraskov estimator) or any bias–corrected CMI estimator.\n",
    "\n",
    "---\n",
    "\n",
    "## Formal Problem Statement (updated)\n",
    "\n",
    "> **Given** a \\$p\\$‑variate time series \\$\\mathbf X(1{:}T)\\$ generated by a smooth, deterministic (or weakly stochastic) dynamical system,\n",
    "> **find** \n",
    "> (i) an embedding map \\$\\Phi:\\mathbb R^p\\to\\mathbb R^{m}\\$ that yields interpretable coordinates \\$\\mathbf y\\_i\\$,\n",
    "> (ii) a directed graph \\$\\widehat{\\mathcal G}\\$ on vertices \\${1,\\dots,m}\\$ detected via CCM (Definition 4), and\n",
    "> (iii) a weight matrix \\$\\mathbf W=\\[W\\_{i\\to j}]\\$ given by Eq. (5),\n",
    "> such that\n",
    ">\n",
    "> *all* true causal influences \\$(i\\to j)\\$ leave statistically significant CCM footprints and their magnitudes are faithfully ranked by \\$W\\_{i\\to j}\\$.\n",
    "\n",
    "Subject to:\n",
    "\n",
    "1. **Embedding validity**: \\$\\Phi\\$ satisfies Takens diffeomorphism requirements for each subsystem.\n",
    "2. **CCM convergence**: trajectory length \\$T\\$ is sufficient for \\$\\rho\\_{i\\to j}(\\tau)\\$ to converge.\n",
    "3. **Faithfulness**: No cancellation of CCM signal by opposing indirect paths.\n",
    "4. **Causal sufficiency after projection**: latent confounders are either absent or represented among the \\$\\widetilde X\\_i\\$.\n",
    "\n",
    "---\n",
    "\n",
    "## Remarks on the Projection Step\n",
    "\n",
    "* If linear ICA is applied (\\$\\mathbf P=\\mathbf W\\_{\\text{ICA}}\\$) the resulting components are *candidate* subsystem variables; SSR is then performed on each independent component.\n",
    "* Alternatively, **multi‑channel delay embedding** (joint SSR) without prior unmixing may already expose a low‑dimensional manifold that is directly interpretable (e.g., principal dynamic components).\n",
    "* Choice of projection should favour **interpretability** (biophysical meaning, sparse loadings) while retaining observability of the causal links.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Workflow\n",
    "\n",
    "1. **Pre‑processing**: detrend, deseasonalise, optionally whiten.\n",
    "2. **Projection \\$\\Phi\\$**:\n",
    "   *Option A*: identity (raw channels)\n",
    "   *Option B*: PCA/ICA/NMF → select \\$m\\$ interpretable sources.\n",
    "3. **SSR parameter search**: optimise \\$(E\\_i,\\tau\\_i)\\$ via false‑nearest‑neighbours and first‑minimum mutual information.\n",
    "4. **CCM screening**: compute \\$\\rho\\_{i\\to j}(\\tau)\\$ up to \\$\\tau\\_{\\max}\\$; test against surrogates → obtain edge set.\n",
    "5. **MI weighting**: estimate \\$W\\_{i\\to j}\\$ for detected edges.\n",
    "6. **Validation**: repeating with bootstrapped libraries, robustness across embedding choices, and, if possible, experimental / interventional confirmation.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of the *SSR → CCM → MI* Chain\n",
    "\n",
    "* **Model‑free** detection of nonlinear, lagged, state‑dependent causation.\n",
    "* **Unified strength metric** (conditional MI) that is comparable across links and scales.\n",
    "* **Projection flexibility**: allows anything from raw EEG channels to latent ecological guilds, making the framework domain‑agnostic yet interpretable.\n",
    "\n",
    "This revised formulation can be dropped into the previous CAICSSR exposition by substituting Sections 1‑3 accordingly, while keeping all theoretical guarantees intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2dc95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
